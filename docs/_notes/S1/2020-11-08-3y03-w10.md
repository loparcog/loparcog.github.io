---
layout: post
title:  "3Y03 - Week 10 Notes"
date:   2020-11-08 12:00:00 -0400
categories: 3Y03
---

Hypothesis Testing for a Single Variable
- A **statistical hypothesis** is a statement about the parameters of one or more populations
    - General structure consists of a guess, and an alternate guess
- Structure is formally as follows:
    - $H_0$, the **Null Hypothesis**: This is the statement we initially assume to be true
        - Usually in the form $\theta = r$ for some value *r*
        - For example, we may guess $\mu = 50$
    - $H_1$, the **Alternate Hypothesis**: A statement that contradicts the null hypothesis. The alternate hypothesis can take several forms. We can have:
        - $\theta \neq r$ (two-sided alternate hypothesis)
        - $\theta \gneq r$ (upper one-sided alternate hypothesis)
        - $\theta \lneq r$ (lower one-sided alternate hypothesis)
- General strategy is as follows:
    1. Assume $H_0$
    2. Consider the data: Is there something that is very unlikely/implausible given that $H_0$ is true?
    3. If YES, reject $H_0$ in favour of $H_1$
    4.If NO, accept $H_0$
- We use this to test if a parameter has changed, test a theory, conformance testing, etc.
- Method for testing is as follows:
    1. Assume the null hypothesis is true, $H_0 : \theta = r$
    2. Pick an estimator for your parameter, $\hat{\Theta}(X_1, ..., X_n)$
    3. Choose a *critical region* determined by some critical values
        - For example, for a two-sided alternate hypothesis, we could choose a critical region to be the complement of $C = \mathbb{R} (r-l, r+u)$ for critical values $r-l$ and $r+u$, making the critical region $C = (-\infty, r-l] \cup [r+u, \infty))$
    4. Take a sample $x_1, ..., x_n$ and compute a point estimate $\hat{\theta} = \hat{\Theta}(x_1, ..., x_n)$
    5. Is the point estimate in the critical region? In other words, $\hat{\theta} \in C$? If yes, reject $H_0$ and accept $H_1$. If $\hat{\theta} \notin C$, then accept $H_0$
    - The critical region is determined by how likely a sample statistic is to take a value if $H_0$ is true
- There are four possible outcomes to a hypothesis test:
    1. If $H_0$ is true and we accept $H_0$
    2. If $H_1$ is true and we reject $H_0$
    3. If $H_0$ is true and we reject $H_0$ in favour of $H_1$, then we say we made a "Type I Error"
    4. If $H_0$ is false (so $H_1$ is true) and we accept $H_0$ then we say we have made a "Type II Error"
    - We define the *"significance level"* of the test to be the probability of making a Type I Error, which we call $\alpha = P(\text{Type I Error})$
    - The *"power"* of the test is $1 - \beta$, where $\beta = P(\text{Type II Error})$

Fixed Significance Level Testing
- One way to determine a critical region is to require that our test has a fixed significance level $\alpha$
- Suppose $\hat{Theta}$ is an estimator for parameter $\theta$ and we have a null hypothesis $H_0: \theta = r$ and two-sided alternate hypothesis $H_1: \theta \neq r$
- We fix a significance level $\alpha$ and choose critical values based on this value
- In this two sided case, we aim to find a symmetric region defined by critical values $r - a$ and $r + a$ such that:  
$$\alpha = P(\text{Reject } H_0 | H_0 \text{ is True})$$  
$$ = P(\hat{\Theta} \in (-\infty, r-a] \cup [r+a, \infty) | H_0 \text{ is True})$$
- Similar process for one-sided tests

*P*-values
- Gives a more dynamic approach to hypothesis testing than the fixed significance level approach
- Suppose we have a test with a null hypothesis $H_0$, and we have collected a sample of data $(x_1, ..., x_n)$
- The *P*-value of the test is the smallest significance level that would lead to a rejection of $H_0$ with the collected sample
    - Can think of the *P*-value as the probabiliy of being in a sort of variable critical region. The *P*-value is sometimes called the *"observed significance"*
- Let's consider a general two sided test with $H_0: \theta = r$ and $H_1: \theta \neq r$ and estimator $\hat{\Theta}(x_1, ..., x_n)$
- Suppose we take a sample $(x_1, ..., x_n)$ and produce a point estimate $\hat{\theta} = \hat{\Theta}(x_1, ..., x_n)$
- Assuming $H_0$, our "observed" critical values are $r \pm | r - \hat{\theta} | $
- Then the *P*-value of the test is:  
$$P\text{-value} = P(\hat{\Theta} \in (-\infty, r - | r - \hat{\theta} | ] \cup [r + | r - \hat{\theta} |, \infty) | \theta = r)$$
- Given a hypothesis test and observation $\hat{\theta}$, the *P*-value of the observation is the probability of making an observation at least as far from $\theta = r$ as $\hat{\theta}$
- Another way to think of *P*-value is as a measure of the risk that we make an incorrect design if we reject $H_0$ based on the given sample data
    - So for example, if we make an observation/point estimate $\hat{\theta}$ that has a very high *P*-value, then we are at high risk of making a Type I error if we reject $H_0$
- We can use *P*-values to refine the fixed significance level testing procedure:
    - Fix a significance level $\alpha$ and construct a critical region based on it
    - The observed value $\hat{\theta}$ is in the critical region **if and only if** the *P*-value is at most $\alpha$, and so we reject $H_0$ iff $P\text{-value} \leq \alpha$
    - Conversely, we accept $H_0$ iff $P\text{-value} \gt \alpha$

Relationship between Hypothesis Testing and Confidence Intervals
- Let $\theta$ be an unknown parameter. There is a close relationship between a hypothesis test for $\theta$ and confidence intervals for $\theta$
- Suppose $(L,U)$ is a $100(1-\alpha)\%$ confidence interval for $\theta$ constructed around the point estimate $\hat{\theta}$
- Consider a two sided hypothesis test with $H_0: \theta = r$ and $H_1: \theta \neq r$
    - Then the observation $\hat{\theta}$ leads to a rejection of $H_0$ if and only if $r \neq (L,U)$
- This gives us an equivalent way of performing fixed significance level testing
    - Given a test $H_0: \theta = r$ and $H_1: \theta \neq r$, choose test statistic $\hat{\Theta}$ and a significance level $\alpha$
    - For a point estimate $\hat{\theta} = \hat{\Theta}(x_1, ..., x_n)$, construct the $100(1-\alpha)\%$ confidence interval around $\hat{\theta}$
    - If *r* is in the confidence interval, accept $H_0$
    - If *r* is not in the confidence interval, reject $H_0$ in favour of $H_1$