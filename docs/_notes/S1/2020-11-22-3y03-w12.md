---
layout: post
title:  "3Y03 - Week 12 Notes"
date:   2020-11-22 12:00:00 -0400
categories: 3Y03
---

Tests on a Population Proportion
- Modelling the occurrence of defectives with the binomial distribution is usually reasonable when the binomial parameter *p* represetns the proportion of defective idtems produced
    - Consequently, many engineering decision problems involve hypothesis testing about *p*
- An approximate test based on the normal approximation t the binomial is given. As noted earlier, this approximate procedure will be valid as long as *p* is not extremely close to 0 or 1, and if the sample size is relatively large
- Let *X* be the number of observations in a random sample of size *n* that belongs to the class associated with *p*. Then if the null hypothesis $H_0: p = p_0$ is true, we have $X \approx N[np_0, np_0(1-p_0)]$
- ![img]({{ site.url }}/assets/3y03/thbp.PNG)

Type II Error and Choice of Sample Size
- Approximate sample size for a two-sided test on a binomial proportion is given as:  
$$n = \left[ \frac{z_{\alpha/2} \sqrt{p_0(1-p_0) + z_\beta\sqrt{p(1-p)}}}{p-p_0}   \right]^2$$
- For a one-sided test:  
$$n = \left[ \frac{z_{\alpha} \sqrt{p_0(1-p_0) + z_\beta\sqrt{p(1-p)}}}{p-p_0}   \right]^2$$

Hypotheses Tests on the Difference in Means, Variances **Unknown**
- **Case 1:** $\sigma_1^2 = \sigma_2^2 = \sigma^2$  
$$H_0: \mu_1 - \mu_2 = \Delta_0, H_1: \mu_1 - \mu_2 \neq \Delta_0$$
    - Let $X_{11}, ..., X_{1n_1}$ be a random sample of $n_1$ observations from the first population, and $X_{21}, ..., X_{2n_2}$ be a random sample of $n_2$ observations from the second population
    - Let $\bar{X}_1, \bar{X}_2, S_1^2$ be the sample means and variances, respectively. Now the expected value of the difference of sample means $\bar{X}_1 - \bar{X}_2$ is:  
    $$E(\bar{X}_1 - \bar{X}_2) = \mu_1 - \mu_2 $$  
    so $\bar{X}_1 - \bar{X}_2$ is an unbiased estimator of the difference in means
    - The variance of $\bar{X}_1 - \bar{X}_2$ is:  
    $$V(\bar{X}_1 - \bar{X}_2) =\sigma^2(1/n_1 + 1/n_2)$$
    - The pooled estimator of $\sigma^2$, denoted by $S_p^2$ is given as:  
    $$S_p^2 = \frac{(n_1 - 1)S_1^2 + (n_2-1)S_2^2}{n_1 + n_2 - 2}$$  
    - ![img]({{ site.url }}/assets/3y03/twodist.PNG)
- **Case 2:** $\sigma_1^2 \neq \sigma_2^2$
    - If $H_0: \mu_1 - \mu_2 = \Delta_0$ is true, the statistic:  
    $$T_0^* = \frac{\bar{X}_1 - \bar{X}_2 - \Delta_0}{\sqrt{S_1^2/n_1 + S_2^2/n_2}}$$  
    is distributed approximately as *t* with degrees of freedom given by (rounded down to the nearest integer):  
    $$v = \frac{(s_1^2/n_1 + s_2^2/n_2)^2}{\frac{(s_1^2/n_1)^2}{n_1 - 1} + \frac{(s_2^2/n_2)^2}{n_2 - 1}}$$

Empirical Models
- The collection of statistical tools used to model/explore relationships between variables that are related in a nondeterministic manner is called **regression analysis**
- We present the situation in which there is only one independent/predictor variable *x*, and the relationship with the response *y* is assumed to be linear. It is probably reasonable to assume that the mean of the random variable *Y* is related to *x* by the following straight-line relationship:  
$$Y = \beta_0 + \beta_1x + \epsilon$$  
where the slope and intercept of the line are called **regression coefficients** and $\epsilon$ is the random error term
    - We call this model the **simple linear regression model** because it has only one independent variable or regressor
- Suppose that the mean and variance of $\epsilon$ are 0 and $\sigma^2$, respectively. Then:  
$$E(Y | x) = \beta_0 + \beta_1 x, V(Y | x) = \sigma^2$$
- Suppose that the true relationship between *Y* and *x* is a straight line and that the observation *Y* at each level of *x* is a random variable:  
$$Y = \beta_0 + \beta_1 x + \epsilon$$  
where the intercept $\beta_0$ and the slope $\beta_1$ are unknown regression coefficients
- Suppose that we have *n* pairs of observations $(x_1, y_1), ..., (x_n, y_n)$. The estimates of $\beta_0$ and $\beta_1$ should result in a line that is a "best fit" to the data. We can estimate these parameters using the method of **Least Squares**
- We may express the *n* observations in the sample as:  
$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i, i = 1,2,...,n$$  
and the sum of the squares of the deviations of the observations from the true regression line is:  
$$L = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^2 (y_i - \beta_0 + \beta_1 x_i)^2$$
- The **least squares estimates** of the intercept and slope in the simple linear regression model are:  
$$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$$  
$$\hat{\beta}_1 = \frac{\sum_{i=1}^n y_i x_i - \frac{(\sum_{i=1}^n y_i)(\sum_{i=1}^n x_i)}{n}}{\sum_{i=1}^n x_i^2 - \frac{(\sum_{i=1}^n x_i)^2}{n}}$$  
where $\bar{x} = \frac{\sum x_i}{n}, \bar{y} = \frac{\sum y_i}{n}$
- The fitted or estimated regression line is therefore:  
$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$$  
    - Note that each pair of observations satisfies the relationship:  
    $$y_i - \hat{\beta}_0 + \hat{\beta}_1 x_i + e_i, i = 1, 2, ..., n$$  
    where $e_i = y_i - \hat{y}_i$ is called the **residual**
    - The residual describes the error in the fit of the model to the *i*th observation $y_i$
- Let  
$$S_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2 = \sum_{i=1}^n x_i^2 - \frac{(\sum x_i)^2}{n}$$  
and  
$$S_{xy} = \sum_{i=1}^n (y_i - \bar{y})(x_i - \bar{x})^2 = \sum_{i=1}^n x_iy_i - \frac{(\sum x_i)(\sum y_i)}{n}$$ 
- The residuals $e_i$ are used to obtain an estimate of $\sigma$. The sum of squares of the residuals, often called the **error sum of squares** is:  
$$SS_E = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y-i-\hat{y}_i)^2$$
- Therefore, an unbiased estimator of $\sigma^2$ is:  
$$\hat{\sigma}^2 = \frac{SS_E}{n-2}$$  
- Note that $SS_E$ can also be calculated with:  
$$SS_E = SS_T - \hat{\beta}_1 S_xy$$  
where $SS_T = \sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n y_i^2 - n\bar{y}^2$ is the total sum of squares of the response variable *y*
- $\hat{\beta}_1$ is an unbiased estimator in simple linear regression of the true slope $\beta_1$ and $V(\hat{\beta}_1) = \sigma^2/S_{XX}$
- $\hat{\beta}_0$ is an unbiased estimator of the intercept $\beta_0$ and $V(\hat{\beta}_0) = \sigma^2 \left[ \frac{1}{n} + \frac{\bar{x}^2}{S_{XX}} \right]$
- The **estimated standard error** of the slope and intercept are:  
$$se(\hat{\beta}_1) = \sqrt{\frac{\hat{\sigma}^2}{S_{XX}}}$$  
$$se(\hat{\beta}_0) = \sqrt{\hat{\sigma}^2 \left[ \frac{1}{n} + \frac{\bar{x}^2}{S_{XX}} \right]}$$