---
layout: post
title:  "3Y03 - Lecture 34"
date:   2020-12-07 10:30:00 -0400
categories: 3Y03
---

Final Exam Review
===

Exam Coverage
- Note: All textbook sections are based on V7 of the textbook
- **Probability (Ch.2 - Ch.5)**
- Ch.2: 2.1-.7, .9
- Ch.3: 3.1-.4, .6-.8
- Ch.4: 4.1-.2, .5,  
.6 (**omit** $N(\mu, \sigma^2) \approx Poisson$),   
.7
- Ch.5: 5.1 (**omit** discrete random variables, $\mu, \sigma^2$),  
5.2 (**omit** conditional probability distribution),  
5.4, 5.6
- **Statistics (Ch.6 - CH.?)**
- Ch.6: 6.1-.4, .7
- Ch.7: 7.1-.2,  
.3 (**omit** bootstrap)
- Ch.8: 8.1-.2, .4
- Ch.9: 9.1-.2,  
.3 (**omit** 9.3.2)
- Ch.10: 10.2 (**omit** 10.2.2)
- Ch.11: 11.1-.5,  
.6 (**omit** CI of population proportion)

Review
- Ch.2: Definition and Terminology of Probability
    - Meaning for sample space, event, outcomes, axiom of probability
    - Sets (computation, $\cup, \cap$, c, distributive law, DeMorgan's law)
        - Venn diagrams will be included in this section
    - Probability of events ($\cap, \cup$, c)
        - Conditional probability, independence
        - $P(A \cap B) = P(A) P(B)$
        - $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
    - Random Variables, $X, Y, Z$
- Ch.3: Discrete Random Variables
    - Probability Mass Function (pmf):  
    $$f(x) = P(X = x)$$  
    where $X$ is the random variable and $x$ is the number/value
    - Cumulative Distribution Function (cdf):  
    $$F(x) = \sum_{X \leq x} f(x)$$
    - Mean, variance, standard deviation
    - Binomial, Geometric, Negative Binomial, Hypergeometric, and Poisson Distribution
        - Know pdf, cdf, mean, variance equations
- Ch.4: Continuous Random Variables
    - Probabilty Density Function (pdf):  
    $$f(x)$$  
    - Cumulative Density Function (cdf):  
    $$F(x) = \int_{-\infty}^x f(u) du$$
    - Mean, variance, standard deviation
    - Normal Distribution $N(\mu, \sigma^2)$
        - Know standard normal distribution and how to use the table to approximate binomial distribution
    - Exponential Distribution
- Ch.5: Joint Probability
    - Joint pdf:  
    $$f_{XY}(x,y), F_{XY}(x,y)$$  
    - Independence:  
    $$f_{XY} = f_X f_Y$$  
        - Marginal pdf
    - Covariance:  
    $$cov(x,y)$$  
    - Correlation:  
    $$P_{XY}: corr(x,y)$$
    - Linear function of random variables ($X_1, ..., X_n$):  
    $$Y = c_0 + c_1 x_1 + c_2 x_2 + . .. + c_n x_n$$
        - $E(Y) = c_0 + c_1 E(X_1) + ... + c_n E(X_n)$
        - $V(Y) = c_1^2 V(X_1) + ... + c_n^2 V(X_n) + \text{covariance term}$
- Ch.6: Basics of Statistics
    - Sample, Random sample
        - Sample: observations, $x_1, ..., x_n$ (numbers)
        - Random Sample: $X_1, X_2, ..., X_n$ (random variables)
    - Sample mean $\bar{x}$, sample variance $s^2$, sample standard deviation $s$, median $m$, quartiles $Q_1, Q_3$, range, mode
    - Graphical visualization
        - Stem and Leaf (deep, leaf)
        - Histogram (frequency, relative frequency, cumulative frequency)
        - Box Plot (vertical/horizontal, outliers)
        - Probability Plot (Q-Q plot)
        - Scatter Plot (Dashed line)
- NOTE: Statistical inference (3 groups)
    1. Point estimation (estimation, Ch. 7)
    2. Confidence interval (estimation, Ch. 8)
    3. Hypothesis testing (Ch. 9)
    - All covered with two populations in Ch. 11
- Ch.7: Point Estimation
    - $\mu$: $\bar{x}, $m$, $\bar{x}$ without outliers, etc...
    - $\sigma^2$L: $s^2$, $s^2$ without outliers, etc...
    - $\hat{\Theta}$ as point estimator
    1. $E(\hat{\Theta})$: Unbiased estimator
    2. $V(\hat{\Theta}), E(\hat{\Theta})$ MVUE (minimum variance unbiased estimator)
    3. Estimated standard error of $\hat{\Theta}$, $SE(\hat{\Theta})$
    4. Mean squared error $E[(\hat{\Theta} - \theta)^2] = V(\hat{\Theta}) + (bias)^2$
    - Efficiency of two estimator $\hat{\Theta}_1, \hat{\Theta}_2$ (optimal):  
    $$\frac{MSE(\hat{\Theta}_1)}{MSE(\hat{\Theta}_2)}$$
    - Central Limit Theory: N (large enough) ~ $N(\mu, \sigma^2) \to ...$
- Ch.8: Confidence Interval
    1. Population Mean
    - Population variance $\sigma^2$ known:  
    $$z = \frac{\bar{x} - \mu}{\sigma/\sqrt{n}}$$
        - Two-sides interval:  
        $$LR = \bar{x} \pm Z_{\alpha/2} (\sigma/\sqrt{n})$$  
        - One-side interval:  
        $$L = \bar{x} - Z_{\alpha} (\sigma/\sqrt{n}), R = \bar{x} + Z_{\alpha} (\sigma/\sqrt{n})$$  
    - Population variance $\sigma^2$ unknown
        - Sample size *n* large enough ($n \geq 30$):  
        $$z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}}$$
            - Uses standard normal (z) distribution
        - Sample size $n \lt 30$:  
        $$T = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}}$$
            - Uses t-distribution (fat-tailed)
            - Two-sides interval:  
            $$LR = \bar{x} \pm t_{\alpha/2, n-1} (s/\sqrt{n})$$  
            - One-side interval:  
            $$L = \bar{x} - t_{\alpha, n-1} (s/\sqrt{n}), R = \bar{x} + t_{\alpha, n-1} (s/\sqrt{n})$$  
    2. Population Proportion *p*
    - Binomial(n,p): $\mu = np, \sigma^2 = np(1-p)$
        - z-score, two-sides interval:  
        $$\hat{p} \pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$$  
        one-side interval:  
        $$\hat{p} +/- z_{\alpha}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$$
- Ch.9: Hypothesis Testing
    1. Population Mean
    - Population variance $\sigma^2$ known: **z-test**
        - Two-sides test: $H_1: \mu \neq \mu_0$
        - One-side test: $H_1: \mu \gt \mu_0, \mu \lt \mu_0$
    - Population variance $\sigma^2$ unknown
        - Sample size $n \geq 30$: z-test
        - Sample size $n \lt 30$: t-test
    2. Population Proportion
    - Value given in table
- Ch.10: Multiple populations
    - $\mu_1, \sigma_1^2, p_1$ vs $\mu_2, \sigma_2^2, p_2$
    1. Population Mean $H_0: \mu_1 - \mu_2 = \mu_0 \text{ or } \mu_1 = \mu_2$
        - Population variance $\sigma_1^2, \sigma_2^2$ known:  
        $$\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$$
        - Population variance $\sigma_1^2, \sigma_2^2$ unknown:  
        $$\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}$$

- Ch.11: Linear Regression
    - $Y = \beta_0 + \beta_1 x + \epsilon$
        - $\beta_0$: intercept  
        $\beta_1$: slope  
        $\epsilon$: error term
    - $H_0: \beta_1 = 0, H_1: \beta_1 \neq 0$
    - Check $\epsilon$ with residual, follows $N(0, \sigma^2)$