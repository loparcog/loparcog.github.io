---
layout: post
title:  "4E03 - Lecture 3"
date:   2020-09-14 16:30:00 -0400
categories: 4E03
---

Further Probability
===

**Bayes' Law**
- Suppose that we know P{F\|E} but we would like to know P{E\|F}. The theorem would be:  
$P(F\|E) = \frac{P(E|F)P(F)}{P(E)}$
- Example: Someone is tested for a rare disease. The testing methods are correct 99% of the time (if the person has the disease, 99% chance to return positive, and vice versa). Suppose the disease occurs in the general population at the rate of out of every 10000 people. **What are the changes that someone testing positive actually has the disease?**  
SOLN:
The test will give a false positive 1/100 of the time. However, only 1/10000 people have it. Sorting this into events:  
Event A: One has the disease
Event B: One tests positive  
P{A|B} = ?
P{A} = 0.0001, P{B|A} = 0.99, but what is P{B}?
$P(B) = P(B|A)P(A) + P(B|A)P(A')$  
$ = (0.99)(0.0001) + (0.01)(0.9999)$  
**$P(A|B) = 0.0098$**

Random Variables
- A random variable X (capital letters) is a real-valued function of the outcome of an experiment. Some examples of these are:
    1. The sum of the rolls of two dice
    2. Number of requests for a website by some time *t*
    3. The time until the next request arrives for a website
    4. The CPU requirement (in seconds) of an HTTP request
- 1 and 2 are **discrete**, while 3 and 4 are **continuous**
    - For 2, if *N* is the associated random var, then we can define events such as "the number of arrivals by the time *t* is less than or equal to 5", and "the number of arrivals by time *t* is between 1 and 3"

Distributions
- We would like a compact way of defining the probabilities of events
    - We will use the (cumulative) distribution function (distribution c, d, f):  
    $F_X(x) = P(X \lt x)$
    - We will use $F(x)$ if the variable is clear

Continuous Random Variables
- For quantities that take on values on a continuum, like time
- F(x) is continuous in this case
- The associated density function is:  
$$f_X(x) = \frac{dF_X(x)}{dx}$$  
$$P(a \leq X \leq b) = F_X(b) - F_X(a) = \int_a^b f_X(x) dx$$  
$$\int_{-\inf}^{\inf} f_X(x) dx = 1$$


Discrete Random Variables
- Probability mass function (p.m.f.) gives the probabilities that the random variable equals values in a discrete set (its range of possible values). 
- Its density function is:  
$$p_X(i) = P(X = i)$$  
$$P(j \lt X \leq k) = F_X(k) - F_X(j) = \sum_{j \lt i \leq k} p_X(i)$$  
$$\sum_i p_X(i) = 1$$

Summary Statistics - Mean
- Expected value (or mean) is given by:  
$$E[X] = \sum_i ip_X(i)$$ (Discrete)  
$$E[X] = \int_{-\inf}^{inf} xf_X(x) dx$$ (Continuous)
- Also sometimes called the average, but that word will be reserved for later
- Note that the expectation does not have to be an outcome, the roll of a die is a classic example:  
$E|X| = 3.5$

Higher Moments
- In general, one can compute the *k*th moment:  
$$E[X^k] = \sum_i i^k p_X(i)$$ (Discrete)  
$$E[X^k] = \int_{-inf}^{\inf} x^k f_X(x) dx$$ (Continuous)
- Important to us are k=1 (mean), k=2

Variance
- An important summary statistic is the *variance*, which measures the expected deviation from the mean  
$$Var(X) = \sigma^2_X = E[(X - E[X])^2]$$
    - How spread out the distribution is
- Related to the variance is its square root, the *standard deviation**, $\sigma_X$. We can use the following fact to compute the variance, using the first and second moments:  
$$E[(X - E[X])^2] = E[X^2] - 2E[XE[X]] + (E[X])^2$$  
$$= E[X^2] - 2E[X]E[X] + (E[X])^2$$  
$$= E[X^2] - (E[X])^2$$
- Example: Let  
$f(x) = kx(100 - x) \text{ if } 0 \leq x \leq 100$  
$0 \text{ otherwise}$  
be the density of the lifetime of a component (in days)
    1. Find the probability the lifetime is between 9 and 90 days  
    SOLN:  
    Let X be the lifetime. We want:  
    $P(9 \leq X \leq 90)$  
    $= \int_a^b f(x) dx$  
    $= \int_9^{90} kx(100 - x) dx$  
    We don't want k there, but we also know that we need  
    $\int_0^{100} f(x) dx = 1$  
    ...  
    $k = \frac{3}{500000}$  
        - NOTE: We're using 0 to 100 instead of -inf to inf since that's what the function is defined in, and it is 0 otherwise
    - Now, given this, we have 
    $P(9 \leq X \leq 90) = \int_9^{90} \frac{3}{500000} x (1-x) dx$  
    **$= 0.9492$**

    2. Find the mean lifetime  
    SOLN:  
    $E(X) = \int_0^{100} x f(x) dx = 50$

Some useful distributions

- **Geometric** (Discrete)  
$p_X(k) = q(1 - q)^{k-1}, k=1, 2, ...$  
where *q* is the probability of success, *1-q* is the probability of failure, and *p<sub>X</sub>(k)* is the probability that success happens on the *k*th trial. 
    -Arises in applications like testing, if we do independent tests, how many tests must be performed before success?
    - With a bit of work, we can show that:  
    $E|X| = \frac{1}{q}$  
    $Var(X) = \frac{1-q}{q^2}$
- **Poisson** (Discrete)  
The Poisson distribution (no relation to fish) has one parameter $\lambda$ and has p.m.f:  
$$p_X(k) = \frac{e^{-\lambda}\lambda^k}{k!}, k = 0,1,2...$$
    - Does it sum to one?  
    $$\sum_{k=0}^{\inf}e^{-\lambda}\frac{\lambda^k}{k!}$$  
    $$= \sum_{k=0}^{\inf}e^{-\lambda}e^{\lambda}$$  
    $$= 1$$
    - Also,  
    $E|X| = \lambda$ 
    $Var(X) = \lambda$
- **Uniform** (Continuous)  
The uniform distribution, denoted by *U(a, b)*, has the density:  
$$f(x) = \frac{1}{(b-a)}, a \leq x \leq b$$  
$$0 \text{ otherwise}$$  
    - This means that the random variable is "equallty likely" to take values anywhere between *a* and *b*. This **does not mean** that $P(X=i) = P(X=j)$, although this is trivially true. It does mean, for example,  
    $$P(0.1 \leq X \leq 0.2) = P(0.3 \leq X \leq 0.4)$$  
    (As long as $a \leq 0.1$ and $b \geq 0.4$)
    - An important special case is *U*(0, 1), as many built in random number generators (RNG) produce samples from this distribution
    - Mean and variance values follow:  
    ![img]({{ site.url }}/assets/4e03/uni-dist.png)
    