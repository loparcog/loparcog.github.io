---
layout: post
title:  "3Y03 - Week 5 Notes"
date:   2020-10-04 12:00:00 -0400
categories: 3Y03
---

Normal Distribution
- Density function of a normal distribution with mean $\mu$ and variance $\sigma^2$:  
AAA (img)
- Approximation to the Binomial Distribution
    - Binomial distribution is roughly bell-shaped, just like the normal distribution
    - Binomial random var is discrete, and the normal random variable is continuous, but under special circumstances we may approximate one by the other
    - Typically, we're interested in approximating the binomial distribution by the normal distribution, since the computations involved in the binomial distribution can be intensive, involving very large numbers
    - Suppose $X ~ Bin(n, p)$ is the binomial random variable with parameters $n, p$
        - If $n$ is *large enough*, then $x \approx N(np, np(1-p))$, the normal random variable with mean $np$ and variance $np(1-p)$
        - These approximations are usually good when $np \gt 5$ and $n(1-p) \gt 5$ (what we mean by $n$ being sufficiently large)
- Suppose $X$ is binomial with parameters $n,p$. If $n$ is sufficiently large, then:  
$$\frac{X - np}{\sqrt{np (1-p)}} \approx Z = N(0,1)$$
    - We can obtain a slightly better approximation by accounting for the continuity of the normal random variable
- Continuity Correction: Let $X$ be binomial with parameters $n,p$. If $n$ is sufficiently large then, for $x \in \mathbb{R}$:  
$$P(X \leq x) = P(X \leq x + 0.5) \approx P(X \leq \frac{x - np - 0.5}{\sqrt{np (1-p)}})$$  
where the first equality holds since $X$ is discrete

Exponential Distribution
- The exponential distribution is a continuous distribution that is intimately related to the Poisson DRV
    - Consider an interval $[0,x]$, where $x \in \mathbb{R}$
    - Suppose that on the interval, there is some event that occurs according to a Poisson distribution with parameter $\lambda$
    - Define a random variable $X$ to be the distance in $[0,x]$ from zero to the first event. $X$ is clearly a CRV, and is called an exponential RV
    - Consider $P(X \gt x)$, that is, the probability that *no events* occur in the interval $[0,x]$
    - Since we assumed that the events are distributed according to a Poisson RV, we have:  
    $$P(X \gt x) = \frac{e^{- \lambda x} (\lambda x)^0}{0!} = e^{- \lambda x}$$  
    - By the axioms of probability, we have:  
    $$P(X \leq x) = 1 - P(X \gt x) = 1 - e^{- \lambda x}$$  
    - Thus, if $X$ is exponential with parameter $\lambda$, then $X$ has a cumulative distribution function given by $F(x) = 1 - e{- \lambda x}$
    - Get the pdf of $X$, we simply differentiate with respect to $x$:  
    $$f(x) = F'(x) = \lambda e^{- \lambda x}$$
- If $X$ is an exponential RV with parameter $\lambda$, then $f(x) = \lambda e^{-\lambda x}$, and:  
$$E(X) = \frac{1}{\lambda}$$  
$$\sigma^2 = V(X) = \frac{1}{\lambda^2}$$
- Lack-of-Memory Property: Let $X$ be a random variable. We say that $X$ is *memoryless* or has the *lack-of-memory property* iff for all $s, t \in \mathbb{R}$:  
$$P(X \lt s + t | X \gt s) = P(X \lt t)$$
    - While there are several discrete random vars that are memoryless, turns out that exponential random variables are completely characterized by this property
- Let $X$ be a continuous random variable. If $X$ is exponential, then $X$ is memoryless. Conversely, if $X$ is any continuous random variable that is memoryless, then $X$ must be an exponential random variable

Joint Probability Distribution
- ONLY working with continuous random variables
    - Discrete is similar (turn integrals into sums), and is in the textbook
- Let $X, Y$ be two CRV. They may or may not be related. The **joint probability density function** of $X, Y$ is a function $f_{X,Y}(x, y)$ satisfying the following properties:
    1. $f_{X, Y}(x, y) \gt 0$ for all $x, y \in \mathbb{R}$
    2. $\int \int_{\mathbb{R}^2} f_{X, Y}(x, y) dx dy = 1$  
    3. For any region in $A \subseteq \mathbb{R}^2$:  
    $$P((X,Y) \in A) = \int \int_A f_{X, Y}(x, y) dx dy$$
- Suppose $X, Y$ are continuous RV's defined in a region $R \subseteq \mathbb{R}^2$ (we may just assume that $f_{X,Y}(x, y)$ is defined to be 0 outside of $R$)
- Suppose also that we are given a joint pdf $f_{X,Y}(x,y)$ such that $\int \int_R f_{X,Y}(x,y) dx dy = 1$
- We would like to understand the distributions of $X, Y$ as random variables in their own right, and so we would like to know their pdfs
- For $a, b \in \mathbb{R}$, define:  
$$R_X(a) := (y \in \mathbb{R} : (a, y) \in R)$$
$$R_Y(b) := (x \in \mathbb{R} : (x, b) \in R)$$
- Then, the $$marginal probability densities$$ are:  
$$f_X(x) = \int_{R_X(x)} f_{X, Y} (x, y) dy$$
$$f_Y(y) = \int_{R_Y(y)} f_{X, Y} (x, y) dx$$
- Using the marginal pdfs, we can compute the expected value of $X, Y$ as follows:  
$$\mu_X = E(X) = \int_{- \inf}^{\inf} x f_X(x) dx = \int \int_{\mathbb{R}^2} x f_{X,Y}(x,y) dx dy$$  
and similarly for $Y$
- The variance of $X,Y$ can be computed from the usual formula:  
$$V(X) = E((X - \mu_X)^2) = E(X^2) - E(X)^2$$
