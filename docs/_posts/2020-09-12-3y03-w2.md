---
layout: post
title:  "3Y03 - Week 2 Notes"
date:   2020-09-12 21:00:00 -0400
categories: 3Y03
---


Interpretations and Axioms of Probability
- Probability used to quantify likelihood/chance that an outcome of a random experiment will occur
    - Probabilities interpreted as the limiting value of times an outcome occurs in *n* repetitions of a random experiment as *n* goes to infinity
    - Probabilities chosen so the sum of all outcomes in an experiment add to 1
- Whenever a sample space consists of *N* possible outcomes that are equally likely, the probability of each outcome is 1/*N*
- In a discrete space, the probability of event *E*, denoted as $P(E)$, is the sum of probabilities of the outcomes in E
- A probability must satisfy the following properties:
    - $P(S) = 1$, if S is the sample space
    - $0 \le P(E) \le 1$ for any event *E*
    - For two events, if $E_1 \cap E_2 = \emptyset$, then $P(E_1 \cup E_2) = P(E_1) + P(E_2)$
- Also note the following axioms:
    - $P(\emptyset) = 0$
    - $P(E') = 1 - P(E)$
- The probability of a union is denoted as:  
$P(A \cup B) = P(A) + P(B) - P(A \cap B)$
    - However, if the events are mutually exclusive, the union is:  
    $P(A \cup B) = P(A) + P(B)$
    - Events are mutually exclusive if their intersections equal the empty set:  
    $P(E_1 \cap E_2 \cap ... \cap E_n) = \emptyset$
- For three or more events, the following equations can be used:  
$P(A \cup B \cup C) = P[(A \cup B) \cup C] = P(A \cup B) + P(C) - P[(A \cup B) \cap C]$  
or...  
$P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$
- The conditional probability of event B given event A is denoted as:  
$P(B \| A) = \frac{P(A \cap B)}{P(A)}$, for P(A) \> 0
    - This can also turn into an intersection given the multiplication rule:  
    $P(A \cap B) = P(B \| A)P(A) = P(A \| B)P(B)$
- For any events A and B, totality can be shown through the following:  
$P(B) = P(B \cap A) + P (B \cap A') = P(B \| A)P(A) + P(B \| A')P(A')$
    - This can also be done with any number of events, given they're mutually exclusive and exhaustive, adding all intersects and given without adding the compliment functions
- Two statements are also independent if any of the following is true:  
$P(A \| B) = P(A)$  
$P(B \| A) = P(B)$  
$P(A \cap B) = P(A) P(B)$
    - Furthermore any number of events are independent if:  
    $P(E_1 \cap E_2 \cap ... \cap E_n) = P(E_1) * P(E_2) * ... * P(E_n)$

Random Variable / Distributions
- A random variable is a function that assigns a real number to each outcome in a sample space of a random experiment
    - Denoted by an uppercase letter, such as *X*. After an experiment is conducted, the measured value of the random variable is denoted by a lowercase letter, such as *x* = 70
    - A discrete random variable is a random variable with a finite (or countably infinite) range
        - Ex. number of scratches on a surface, proportion of defective parts among 1000 tested
    - A continuous random variable is a random variable with an interval (finite or infinite) of real numbers for its range
        - Ex. Electrical current, length, pressure, temp, time
- The probability distribution of *X* is a description of the probabilities associated w/ the possible values of *X*. For a discrete random var, the distribution is often specified by just a list of the possible values along with their probabilities
- Probability Mass Function
    - For a discrete random variable *X* with possible values $x_1, x_2, ..., x_n$, a *probability mass function* (PMF) is a function such that:
        1. $f(x_i) \geq 0$
        2. $\sum^n_{i=1} f(x_i) = 1$
        3. $f(x_i) = P(X = x_i)$
