---
layout: post
title:  "4AA4 - Midterm 1 Cheat Sheet"
date:   2020-10-14 13:00:00 -0400
categories: 4AA4
---

RTOS Intro
===

General System
- A set of interacting or independent component parts forming a complex/intricate whole
    - In a control system/app, **response delay** is an important characteristic
        - A **computing system** takes inputs and, with programs and storage, processes a result
        - Some characteristics include accuracy, functionality, robustness, usability, response speed (delay), etc.

**Real-Time System (RTS)**
- **Response Time**: Time interval between a stimulus/input and the corresponding result/output
    - A "timely" response to external stimuli is vital
- **Classification of RTS**
    - A **soft RTS** is one where performance is degraded but not *destroyed* by failure to meet a response-time constraint
        - Ex. Streaming videos, computer games, online chatting
    - A **firm RTS** is one where missing a few deadlines will not lead to total failure, but missing more than a few may lead to complete and catastrophic system failure
        - Ex. Manufacturing systems with robot assembly lines, coursework submissions
    - A **hard RTS** is one where failure to meet a single deadline may lead to complete and catastrophic failure
        - Ex. Mission critical systems, nuclear systems, medical applications such as pacemakers
    - Cost for missing deadlines increases from sort to hard
- **Terminology for RTS**
    - Multitasks (periodic, aperiodic tasks)
    - Schedulability (ability of tasks to meet hard deadlines)
    - Performance (response time, cost of missing deadlines)
    - Does a RTS mean a *fast* system?
        - No, as long as the tasks execute when needed, that's a real time system

**RTOS and Kernel Modules**
- **Kernel and Kernel Space**
    - Kernel runs within the OS
    - Provides services like process/memory management, file systems, scheduling, etc.
    - User space is the space in memory where user processes run, and kernel space is the space where kernel processes run
        - User space only allows limited instructions and prohibits direct access to the hardware/memory
        - Kernel space allows all instructions as well as full hardware access
- How does the OS deliver the services to user processes?
    - User processes request a service from the kernel by **making a system call**, utilizing the **library procedure**
        - This procedure puts params of the system call in suitable registers and then issues a **TRAP** instruction
        - Control is the passed onto the kernel, checking the validity of the params and performing the requested service
        - When finished, a code is put into a register to let the user know if a process passed or failed
        - Finally, a return from the TRAP instruction is executed, and control is passed back to the user process
    - **A user process becomes a kernel process when it executes a system call**
- **Real-Time Operating System (RTOS)**
    - Designed for real time tasks, correctness depends on logic of the result and time
    - Based on predictability and guarentee deadlines are met
        - Can be hard to soft depending on cost of missing a deadline
- **Process Scheduling**
    - Decision on which process runs first, which order
    - Difference between an OS and RTOS are seen in the table below:  
    ![img]({{ site.url }}/assets/4aa4/OS-RTOS.png)
    - Also note, **preemption** (the ability to interrupt a current task for one with a higher priority), which is a requirement for a RTOS

Kernel Modules and Code
===

Kernel Modules
- Two approaches to implement RTOS based on linux:
    1. Add a new layer of RT Kernel with full control of interrupts and processor key features
        - RTAI and RTLinux both use real-time kernels as the main kernel
    2. Make necessary changes in the Linux Kernel to make it suitable for real time applications (preempt RT)
        - Preempt RT is used on the myRIO devices, so the kernel threads are preemptable - a **kernel module** is a piece of code that can be loaded and unloaded into the kernel upon demand
        - Stops wasting memory and users don't need to rebuild and reboot the base kernel every time they require new functionality
- To interact with kernel modules on the system, use the following commands:
    - `insmod`: Insert a module
    - `rmmod`: Remove a module
    - `lsmod`: Show currently loaded modules
    - `modinfo`: Show information about a given Linux Kernel module

Kernel Module Development
- A real-time task running as a kernel module consists of three sections:
    1. `init_module()`: Invoked by `insmod` to prepare for later invocations of module's functions
        - Can be used to allocate required system resources, declare and start tasks, etc.
    2. Task-specific code (based on POSIX API)
    3. `cleanup_module()`: Invoked by `rmmod` to inform the kernel that the module's functions will not be called anymore
        - Good place to release all sys resources allocated during the lifetime of the module, stop and delete tasks, etc.
- A simple example of this can be seen below:

```C
#include <linux/module.h>
#include <linux/kernel.h>

int init_module(void){
  printk(KERN_INFO "Hello World\n");
  return 0;
}

void cleanup_module(void){
  printk(KERN_INFO "Goodbye Cruel World!\n");
}

```

- Compiling a kernel module is done using a make file, example in L3
    - Note, `init_module` can be any function, that function just needs to be tagged with `__init` (or `__exit` for cleanup), marked as static, and then declared at the bottom of the function with the `module_init()` function
    - Other macros also exist like `DRIVER_AUTHOR`, `DRIVE_DESC`, etc.
    - Example:

```C
#define DRIVER_AUTHOR "Giacomo Loparco"
#define DRIVER_DESC "4aa4_lab2_part3"

static int intval __initdata = 2;
static char *my_string = "dummy";

static int __init hello_init(void)
{
    printk (KERN_INFO  "Hello %s world number 2\n", my_string);
    return 0;
}

static void __exit hello_clean(void)
{
    printk (KERN_INFO "<1>Hello cruel world 2\n");
}


module_init(hello_init);
module_exit(hello_clean);

module_param(my_string, charp, 0000);
MODULE_PARM_DESC(my_string, "A character string");
```

- Command line arguments can be passed, but NOT with `argv` or `argc`
    - First, values need to be declared to store values passed on the command line
    - Next, they should be set up with the macro `module_param(name, type, permissions)`
    - At run time, `insmod` will fill up the variables with values passed

Real-Time Scheduling
===

Process
- A program in execution, abstraction of a running system
- Also known as the logical unit of work scheduled by an OS
    - Processes are independent, carry considerable state information, have separate address spaces, and interact threough system-provided inter-processes communication mechanisms
- A program has the following virtual memory layout:
    - **Stack**: Used to store function args and local variables and the return address of functions that called the current function
    - **Heap**: Memory dynamically located by system calls
    - **BSS Segment**: Uninitialized data
    - **Data Segment**: Initialized data
    - **Text**: Read-only region containing program instructions/code  
    ![img]({{ site.url }}/assets/4aa4/vmem.png)
    - Difference of stack vs heap  
    ![img]({{ site.url }}/assets/4aa4/stackvsheap.png) 

Multiple Processes
- A program can be divided into multiple tasks by creating multiple processes, which can be done in one of two ways.
- **Fork** 
    - The `fork()` command:
        - Creates a child process identical to its parent
        - Returns a value of 0 to the child process and returns the process ID of the child process to the parent process
    - There is a lot of overhead to create a new process, as everything is duplicated. Data space is not shared, so its harder to communicate
    - Variables initiated **before** `fork()` will be duplicated from the parent to child process. After `fork()`, a branch is needed to separate parent and child processes, for example:
    
```C
pid = fork();
if (pid == 0){
// Do child process work (fork process)
ChildProcess();
} else{
// Do parent process work (main process)
ParentProcess();
}
```

- `sleep(x);` is commonly used to make child/parent threads wait for one another
- **Threads**
    - Gives a more efficient way to implement a task
    - Multiple subtasks can be implemented as separate streams in a single process
    - The thread model breaks the memory space into two parts:
        1. Contains the program-wide resources such as global data and program instructions/code
        2. Information pertaining to the execution state of control stream, such as the PC and the stack  
    ![img]({{ site.url }}/assets/4aa4/threadex.png)
    - Advantages of threads
        - Shared address space, communication between threads is more efficient
        - Context switching between threads in the same process typically faster than processes
        - Much quicker to make a thread than process
        - Communication is possible through **pointers** to change variables in different threads, not possible with fork
        - Supported by POSIX (Portable OS interface)
    - Disadvantages of threads
        - Need of synchronization, global variables are shared between threads, so any modification to these or shared variables could be disastrous (deadlock, crashes)
        - Less secure, many library functions are not thread safe, multiple threads trying to access the same function can throw an error
        - Lack of robustness, if one thread crashes, the whole application crashes
    - Example:

```C
// Define the number of threads
#define NUM_THREADS 5

// Define a worker function
void  *print_hello(void *threadid)
{
    long tid;
    tid = (long) threadid;
    printf("Hello World! It's me, thread #%ld! \n", tid);
    pthread_exit(NULL);
}

// Go through main thread
int main (int argc, char *argv[])
{
    pthread_t threads[NUM_THREADS];
    int rc;
    long t;
    for (t = 0; t < NUM_THREADS; t++)
    {
        printf("In main: creating thread %ld \n", t);
        rc = pthread_create(threads[t], NULL, print_hello, (void *) t);
        if (rc)
        {
            printf("ERR: Return code from pthread_create() is $d \n", rc);
            return -1
        }

        // If you want to waitr for the thread to finish
        pthread_join(threads[t], NULL);
    }

    return 0
}
```
- Race Condition
    - An error condition in parallel programs, where the outcome of a program changes depending on what code is executed first in a set of thread/fork codes where either any program can run first or second
    - Causes the outcome to not be predictable or repeatable
    - Can be avoided by using thread-safe locks, like semaphores

Event-Driven Programming
- Normal functions would use sleeps to manage child and parent thread timings, but event-driven programming puts signals into use, acting on those signal events
    - Functions used given as follows:

```C
// Set a signal to a given function
signal(SIGUSR1, funcName1);
signal(SIGUSR2, funcName2);

// Get the current process ID
getpid();
// Get the parent process ID
getppid();

// Wait for a process @ ID pid to end
waitpid(pid, NULL, 0);

// Send a signal to a given process
kill(getppid(), SIGUSR1);
```

- Example:

```C
int main ()
{

pid_t pid;
if ((pid = fork()) == 0)
{
    int i;
    for(i=0; i<5; i++)
    {
        sleep(1);
        kill(getppid(), SIGUSR1);
    }
    exit(0);
}
else
{
    signal(SIGUSR2, handle_printSum);
    signal(SIGUSR1, handle_addRNGs);
    waitpid(pid, NULL, 0);
    //printf("The End!\n");
    exit(0);
}

}
```

- How can a race condition be solved in a uniprocessor system?
    - Disable preemptions when scheduling processes (only the process itself can voluntarily relinquish the CPU)
    - Use semaphores as **atomic** operation
- Linux Priority Levels and Nice Values
    - Linux has static priority ranged from 0 to 139, where 0 to 99 are reserved for real time kernel events, and 100 to 139 for users
    - Priority is represented as a **nice value (niceness)**, from [-20, 19], mapping to 100 to 139 (NOTE: the nice value **cannot map to below 100**. -20 is the lowest it goes)
        - The lower the nice value, the higher the priority of the process
        - By default, the priority value is 0
    - A process priority can be changed on linux using `nice -x <pname>`, setting pname's priority to x
    - Can also be done using C code, although a bit more work 
    - Threads can also be declared with a given priority value, allowing the system to know what to run first in a race condition
        - In general though, the parent process will *usually* run first
    - Functions are given as follows:

```C
// Set some default val
int which = PRIO_PROCESS;

// Get a given PID
pid = getpid();

// Get the priority of process at pid
getpriority(which, pid);
// Set the priority of process at pid
int newPriority = 10
setpriority(which, pid, newPriority)

// Alternatively, nice can be called to increase the current process
// Increase the current process priority by x
nice(x);
// NOTE: unprivileged users can only lower the process priority

// Thread priority can be handled with the following

// Set a param object
struct sched_param param;

// Also set some default vals
pthread_attr_t attr;
pthread_attr_init(&attr);

// Set priority to x
param.sched_priority = x

// Can be used for a given thread...
pthread_attr_getschedparam(&tattr, &param);
param.sched_priority = x;
pthread_attr_setschedparam(&tattr, &param)

// ... or in creating a thread
pthread_create(&thread, &tattr, functionName, arg);

// Finally, for fork, there is also sched_setscheduler
// ONLY ACCESSIBLE TO PRIVILEGED USERS
struct sched_param param;
param.sched_priority = 10;
if ( sched_setscheduler(pid, SCHED_FIFO, &param) == -1){
    perror("SetScheduler failed in parent process. \n")
}
```

Tasks and Scheduling
===

Real-Time Tasks
- In real-time systems, several tasks execute **concurrently**, with each having its own **real-time constraint**
- **Periodic Tasks**: Inter-arrival times between two instances are almost the same
- **Sporadic Tasks**: Inter-arrival times between two consecutive instances differ widely (hard timing constraints)
- Temporal Parameters
    - In order to meet real-time reqs of hard real-time tasks, it's assumed many parameters of these tasks are known at all times. Some are:  
        - Number of tasks ($n$)
            - Many embedded systems have number of tasks fixed/limited
        - Arrival/Release time ($r_{i,j}$)
        - Absolute deadline ($d_i = r_{i,j} + D_i$)
            - Moment in time at which the job must be completed
        - Relative deadline ($D_i$)
            - Interval of time when the job should be run and complete
        - Execution time ($E_i$)
            - Counts with no other tasks sharing resources
                - Depends on processor speed, complexity of instructions
            - Actual amount of time required by a job to complete its execution
        - Response time ($R_i$)
            - Time span between task activation and completion
- Periods and Phases of Periodic Tasks
    - A **period** ($p_i$) of a periodic task $T_i$ is the *minimum* length of all time intervals between the release times of consecutive tasks
    - **Phase** of a task ($\phi_i$) is also known as the release time ($r_{i, j}$) of a task $T_i$
        - First instances of several tasks may be released simultaneously, they are called in phase and have a 0 phase
- A typical task model for periodic tasks:
    - All tasks in the task set are **strictly** periodic
    - The relative deadline of a task is equal to its period (if not specified)
    - All tasks are independence, no precedence constraints
    - No task has any non-preemptible section and the cost of preemption is negligible
    - CPU processing requirements are significant, while memory and I/O reqs are negligible
- Representations
    - A periodic task $T_i$ can be represented by a 4 tuple:  
    $\Phi_i , P_i, e_i, D_i$
        - If using 3 tuple $P_i, e_i, D_i$, assume phase = 0
        - If using 2 tuple $P_i, e_i$, assume phase = 0 and $D_i = P_i$
- CPU Utilization
    - A measure of the percentage of non-idle processing, denoted as $U$. Calculated by symming the contribution of utilization factors for each (periodic or aperiodic) task. The utilization factor $u_i$ for a task $T_i$ with execution time $e_i$ and period $p_i$ is given by:  
    $$u_i = \frac{e_i}{p_i}$$
    - And for a system with n tasks, the overall system utilization is:  
    $$U = \sum_{i=1}^n u_i$$
- Polling and Interrupt
    - Polling: Constantly reading a memory location, in order to receive updates of an event or input value
        - Polled loops used for fast response to single devices
        - Pros: Simple to write and debug, response time easy to determine
        - Cons: Generally not sufficient to handle complex systems or burst events, waste of CPU time, particularly when events polled occur infrequently
    - Interrupt: Upon receiving an interrupt signal, the processor interrupts whatever it's doing and serves the request
    - Brief Comparison:  
    ![img]({{ site.url }}/assets/4aa4/polint.png)
- Periodic Task Implementation
    - You can use the functions `sleep()`, `nanosleep()`, and `clock_nanosleep()` to generate periodic tasks
        - All functions suspend the execution of the calling process
        - `sleep()` has a low resolution, where as the other two have high resolution of wait times
        - `clock_nanosleep()` can also change clock type, using times different than regular clock times
- Note for tuples:
    - A periodic task $T_i$ can be represented as a 4 tuple:  
    $(\phi_i, P_i, e_i, D_i)$
    - If using a 3 tuple, we can use:  
    $(P_i, e_i, D_i) = (0, P_i, e_i, D_i)$
    - If using a 2 tuple, we can use:  
    $(P_i, e_i) = (0, P_i, e_i, P_i)$

**Quiz on L9 to go over**
    
Cyclic Executive (CV)
- Table-driven gives off-line static-schedule which specifies *exactly* when each job executes
    - Assumptions:
        - Parameters of jobs with hard deadlines known
        - Task scheduling is non-preemptive
    - Non-periodic work can be run during time slots not used by periodic tasks
    - Sophisticated algorithms can be used
- Consider the following 4 periodic tasks:  
$T_1 = (4; 1)$  
$T_2 = (5; 1.8)$  
$T_3 = (20; 1)$  
$T_4 = (20; 2)$  
    - What is the total utilization? How can we construct a schedule for this process?  
    - Utilization is given as $e_i/P_i$, so this system's total utilization is:  
    $$U = \sum_{i=1}^{n} u_i$$  
    $$= 1/4 + 1.8/5 + 1/20 + 2/20$$  
    $$= 0.76$$
    - Furthermore, a possible schedule for this would be:
    ![img]({{ site.url }}/assets/4aa4/totut.png)
- Hyperperiod (H)
    - The least common multiple (LCM) of the periods of all periodic tasks
    - The max number of arriving jobs in a hyperperiod is denoted as N, with  
    $$N = \sum_{i=1}^n \frac{H}{p_i}$$  
    where $p_i$ is the period of task *i*
        - In the example above, the hyperperiod H is 20 for the four tasks, and N=11
- Frames
    - We want scheduling decisions to be made at random intervals rather than arbitrary times
    - We can do this by dividing a hyperperiod into frames
        - Timing is enforced only at frame boundaries
        - Each task must fit within a single frame
        - Frame size is *f*
        - Number of frames per hyperperoid is $F = H/f$
    - Frame sizes have the following constraints:
        1. A job/instance must fit into a frame, so:  
        $$f \geq \max_{1 \leq i \leq n} e_i$$  
        for all tasks
            - We do this so non-preemptive tasks can finish executing within a single frame
        2. *H* must be evenly divided by *f*, so there are an integer number of frames in the hyperperiod
            - Done to keep the cyclic schedule table size small
        3. *f* should be sufficiently small, so that there should be a complete frame between the release and the deadline of every task
            - Done to schedule the task before the deadline is missing
            - This is also given by the equation for each task *i*:  
            $$2f - gcd(P_i, f) \leq D_i$$ 
            - Remember that in a two tuple, $D_i = P_i$ 
    - Back to the first example, what value should *f* take?
        - C1: $f \geq 2$
        - C2: *f* can be 1, 2, 4, 5, 10, or 20
        - C3: Only 2 works  
            - For 1:  
            $$2f - gcd(4, f) \leq 4$$  
            $$2(2) - 2 = 2$$  
            $$2(4) - 4 = 4$$  
            $$2(5) - 1 = 9$$  
            Therefore 2 and 4 works
            - For 2:  
            $$2f - gcd(5, f) \leq 5$$  
            $$2(2) - 1 = 3$$  
            $$2(4) - 1 = 7$$  
            Therefore only 2 works
            - You can go through the other numbers for completeness but we already know 2 is the only working frame size fo this problem by these two
        - Layout:  
        ![img]({{ site.url }}/assets/4aa4/fval.png)
- Task Slices
    - What if frame size constraints conflict/cannot be met?
        - Example being T = (4,1),(5,2,7),(20,5)
        - C1: $f \geq 5$
        - C2: $f \leq 4$
    - Solution is to "slice" a task into smaller sub tasks, ex:  
    $$T_3 = (20,5) := T_{31} = (20, 1), T_{32}=(20, 3), T_{33}=(20, 1)$$
    - Q: Why not split $T_3$ into $T_{31} = (20, 2), T_{32}=(20, 3)$
        - ANS:  
        $T_1$ with a period of 4 must be scheduled in each frame of size 4  
        $T_2$ with a period of 5 must be scheduled in 4 out of 5 frames  
        This leaves only 1 frame with 3 units of time for T3, other frames have only 1 unit of time and cannot have a job w/ execution time of 2
        - Splitting tasks is a pain and can be very error prone, so try to avoid it 
- Cycle Scheduling Decision Summary
    - Three decisions:
        1. Choose a frame size (consider the 3 constraints)
        2. If a suitable frame size was not found, break constraint 1 to select a smaller frame size to satisfy C2 and C3, and partition tasks into slices
        3. Place jobs/slices into frames
    - In general, these decisions are not independent
    - Try to partition a job into as few slices as necessary
Network Flow Problem
- Given a graph of links, each with a fixed capacity, determine the maximum flow through the network
    - Efficient algorithms exist to get the max flow
- How to derive frame size and schedule meeting all constraints?
    - Reduce it to a network flow problem
    - Use constraints to compute all possible frame sizes
    - For each possible size, try to find a schedule using **network flow algorithm**
        - If max flow has the same value as total execution time of all instances:  
        A schedule is found and we're done
        - Otherwise, a schedule is not found and we need to look at the next frame size
        - If no frame size works, system is not schedulable using cyclic executive
Flow Graph
- Denote all jobs in hyperperiod of F frames as $J_1, ..., J_n$
- Vertices:
    - *N* job vertices $J_1, J_2, ..., J_n$
    - *F* frame vertices $1, 2, ..., F$
- Edges:  
    - (Source, $J_i$) with capacity $C_i(e_i)$
        - Encodes jobs' compute requirements
    - ($J_i$, x) with capacity *f*, iff $J_i$ can be scheduled in frame x
        - Encodes periods and deadlines
    - (*f*, sink) with capacity *f*
        - Encodes limited computational capacity in each frame
- How to find a schedule with a flow graph?
    - Maximum attainable flow is:  
    $$\sum_{i=1..N} C_i$$
        - Total amount of computation in the hyperperiod
        - If a max flow is found with this amount then we have a schedule
- If a task is scheduled across multiple frames, we must slide it into subtasks
    - Potentially difficult, but if we don't allow the algorithm to split taks, the problem becomes NP-complete (?)
    - Analogy: Optimal bin packing becomes easy if we can split objects
Cyclic Executive Overview
- **Advantages**
    - Cyclic executives are very simple - you just need a table
        - Table makes the system very predicable, can validate and test with high confidence
    - No race condition/deadlock
    - Task dispatch is very efficient, just a function call
    - Lack of "scheduling anomalies"
- **Disadvantages**
    - Brittle, any change requires a new table to be computed
    - Number of frames (F) could be huge
        - Implies mode changes may have long latency
    - Release times of tasks must be fixed
    - Slicing tasks into smaller units is difficult and error prone
- **Overall**
    - CE is one of the major software architectures for embedded systems
    - Historically, CEs dominate safety-critical systems
    - Simplicity and predictability win
    - However, significant drawbacks (overhead, strong assumptions)
        - Finding a schedule may require significant offline computation
- Priority based on criticality?
    - If we consider hard real-time applications, they are on the same level of importance
- **Rate Monotonic (RM)**: Shorter period tasks get higher priority
- **Deadline Monotonic (DM)**: Tasks with shorter relative deadlines get higher priority
- Both RM and DM...
    - Have good theoretical properties
    - Work well in practice
- RM Assumptions
    - Tasks are running on a uniprocessor system
    - Task are preemptive
    - There is no OS overhead for preemption
- Rate Monotonic (RM) Scheduling Algorithm
    - Static-priority preemptive approach, and one of the most popular algorithms
    - At any time instant, an RM scheduler executes the instance of the ready task that has the highest priority
    - The priority of a task is inversely related to its period, lower period has higher priority
        - If same period, priority selected at random
- Ex: Given the set of periodic tasks:  
$T_1$: 0, 5, 10, 15, 20, ...  
$T_2$: 1, 5, 9, 13, 17, ...  
$T_3$: 2, 22, 42, 62, ...  
    - Can use a 4-tuple to represent each as well:  
    $T_1 (0, 5, 2, 5)$  
    $T_2 (1, 4, 1, 4)$  
    $T_3 (2, 20, 2, 20)$  
- Can RM make the tasks meet their deadlines?  
SOLN:  
The tasks can be displayed as follows:  
![img]({{ site.url }}/assets/4aa4/tsq4.png)
- Difference between CE and RM
    - $T_1(2, 1)$ and $T_2(3, 1)$  
    What will happen if an instance (say the second) of $T_1$ is delayed for 1 time unit?
    - With CE (f=1):  
    $T_1, T_2, T_1, T_2, T_1, |$ (no late arrival)  
    $T_1 T_2 ? T_2 T_1 |$ (deadline missed when 2nd instance of $T_1$ is late)
    - With RM:  
    $T_1, T_2, T_1, T_2, T_1, |$ (no late arrival)  
    $T_1 T_2 | T_1 T_1 T_2$ (all deadlines met when the 2nd instance of $T_1$ is late)
- Schedulability Test
    - Determining if a specific set of tasks satisfying certain criteria can be successfully scheduled (completing execution of every task by its deadline) using a specific scheduler
    - This test is often done @ compile time, before the computer system and its tasks start execution
- Optimal Scheduler
    - One which may fail to meet the deadline of a task, **only if** no other scheduler can meet it
    - Note that "optimal" in real-time scheduling does not mean "fastest average response time" or "shortest average waiting time", aimed at getting tasks to meet their deadlines
- Schedulability Test for RM (Test 1)
    - There are *n* periodic processes, independent and preemptable
    - $D_i \geq p_i$ for all processes
    - Periods of all processes are integer multiples of each other
    - A **necessary and sufficient condition** for such tasks to be scheduled on a uniprocessor system using RM is the following algorithm:  
    $$U = \sum_{i=1}^n \frac{e_i}{p_i} \leq 1$$
- Schedulability Test for RM (Test 2)
    - If the tasks have arbitrary periods, a *sufficient* but not necessary schedulability condition is:  
    $$U \leq n(2^{1/n} - 1)$$  
    Task sets with a utilization smaller than $n(2^{1/n} - 1)$ are schedulable by RM algorithm
    - For different n values, we get...  
    $$n = 1 \rightarrow U \leq 1$$  
    $$n = 2 \rightarrow U \leq 0.824$$  
    $$n = \inf \rightarrow U \leq 0.693$$  
        - Proof for infinity case is in the slides
- Schedulability Test for RM (Test 3)
    - A *sufficient and necessary* condition for scheduability by RM algorithm can be derived as follows:
        - Consider a set of tasks $(T_1; T_2; ...; T_i)$, with $(p_1 \lt p_2 \lt ... \lt p_i)$
        - __Assume all tasks are in phase__
        - The moment $T_1$ is released, the processor will interrupt anything else it is doing and start processing this task as it has the highest priority (lowest period). Therefore the only condition that must be satisfied to ensure that $T_1$ can be feasibly scheduled is that:  
        $$e_1 \leq p_1$$
        - This is clearly a necessary and sufficient condition
        - The task $T_2$ will be executed successfully if its first iteration can find enough time over the time interval $(0; p_2)$, that is not used by $T_1$ 
            -$p_2$ is the period of $T_2$ and the first instance of $T_2$ must complete before the second instance arrives
        - Suppose $T_2$ finishes at *t*. The total number of instances of task $T_1$ released over the time interval $[0; t)$ is $[\frac{t}{p_1}]$
        - If $T_2$ is to finish at *t*, then every instance of task $T_1$, released during time interval $(0; t)$, must be completed, and in addition, there must be $e_2$ time available for execution of $T_2$, ie. the following condition must be satisfied:  
        $$t = [\frac{t}{p_1}e_1 + e_2]$$ 
- How do we find such *t* for $T_2$?
    - Note: Every interval has an infinite number of points, so we cannot exhaustively check for all possbible *t*
    - Consider $[\frac{t}{p_1}]$, it only changes at multiples of $p_1$, with jumps of $e_1$. So if we can find an integer k, such that time  
    $$t = k * p_1 \gt k * e_1 + e_2$$  
    and,  
    $$k*p_1 \leq p_2$$  
    we have the necessary and sufficient condition for $T_2$ to be schedulable under the RM algorithm
- Consider task $T_3$
    - It is sufficient to show that the first instance of $T_3$ completes before the arrival of its next instance at $p_3$. If $T_3$ completes its execution by *t*, then by an argument similar to that for $T_2$, we must have:  
    $$t = [\frac{t}{p_1}]e_1 + [\frac{t}{p_2}]e_2 + e_3$$  
    - $T_3$ is schedulable iff there is some $t \in (0; p_3)$ such that the above condition is satisfied
    - Also remember that the right side of the equation only changes for multiples of $p_1$ and $p_2$
- General Statement for Test 3
    - The time demand function for task $i (1 \leq i \leq n)$ is given as:
    $$\omega_i (t) = \sum_{k=1}^i [\frac{t}{p_k}] e_k \leq t, 0 \leq t \leq p_i$$  
    This holds for **any time instant t** chosen as follows:  
    $$t = k_jp_j, (j=1, ..., i)$$  
    and  
    $$k_j = 1, ..., [\frac{p_i}{p_j}]$$  
    iff the task $T_i$ is RM-schedulable