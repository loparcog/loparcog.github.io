---
layout: post
title:  "4HC3 - Lecture 31"
date:   2020-11-27 11:30:00 -0400
categories: 4HC3
---

HCI Research Continued
===

Tasks
- After we give participants the interface we'll ask them to do something with it (tasks)
- Should be **representative** of real-world usage
    - Improves external validity
- Tasks should **discriminate** between test conditions
    - When we alter the independent variables, the tasks should be abler to provide meaningful evidence
- When we make tasks more "real-world", it can come at the cost of internal validity
    - Tasks more in the real world may involve the user pausing to think, or doing things outside the interface and then coming back to it
    - Better reflects real-world usage, improving external validity
    - Exactly "what we are measuring" becomes less clear by the introduction of these additional variables
    - Usability testing suffers from the same problem

Participants
- For results to apply to a population, the sample needs to come from that population so it reflects that population
    - eg. randomly selecting participants from a population
    - Otherwise, how can results be generalized
    - Many, many studies that reflect undergraduate student populations
- Ensuring participants and randomly selected from a population is important, how were they recruited?
    - Word of mouth is less random than randomly selecting undergraduates to participate
- Can apply statistical tests to results to determine if the results are **statistically significant**
- **Statistical significance** refers to the claim that a result from data generated by testing or experimentation is not likely to occur randomly or by chance, but is instead likely to be attributable to a specific cause
- In order to obtain statistically significant results, we need to have "enough" participants
- Should we then test with large numbers of participants?
    - One problem is cost, it's too time consuming and expensive
- Another problem is that if so many participants are required to find statistical significance, then the difference is likely very small
    - And if the difference is very small, do we even care about it anyways, even if the result is statistically significant?
- A general rule of thumb is to use similar numbers of participants as in similar experiments
    - We can also compute from statistical tests what number of participants is likely to produce statistically significant results if they are to be found

Questionnaires/Surveys
- Can collect data from users at any point during the experiment
- Questionnaires given before completing tasks typically collect demographic information and data about prior experiences
- Questionnaires given after completing tasks typically collect data related to the task
    - Likert scales can be used to get quantitative data
    - Open-ended questions can be used to get qualitative data

Within-Subjects vs Between-Subjects
- **Within-subjects** experiment design occurs when every participant is tested on each level of a factor
- **Between-subjects** experiment design occurs when a separate group of participants is used for each level of a factor
- Sometimes we don't have a choice between the two (eg. testing between right handedness and left handedness requires between-subjects)
- Between-subjects requires more participants, non-trivial cost to recruit and run sessions
- Within-subjects requires more time per participant, may need to compensate for time
- Variance of predispositions of participants will be the same for each test condition in within-subjects design
    - Predispositions are things like background knowledge, personality, etc.
    - In between-subjects design, the different groups will have different predispositions which could effect outcomes
- Between-subjects designs depend on randomly assigning participants to groups in order to balance group characteristics
    - In within-subjects design, this is unnecessary
- Within-subjects design tends to be more common in HCI studies
- It is possible to blend the two designs
    - eg. keyboard vs mouse controls between-subjects, and software A vs software B within-subjects
- **Order effects** occur in within-subjects design when the order of the interfaces alters the performance of the participants
    - **Counterbalancing** orders test conditions to compensate for other effects (eg swapping order for half of participants)
    - **Latin squares** are $n* n$ tables where each of *n* symbols occurs only once in each column and each row
        - Balancing these ensure that each condition precedes and follows the other conditions an equal number of times (requires an even number of conditions)
        - If we have an odd number of test levels, we can instead avoid a square and use all $n!$ combinations instead
    - The number of levels of the factor must divide equally into the number of participants
        - If we have counterbalanced successfully and the learning effect is the same from condition to condition, we would expect mean performance levels to be the same across groups receiving different orders, despite different performance means per test condition
- **Learning effects** can occur where a participant does better on subsequent test conditions than prior test conditions due to learning that occurs during the prior test conditions
- **Fatigue effects** can occur when the participant's performance worsens across test conditions due to fatigue

Group Effects
- **Asymmetric skill transfer** can occur when the learning effect for one order of testing is different than the other orders of testing (eg. A before B improved a lot better than B before A)
- **Group effects** occur when the mean value for a dependent variable is different between groups
    - Can occur due to asymmetric skill transfer
- **Longitudinal study** involves repeated observations over a prolonged period of time
    - Can be used to track improvements in performance as practice is accumulated with the interface
    - Can be used to test new techniques against conventional techniques to determine the cross-over point (if any) in performance
        - Can use this to estimate whether it is worthwhile to switch techniques
    
Conducting Experiment Sessions
- Experimenter needs to conduct tests professionally
- Needs to understand and follow procedure
- Needs to ensure participants are consenting
- Needs to make sure they don't bias results by indicating what they would like or expect to see
    - Participants are smart, and people naturally like to please others
- Result analysis
    - Results can be reported similar to usability tests
    - Experiments generally conducted with more participants than usability tests
        - Averages, medians, min/max and std dev, all become more interesting data points
        - Graphs/charts to visualize information become more relevant
    - With more participants and experiment methodology, we can potentially have powerful results

Statistical Hypothesis Testing
- Can be used to determine if a result is statistically significant
    - They effectively allow us to say that a result was not likely due to chance, with some degree of confidence
- Very important that we choose the variables, test and confidence levels in advance