---
layout: post
title:  "3Y03 - Midterm 1 Cheat Sheet"
date:   2020-09-29 21:00:00 -0400
categories: 3Y03
---

Probability Theory
===

Experiments & Spaces
- Anything that produces data is an **experiment**
- A *random* experiment can produce different outcomes even when repeated in the same conditions
    - A **sample space**, *S*, is the set of all outcomes of a random experiment
    - A sample space is continuous if it contains real numbers in its range, otherwise it is discrete
- An **event** is a subset of the sample space (like for flipping 3 coins, the outcomes that have two or more heads is an event)
- Given three events $E, E_1, E_2$ from a sample space *S*, then:
    - The **union**,  
    *$E_1 \cup E_2$ := {x $\in$ $S$ : x $\in E_1$ or x $\in E_2$}*  
    the **intersection**  
    *$E_1 \cap E_2$ := {x $\in$ $S$ : x $\in E_1$ and x $\in E_2$}*  
    and the **complement**  
    *$E'$ = {x $\in$ $S$ : x $\notin$ $E$}*  
    are all events as well
- Also some interesting things to note:
    - If $E$ $\subseteq$ $S$ is any event, then $E$ $\cup$ $E'$ = $S$ and $E$ $\cap$ $E'$ = $\emptyset$
    - Both $S$ and $\emptyset$ are also events, and $S'$ = $\emptyset$ and vice versa
    - (A')' = A
    - Distributivity:
        - (A $\cup$ B) $\cap$ C = (A $\cap$ C) $\cup$ (B $\cap$ C) 
        - (A $\cap$ B) $\cup$ C = (A $\cup$ C) $\cap$ (B $\cup$ C)
    - DeMorgan's Laws:
        - (A $\cup$ B)' = A' $\cap$ B'
        - (A $\cap$ B)' = A' $\cup$ B'
- Two events are **mutually exclusive** if they cannot happen simultaneously, or in mathematical terms, $E_1 \cap E_2 = \emptyset$

Permutations & Combinations
- Suppose we have *r* experiments, and the *i*th experiment has $n_i$-many possible outcomes. Then, the total number of outcomes for running all experiments consecutively is:  

$$\prod_{i=1}^{r} n_i = n_1 n_2 ... n_{r-1} n_r$$

- A **permutation** calculates the number of ways we can arrange a given set of data w.r.t order
- Given n distinct objects, the number of ways to **permutate** them is:

$$n! = n \cdot (n-1)\cdot(n-2)\cdot...\cdot3\cdot2\cdot1$$

- By similar reasoning, we can make a more general counting technique for permutations. Given a set of n distinct objects, the number of ways to permutate r $\leq$ n of them is:

$$P_{r}^{n} = nPr = n \cdot (n-1) \cdot ... \cdot (n - r + 1) = \frac{n!}{(n-r)!}$$

- An example for this would be all three letter words with no repeated letters, permutating 3 choices from a choice of 26, which would be $P_3^{26} = 26 \cdot 25 \cdot 24$
- For permutating a set of data with $n = n_1 + ... + n_r$ many objects and $n_i$ identical many objects of type *i*, the number of unique permutations of the object is:  

$$\frac{n!}{n_1!n_2!...n_r!}$$

- A **combination** calculates the number of ways we can get a subset of a set of data without any regard to order
- Given a group of *n* distinct objects, the number of ways to choose $r \leq n$ of them is:

$$C_r^n = nCr = {n \choose x} = \frac{n(n-1)...(n-r+1)}{(n-r)!}=\frac{n!}{(n-r)!r!}$$

- The numbers ${n \choose x}$ are called **binomial coefficients**

Axioms of Probability
- Probability is used to quantify likelihood/chance that an outcome of a random experiment will occur
- Whenever a sample space consists of *N* possible outcomes that are all equally likely, the probability of each outcome is $1/N$
- In a discrete space, the probability of event *E*, denoted as $P(E)$, is the sum of the probabilities of the outcomes in *E*
- A probability must satisfy the following properties:
    - $P(S) = 1$, if S is the sample space
    - $0 \le P(E) \le 1$ for any event *E*
    - For two events, if $E_1 \cap E_2 = \emptyset$, then $P(E_1 \cup E_2) = P(E_1) + P(E_2)$
        - Vice versa is true for this
- Also note the following axioms:
    - $P(\emptyset) = 0$
    - $P(E') = 1 - P(E)$
- The probability of a union is denoted as:  

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

- If they are mutually exclusive, the intersection would be 0, so you could just add their separate probabilities
- For three or more events, the following equations can be used:  

$$P(A \cup B \cup C) = P[(A \cup B) \cup C] = P(A \cup B) + P(C) - P[(A \cup B) \cap C]$$
  
$$P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$$
- The conditional probability of event B given event A is denoted as:  

$$P(B \| A) = \frac{P(A \cap B)}{P(A)}$$, for P(A) \> 0

- Conditional probabilities can also be turned into an intersection given the multiplication rule:  
$P(A \cap B) = P(B \| A)P(A) = P(A \| B)P(B)$
- For any events *A* and *B*, **totality** can be shown given:  

$$P(B) = P(B \cap A) + P (B \cap A') = P(B \| A)P(A) + P(B \| A')P(A')$$

- Two statements are also independent if any of the following is true:  
$P(A \| B) = P(A)$  
$P(B \| A) = P(B)$  
$P(A \cap B) = P(A) P(B)$
    - Furthermore any number of events are independent if:  
    $P(E_1 \cap E_2 \cap ... \cap E_n) = P(E_1) * P(E_2) * ... * P(E_n)$

Random Variables & Distributions
===

Random Variables and Associated Functions
- A **random variable** is a function that assigns a real number to each outcome in a sample space of a random experiment
    - Denoted by an uppercase letter, such as *X*. After an experiment is conducted, the measured value of the random variable is denoted by a lowercase letter, such as *x* = 70
    - A discrete random variable is a random variable with a finite (or countably infinite) range
        - Ex. number of scratches on a surface, proportion of defective parts among 1000 tested
    - A continuous random variable is a random variable with an interval (finite or infinite) of real numbers for its range
        - Ex. Electrical current, length, pressure, temp, time
- The **probability distribution** of *X* is a description of the probabilities associated w/ the possible values of *X*. For a discrete random var, the distribution is often specified by just a list of the possible values along with their probabilities
- **Probability Mass Function**
    - For a discrete random variable *X* with possible values $x_1, x_2, ..., x_n$, a *probability mass function* (PMF) is a function such that:
        1. $f(x_i) \geq 0$
        2. $\sum^n_{i=1} f(x_i) = 1$
        3. $f(x_i) = P(X = x_i)$
- **Cumulative Distribution Function**
    - The cumulative distribution function of X is a function $F: \mathbb{R} \rightarrow [0,1]$ such that, for all $x \in \mathbb{R}$:  
    $F(x) = P(X \leq x)$
    - The cumulative distribution function (cdf) of a DRV *X* is another way to specify the distribution of *X*
- Suppose *X* is a DRV with range {$x_1, x_2, x_3, ...$}, pmf $f(x)$, and cdf $F(x)$. Then $F(x)$ satisfies the following properties:
    1. For all $x \in \mathbb{R}, F(x) = P(X \leq x) = \sum_{x_i \leq x} f(x_i)$
    2. For all $x \in \mathbb{R}, 0 \leq F(x) \leq 1$
    3. For all $x, y \in \mathbb{R}, \text{ if } x \leq y, \text{ then } F(x) \leq F(y)$
- Note, if $f(x)$ and $F(X)$ are the pmf and cdf of a DRV *X* with range {$x_1, x_2, x_3, ...$}, then they are "inter-definabile", in the sense that:  

$$F(x_n) = \sum_{i \leq n} f(x_i)$$

and  

$$f(x_n) = F(x_n) - F(x_{n-1})$$

- Also note, $\lim_{n \rightarrow \inf} F(n) = 1$
- The **mean/expected value** is the average outcome, or the value of the random variable we expect to see most often:  

$$\mu = E(X) := \sum_{i=1} x_i f(x_i)$$

- The **variance** is a measure of how spread out the distribution is, and the square root of the variance is normalized to be the **standard deviation**. The variance is given as follows:

$$\sigma^2 = V(X) := E[(X - \mu)^2] = \sum_{i-1} (x_i - \mu)^2 f(x_i)$$

- Note, the variance can also be given as:  

$$V(X) = \sum_{i=1} x_i^2 f(x_i) - \mu^2 = E(X^2) - E(X)^2$$

- If *X* is a DRV with range {$x_1, x_2, ...$} and pmf $f(x)$, and $h: \mathbb{R} \rightarrow \mathbb{R}$ is any function, then the composition $h(X)$ is also a DRV with range {$h(x_1), h(x_2), ...$} (note h(x_i) does not need to be distinct). Furthermore, the mean and variance can be computed as follows:  
$E(h(X)) = \sum_{i=1} h(x_i) f(x_i)$  
$V(h(X)) = \sum_{i=1} [h(x_i) -  E(h(x_i))]^2 f(x)$
    - A notable case of this is when $h(x) = ax+b$ is linear. Then:  
    $E(h(X)) = aE(X) + b$  
    $V(h(X)) = a^2 V(X)$
    - In general though, $E(h(X)) \neq h(E(X))$

Distributions
- **Binomial Distribution**
    - An experiment with two possible outcomes (success and failure) is called a "Bernoulli Trial"
    - If a Bernoulli trial is successful with probability *p*, then it fails with probability 1-*p*
    - Let *X* be be the number of successful outcomes in *n*-many independent Bernoulli trials
        - We say *X* is a **binomial random variable** with parameters *p* and *n*, sometimes writing as $X ~ Bin(n,p)$
    - If $X ~ Bin(n,p)$ is a binomial random variable, then the functions are given as follows
        - pmf:  
        $f(x) = \binom{n}{x} p^x (1-p)^{n-x}$  
        (Where range of X is x=1, 2, ..., n-1, n)
        - avg:  
        $E(X) = np$
        - var:  
        $Var(X) = np(1-p)$
- **Geometric Distribution**
    - Related to the binomial distribution
    - Running a Bernoulli trial, with success probability of *p*. Let *X* be the number of trials we need to run until a successful trial occurs
        - Then *X* is a **geometric random variable** with parameter *p*
        - The interpretation of pmf $f(n)$ for a geometric random variable is the probability that there are *n*-1 many failures (probability = 1-*p*), followed by a success (probability = *p*)
    - If *X* is a geometric random variable, then the functions are given as follows
        - pmf:  
        $f(x) = (1-p)^{x-1} p$  
        (Where x = 0, 1, 2, 3, ...)
        - avg:  
        $E(X) = \frac{1}{p}$
        - var:  
        $V(X) = \frac{1-p}{p^2}$
- **Inverse Binomial Distribution**
    - Generalization of the geometric distribution
    - Suppose we have a Bernoulli trial with probability *p* of success, and let $r \geq 1$ be an integer
        - Let *X* be the number of trials we need to run until we reach *r*-many successful trials
        - Given this, *X* is an **inverse binomial random variable** with parameters *p* and *r*
            - A geometric random variable is an inverse binomial random variable with r=1
    - Suppose *X* is an inverse binomial random variable, then the functions are given as follows
        - pmf:  
        $f(x) = \binom{x-1}{r-1}(1-p)^{x-r} p^r$  
        (Where $x=r, r+1, r+2, ...$)
            - Also, if $X_i$ is the number of trials required to get the *i*-th success, then $X_i$ is geometric, and $X = X_1 + X_2 + ... + X_r$
        - avg:
        $E(X) = \frac{r}{p}$
        - var:
        $V(X) = \frac{r(1-p)}{p^2}$