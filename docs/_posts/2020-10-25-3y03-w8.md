---
layout: post
title:  "3Y03 - Week 8 Notes"
date:   2020-10-25 12:00:00 -0400
categories: 3Y03
---

Probability Plots
- Probability plots are an empirical way to determine if data fits a particular distribution
- Suppose $S = (x_1, ..., x_n) \subset \mathbb{R}$ is a sample of some data
- Imagine we have a random variable $X$ and we want to know if the distribution of $X$ fits the observed data
- Let $F(x)$ be the cdf of the function
1. Arrange the data points in increasing order, and rename them if necessary:  
$$(x_1, ..., x_n) \rightarrow x_{(1)} \leq x_{(2)} \leq ... \leq x_{(n)}$$
    - Note: $x_{(i)} \approx 100(i/n)$-th percentile. In particular, we want $x_{(n/2)} \approx$ the median
2. For each $1 \leq i \leq n$, choose a value $y_i \in \mathbb{R}$ such that:  
$$F(y_i) = P(X \leq y_i) \approx \frac{i - 0.5}{n}$$
    - The 0.5 is a **correction factor**
3. Plot the pairs $(x_{(i)}, y_i)$
4. Draw a line of best fit
- **Conclusion**: If all the points lay on or near the line, we can conclude that the distribution of $X$ fits the data well. Otherwise, the fit is not so good
    - Most of the time, we will be concerned with looking if the data fits a/the standard normal distribution, $F(z) = \phi(z)$

Point Estimation of Parameters
- The general goal of statistical inference is to make predictions about some population, especially based on limited data
- A major component of statistical inference is called **parameter estimation**
- A **parameter**, $\theta$, is any numerical feature of a population/data set
    - For example, we may want to estimate the mean or variance of a population
    - Recall that a *random sample* (of a variable $X$) is a sequence of independent and identically distributed random variables $(X_1, ..., X_n)$ (with the same distribution of $X$)
    - A tuple of numbers $(x_1, ..., x_n)$ that are particular values of our random sample is called data
- A (sample) statistic is any function of a random sample
    - Ex: $\bar{X} = (1/n)(X_1 + ... + X_n)$ (sample mean)  
    $S^2$ (sample variance)  
    $S$ (sample standard deviation)
- Given a particular parameter, $\theta$, an **estimator** for $\theta$ is a sample statistic:  
$$\hat{\Theta} = h(X_1, ..., X_n)$$  
which we want to use to estimate $\theta$
- If $\hat{\Theta} = h(X_1, ..., X_n)$ is an estimator for $\theta$, and $(x_1, x_2, ..., x_n)$ is some data, then the number $\hat{\theta} = h(x_1, ..., x_n)$ is called a **point estimate** for $\theta$

Sample Distributions and the Central Limit Theorem
- If $Y = h(X_1, ..., X_n)$ is a statistic, then $Y$ is also a random variable
- The distribution associated to such a statistic is called a **sampling distribution**
    - For example, if $X$ is a random variable, and $X_1, ..., X_n$ are all independent with the same distribution as $X$, then the distribution of $\bar{X} = (1/n)(X_1 + ... + X_n)$ is a sampling distribution of the mean of $X$ (and so, the evaluation of $\bar{X}$ at a sample $(x_1, ..., x_n)$ is a point estimate of the mean of $X$)
    - A sampling distribution for a RV $X$ depends on many factors, including the sample size, the sample method, the distribution of $X$, etc. Luckily, we have the following amazing result:
- **Central Limit Theorem**: Suppose $X_1, X_2, ..., X_n, X_{n+1}, ...$ is a random sample (IID) taken from a distribution with mean $\mu$ and variance $\sigma^2$, and if:  
$$\bar{X}_n = \frac{1}{n} (X_1 + ... + X_n)$$  
then:  
$$\frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}}$$  
converges in probability to $N(0,1)$
    - By "converges in probability", we mean that for any *z*:  
    $$\lim_{n \to \inf} P(\frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \leq z) = \phi(z)$$
- NOTE: We need for *n* to be large enough to use this computation, usually needing $n \geq 30$. In some cases, if the distribution is ""nice enough"" (eg. symmetric, unimodal), then you can use $n \geq 5$

Unbiased Estimators
- Let $\theta$ be a parameter for some population
- If $\hat{\Theta}$ is an estimator for $\theta$, we define the **bias** of $\hat{\Theta}$ to be the quantity:  
$$E(\hat{\Sigma}) - \sigma$$
    - You can think of the bias as a measure of how far a way an estimator is from being correct, on average
- For any random sample $X_1, ..., X_n$, the average $\bar{X}$ is an unbiased estimator of the mean $\mu$ of $X = X_i$
- Similarly, the sample variance:  
$$S^2 = \frac{\sum_{i=1}^n (X_i - \bar{X})^2}{n-1}$$  
is an unbiased estimator of variance
- However, the sample standard deviation:  
$$S = \sqrt{S^2}$$ is **NOT** an unbiased estimator (but $E(s) - \sigma$ gets small as the sample gets large, and so $S$ is still a pretty good estimator of standard deviation)

Variance of Estimators
- For a given parameter $\theta$, there is not necessarily a unique unbiased estimator of $\theta$
- For example, both the sample mean and the median are unbiased estimators of the mean
    - How should we pick the "best" estimator of a given parameter?
- Since an estimator $\hat{\Theta}$ is itself a random variable, we can take the variance as a measure of the probability "mass" of the estimator away from $E(\hat{\Theta}) = \theta$
- In other words, if $\hat{\Theta}_1$ and $\hat{\Theta}_2$ are two unbiased estimators of $\theta$, and $V(\hat{\Theta}_1) \lt V(\hat{\Theta}_2)$, then (on average) a point estimate for $\theta$ via $\hat{\Theta}_1$ will be closer to $\theta$ than one from $\hat{\Theta}_2$
    - This suggests a method for choosing an estimator: **minimize the variance**!
- Let $\theta$ be a parameter. Then an estimator of $\theta$, $\hat{\Theta}$, with minimum variance is called the **minimum variance unbiased estimator (MVUE)** of $\theta$
    - An important fact is if $X_1, ..., X_n$ is a random sample of a RV $X$, then $\bar{X}$ is the MVUE of E(X)

Estimator Error
- Another way to choose a good estimator is to minimize the error
- Given an estimator $\hat{\Theta}$, the **standard error** of $\hat{\Theta}$ is:  
$$\sigma_{\hat{\Theta}} = \sqrt{V(\hat{\Theta})}$$
- If the standard error contains unknown parameters, they may be estimated and substituted into the standard error to obtain the **estimated standard error ($S_{\hat{\Theta}}$)**

Biased Estimators
- Sometimes, we have no choice but to use a biased estimator (for example, we often use $S$ as an estimator of $\sigma$)
- When using a biased estimator, it is often useful to consider some measure of error. Typically, the following:  
- If $\hat{\Theta}$ is an estimator of a parameter $\theta$, we define the **mean square error** of $\theta$ to be:  
$$MSE(\hat{\Theta}) = E[(\hat{\Theta} - \theta)^2]$$
    - In general, $MSE(\hat{\Theta}) = V(\hat{\Theta}) + (E(\hat{\Theta}) - \theta)^2$, the variance plus the square of the bias