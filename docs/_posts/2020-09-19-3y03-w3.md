---
layout: post
title:  "3Y03 - Week 3 Notes"
date:   2020-09-19 21:00:00 -0400
categories: 3Y03
---

Cumulative Distribution Functions
- Let *X* be a discrete random variable (DRV). The **cumulative distribution function** of X is a function $F: \mathbb{R} \rarr [0,1]$ such that, for all $x \elem \mathbb{R}$:  
$F(x) = P(X \leq x)$
    - The cumulative distribution function (cdf) of a DRV *X* is another way to specify the distribution of *X*
- Suppose *X* is a DRV with range {$x_1, x_2, x_3, ...$}, pmf $f(x)$, and cdf $F(x)$. Then $F(x)$ satisfies the following properties:
    1. For all $x \elem \mathbb{R}, F(x) = P(X \leq x) = \sum_{x_i \leq x} f(x_i)$
    2. For all $x \elem \mathbb{R}, 0 \leq F(x) \leq 1$
    3. For all $x, y \elem \mathbb{R}, \text{ if } x \leq y, \text{ then } F(x) \leq F(y)
- Note, if $f(x)$ and $F(X)$ are the pmf and cdf of a DRV *X* with range {$x_1, x_2, x_3, ...$}, then they are "inter-definabile", in the sense that:  
$F(x_n) = \sum_{i \leq n} f(x_i)$  
and  
$f(x_n) = F(x_n) - F(x_{n-1})
    - Also note, $\lim_{n \rarr \inf} F(n) = 1$

Mean and Variance of a DRV
- The **mean**, or **expected value** is the average outcome, which can sometimes be interpreted as the value of the random variable that we expect most often
- The **variance** is a measure of how "spread out" the distribution is. There is also the square root of variance, which is the **standard deviation**
    - Both mean and variance are commonly used to summarize properties of a random variable, but they do not determine the random variable uniquely
- Let *X* be a DRV with range {$x_1, x_2, x_3, ...$} and pfm $f(x)$
    - The mean of *X* is the average of the outcomes of *X*, multiplied by the probability of each outcome:  
    $\mu = E(X) := \sum_{i=1} x_i f(x_i)$
    - The variance of *X* is defined as:  
    $\sigma^2 = V(X) := E[(X - \mu)^2] = \sum_{i-1} (x_i - \mu)^2 f(x_i)$
        - It can also be shown that:  
        $V(X) = \sum_{i=1} x_i^2 f(x_i) - \mu^2 = E(X^2) - E(X)^2$
- If *X* is a DRV with range {$x_1, x_2, ...$} and pmf $f(x)$, and $h: \mathbb{R} \rarr \mathbb{R}$ is any function, then the composition $h(X)$ is also a DRV with range {$h(x_1), h(x_2), ...$} (note h(x_i) does not need to be distinct). Furthermore, the mean and variance can be computed as follows:  
$E(h(X)) = \sum_{i=1} h(x_i) f(x_i)$  
$V(h(X)) = \sum_{i=1} [h(x_i) -  E(h(x_i))]^2 f(x)$
    - A notable case of this is when $h(x) = ax+b$ is linear. Then:  
    $E(h(X)) = aE(X) + b$  
    $V(h(X)) = a^2 V(X)$
    - In general though, $E(h(X)) \neq h(E(X))$

Distributions
- **Binomial Distribution**
    - An experiment with two possible outcomes (success and failure) is called a "Bernoulli Trial"
    - If a Bernoulli trial is successful with probability *p*, then it fails with probability 1-*p*
    - Let *X* be be the number of successful outcomes in *n*-many independent Bernoulli trials
        - We say *X* is a **binomial random variable** with parameters *p* and *n*, sometimes writing as $X ~ Bin(n,p)$
    - If $X ~ Bin(n,p)$ is a binomial random variable, then the functions are given as follows
        - pmf:  
        $f(x) = \binom{n}{x} p^x (1-p)^{n-x}$  
        (Where range of X is x=1, 2, ..., n-1, n)
        - avg:  
        $E(X) = np$
        - var:  
        $Var(X) = np(1-p)$
- **Geometric Distribution**
    - Related to the binomial distribution
    - Running a Bernoulli trial, with success probability of *p*. Let *X* be the number of trials we need to run until a successful trial occurs
        - Then *X* is a **geometric random variable** with parameter *p*
        - The interpretation of pmf $f(n)$ for a geometric random variable is the probability that there are *n*-1 many failures (probability = 1-*p*), followed by a success (probability = *p*)
    - If *X* is a geometric random variable, then the functions are given as follows
        - pmf:  
        $f(x) = (1-p)^{x-1} p$  
        (Where x = 0, 1, 2, 3, ...)
        - avg:  
        $E(X) = \frac{1}{p}$
        - var:  
        $V(X) = \frac{1-p}{p^2}$
- **Inverse Binomial Distribution**
    - Generalization of the geometric distribution
    - Suppose we have a Bernoulli trial with probability *p* of success, and let $r \geq 1$ be an integer
        - Let *X* be the number of trials we need to run until we reach *r*-many successful trials
        - Given this, *X* is an **inverse binomial random variable** with parameters *p* and *r*
            - A geometric random variable is an inverse binomial random variable with r=1
    - Suppose *X* is an inverse binomial random variable, then the functions are given as follows
        - pmf:  
        $f(x) = \binom{x-1}{r-1}(1-p)^{x-r} p^r$  
        (Where $x=r, r+1, r+2, ...$)
            - Also, if $X_i$ is the number of trials required to get the *i*-th success, then $X_i$ is geometric, and $X = X_1 + X_2 + ... + X_r$
        - avg:  
        $E(X) = $frac{r}/{p}$
        - var:  
        $V(X) = \frac{r(1-p)}{p^2}$
- **Hypergeometric Distribution**
    - Used for a **hypergeometric experiment**, described as follows:
        - A sample size *n* is randomly selected without replacement from a population of *N* items
        - In the population, *k* items can be classified as successes, and *N*-*k* can be classified as failures
    - Different from binomial because this one does not have a constant probability
    - If *X* is a **hypergeometric random variable**, with *N* being the number of items in the population, *n* being the number of items taken in the sample, *k* being the number of "success" objects in the population, and *x* being the number of "success" objects in the sample,  the functions are given as follows
        - pmf:  
        $f(x) = \binom{k}{x} \frac{\binom{N-k}{n-x}}{\binom{N}{n}}$
        - avg:  
        $E(X) = np$  
        (Where $p=k/N$)  
        - var:  
        $V(X) = np(1-p)(\frac{N-n}{N-1})$
- **Poisson Distribution**
    - Used to model number of events occurring within a given time interval
        - Can also be used for number of events in a distance, area, volume, any sort of **continuous** domain
    - If *X* is a **poisson random variable**, where *x* is the discrete number of occurrences and $\lambda$ is the average number of events per time interval, then the functions are given as follows
        - pmf:  
        $f(x) = \frac{e^{-\lambda}\lambda^x}{x!}$
        - avg:  
        $E(X) = \lambda$
        - var:  
        $V(X) = \lambda$