---
layout: post
title:  "4C03 - Exam Review 2 (L10-18)"
date:   2021-04-22 11:40:00 -0500
categories: 4C03
---

IN THIS REVIEW:
- Sockets (TCP)
- Transport Layer
- UDP and TCP
- Network Layer Start

L10: Socket Programming
===

Socket Programming
- Goal: Learn how to build client/server applications that communicate using sockets
- **Socket**: Door between application process and end-to-end transport protocol
    ![img]({{ site.url }}/assets/4c03/sock.PNG)
- Two socket types for two transport services
    - **UDP**: Unreliable datagram, **connection-less**
    - **TCP**: Reliable, byte stream-oriented, **connection-oriented**
- Application Example:
    1. Client reads a line of characters (data) from its keyboard and sends the data to the server
    2. The server receives the data and converts characters to uppercase
    3. The server sends the modified data to the client
    4. The client receives the modified data and displays the line on its screen
- With UDP
    - UDP: No "connection" between client and server
        - No handshaking before sending data
        - Sender program explicitly attaches IP destination address and port # to **each packet**
        - Receiver program extracts sender IP address and port # from received packet
    - UDP: Transmitted data may be lost or received out of order
    - Application viewpoint: UDP provides unreliable transfer of groups of bytes ("datagrams") between client and server
    - Client/server socket interaction: UDP
        ![img]({{ site.url }}/assets/4c03/sudp.PNG)
        - Actual python code for client and server in slides

Socket Programming with TCP
- Client must **connect to** server
    - Server process must first be running
    - Server must have created socket (door) that welcomes client's contact
- Client connects to server by:
    - Creating TCP socket, specifying IP address, port number of server process
    - Client TCP establishes connection to server TCP
- When contacted by client, server TCP creates a **new socket** for server process to communicate with that particular client
    - Allows server to talk with multiple clients
    - **Source port** numbers used to distinguish clients
- TCP socket interaction:  
    ![img]({{ site.url }}/assets/4c03/tcpsock.PNG)

UDP and TCP Socket Comparison  
    ![img]({{ site.url }}/assets/4c03/uvt.PNG)

Port Number
- 16-bit: 0 - 65535
- Port numbers are application-layer addressed
- The port numbers in the range from 0 to 1023 are the *well-known ports*, or **system ports**
    - SSH: port 22
    - SMTP: port 25
    - HTTP: port 80
- Port numbers from 1024 to 49151 can be used w/o superuser privileges (sometimes registered by a service, eg. bittorrent 6888 - 6900)
- \> 49151 private ports

Transport Layer
===

Transport Services and Protocol
- Provide **logical communication** between app **processes** running on different hosts
- Transport protocols run in end systems
    - Sender side: Breaks app messages into **segments**, passes to network layer
    - Receiver side: Reassembles segments into messages, passes to app layer
- More than one transport protocol available to apps
    - Internet: TCP and UDP  
        ![img]({{ site.url }}/assets/4c03/ts.PNG)
- Transport vs Network Layer
    - **Network Layer**: Logical communication between hosts
    - **Transport Layer**: Logical communication between processes
        - Relies on, enhances, network layer services

Internet transport-layer protocols
- Reliable, in-order delivery (TCP)
    - Congestion control
    - Flow control
    - Connection setup
- Unreliable, unordered delivery (UDP)
    - No-frills extension of "best-effort" IP
- Services not available to both methods:
    - Delay guarantees
    - Bandwidth guarantees

Multiplexing/Demultiplexing
- Multiplexing at sender host
    - Gather data from multiple sockets, enveloping data with header (later used for demultiplexing)
- Demultiplexing at receiver host
    - Delivering received segments to correct sockets
- Image example:
        ![img]({{ site.url }}/assets/4c03/mdm.PNG)
- How demultiplexing works
    - Host receives IP datagrams
        - Each datagram has a source IP address and destination IP address
        - Each datagram carries 1 transport-layer segment
        - Each segment has source, destination port number
    - Host uses **IP addresses and port numbers** to direct segment to appropriate socket
        - TCP sockets are identified by `(S-IP, D-IP, SP, DP)` 4-tuple
        - UDP sockets are identified by `(D-IP, DP)` 2-tuple

Principles of Reliable Data Transfer Protocols
- Important in applications, transport layer and link layers
- Mechanisms:
    - **Error detection and correction**: A packet is received but may be erroneous
    - **Loss detection and recovery**: A packet is missing
- Design issues:
    - Where to put the functionality?
    - Efficiency; utilization of bandwidth resource (ie. sending a message 1000 times will ensure message is properly sent, but is highly inefficient)  
        **Utilization = (Maximum application data rate)/(Available bandwidth)**
        - We would like to see 100% utilization, but due to overhead and such, utilization will be much lower than 100%
- Error Detection
    - Problem: Detect bit errors in packets (frames)
    - Solution: Add extra bits to each packet
    - Techniques (different use case for each):
        - Parity check
        - Checksum
        - Other sophisticated coding schemes such as Reed-Solomon code

L11: Transport Layer Design
===

Error Detection
- **Parity Checking**: Add an extra bit to make the line have an odd/even number of 1's
    - If odd parity check, sum up bits mod 2
        - If zero, parity bit is 1, otherwise it is 0
    - If even parity check, sum up bits mod 1
        - If zero, parity bit is 0, otherwise it is 1
    - The **redundancy** calculation is the number of bits used in full divided by number of bits in the message
        - Eg. using one parity bit for a 7-bit message would be 8/7
        - Can also be looked at for parity messages as $(d+1)/d$
    - Does not have a lot of overhead and redundancy, easy to compute, however cannot detect even number of bit errors and cannot correct any errors
    - Can make it stronger through **two dimensional bit parity**, adding a byte on top of the messages already with parity bits
        - Can detect many more errors, and can even correct single bit errors
        - More redundancy though, for N bits in a message and M messages, redundancy is calculated by $(M+1)(N+1)/(MN)$
        - Values will always work **if using even parity**, can run into issues using odd parity
        - Not really used, checksum more common
- **Internet Checksum**: Send sum of messages and compare results
    - 16-bit one's complement of the one's complement sum of all 16-bit words in the content to be protected
        - Two's complement sum: Summing the numbers (with carries)
        - One's complement sum: Summing the numbers and adding the carry (or carries)
            - Wrap around any carriers to the first bit to keep the message 16-bits long
    - From the sender:
        - Treat contents to be protected as sequence of 16-bit integers
        - Checksum: Addition (1's complement sum) of segment contents
        - Sender puts checksum value into the checksum field
    - From the receiver:
        - Compute checksum of received data
        - Check if computed checksum equals checksum field value:
            - NO: Error detected
            - YES: No error detected, but an error could have occurred
    - **In UDP**, the checksum is computed using the payload and a "pseudo header" that contains some of the same information from the real IP header
        ![img]({{ site.url }}/assets/4c03/hedr.PNG)

Loss Detection
- Causes of packet losses:
    - Buffer overflow
    - Drop after error detection
- Detection methods:
    - At the "crime scene"
    - At the receiver
        - How do I know that I am supposed to get certain data?
- **Loss Recovery**
    - Once packet losses are detected, the source needs to be informed
        - Negative ACK (NACK): "Packet xx is missing" vs...
        - Positive ACK (PACK): "Packet xx is received" -- timeout
    - Source action
        - Should I proceed to transmit the next message w/o the knowledge of the reliable delivery of the current one?
    - Retransmission
        - Retransmit every unacked packet?
        - Selectively retransmit the lost packets only?

L12: Transfer Control
===

Flow Control
- In internet terms, **flow control** aims to control the rate of the sender not to overwhelm the receiver
    - How is this done?
    - Messaging the senders to let them know how much should be sent at what rate

Congestion Control
- Congestion caused from too many sources sending too much data too fast for the network to handle
    - Different from flow control!
    - Flow control concerns not to overload the receiver
- Can be manifests in
    - Lost packets (buffer overflows at routers)
    - Long delays (queueing in router buffers)

Causes/Costs of Congestion
- **Scenario 1**
    - Two senders, two receivers
    - One router w/ infinite buffer
    - Output link capacity *R*
    - No retransmission  
        ![img]({{ site.url }}/assets/4c03/s11.PNG)
    - This scenario would then have the following properties:
        - Maximum **per-connection** throughput of $R/2$
            - $\lambda_{out}$ is limited in the router since it is going to both Host C and D, so that is why it is limited to $R/2$, rather than $R$
        - Large delays as arrival rate, $\lambda_{in}$, approaches capacity  
            ![img]({{ site.url }}/assets/4c03/s12.PNG)
- **Scenario 2**
    - One router, **finite** buffers
    - If a packet is lost at router due to a full buffer, the sender retransmits the lost packet
        - Application-layer has $\lambda_{in} = \lambda_{out}$, however, transport-layer includes retransmissions, making $\lambda_{in}' \geq \lambda_{in}$
        - $\lambda_{in}'$ is the original data, plus any retransmitted data  
            ![img]({{ site.url }}/assets/4c03/s21.PNG)
    - Realistic: When $\lambda_{in}$ approaches $R/2$, some packets are retransmitted with duplicates and are delivered  
        ![img]({{ site.url }}/assets/4c03/s22.PNG)
    - This would make the following costs:
        - More work (retransmits) for a given "good output"
        - Unneeded retransmissions would mean that a link would carry multiple copies of a packet
- **Scenario 3**
    - Q: What happens as $\lambda_{in}'$ and $\lambda_{in}$ increase?  
        ![img]({{ site.url }}/assets/4c03/s31.PNG)
        - Every connection here now has two routers to go through to reach their destinations, also known as **"multihop paths"**
    - The cost here would be when a packet is dropped, any "upstream" transmission capacity used for that packet will be wasted
        - As $\lambda_{in}'$ increases, $\lambda_{out}$ will also increase to a point, and then drop off  
            ![img]({{ site.url }}/assets/4c03/s32.PNG)

Approaches Towards Congestion Control
- Two broad approaches towards congestion control
    1. **End-end congestion control**
        - No explicit feedback from network
        - Congestion inferred from end-system observed loss and delay
        - Approach taken by TCP
    2. **Network-assisted congestion control**
        - Routers provide feedback to end systems
        - Single bit indicating congestion (SNA, DCbit, TCP/IP, ECN, ATM)
        - Explicit rate sender should send at
- Bag of tricks
    ![img]({{ site.url }}/assets/4c03/bot.PNG)

UDP and TCP
- Understanding the protocol details of UDP and TCP
    - Header format
    - TCP state machine
        - Connection setup and tear-down
    - Sliding window in TCP
    - TCP flow control
    - TCP congestion control
- **UDP: User Datagram Protocol [RFC 768]**
    - "Bare bones" internet transport protocol
    - "Best effort" service, UDP segments may be:
        - Lost
        - Delivered out of order to app
    - **Connectionless**
        - No handshaking between UDP sender and receiver
        - Each UDP segment handled independently of others
    - Why is there a UDP?
        - No connection establishment (less delay)
        - Simple: no connection state at sender/receiver
        - Small segment header
        - No congestion control: UDP can blast away as fast as "desired"
    - Often used for streaming multimedia apps
        - Loss tolerant
        - Rate sensitive
    - Other UDP uses
        - DNS, SNMP
    - Reliable transfer over UDP: Add reliability at application layer
        - Application-specific error recovery
    - Header example  
        ![img]({{ site.url }}/assets/4c03/udp.PNG)

L13: TCP
===

TCP Overview (RFCs: 793, 1122, 1323, 2018, 2581)
- **Point-to-point**: One sender, one receiver
- **Reliable, in-order byte stream**: No "message boundaries"
- **Pipelined**: TCP congestion and flow control set window size
- Send and receive buffers
- **Full duplex data**: Bi-directional data flow in same connection
    - **MSS**: Maximum segment size
- **Connection-oriented**: Handshaking (exchange of control msgs) initiates sender, receiver states before data exchange
- **Flow controlled**: Sender will not overwhelm receiver

TCP Segment Structure
- Header and payload shown here:  
    ![img]({{ site.url }}/assets/4c03/tcps.PNG)
    - Each row is 4 bytes, so the total minimum header length (header w/o options) is 20 bytes, actual length depends on the Options sent
    - `Acknowledgement Number` used to mark any acknowledgement one side may be sending to the other, whereas `Sequence Number` is the bytes of actual application data being sent
    - `U, A, P, R, S, F` are all flags

TCP Segments
- TCP "Stream of Bytes" service
- TCP **segment**
    - No more than **Maximum segment size (MSS)** bytes
    - Segment sent when segment full (MSS) or "pushed" by application
- Ex: MSS = 100 bytes, data received from application  
    ![img]({{ site.url }}/assets/4c03/seg.PNG)

TCP Sequence Numbers, ACK
- **Sequence numbers**: Starting byte offset of data carried in the segment
- **ACKs** ("What Byte is Next"): Gives sequence number just beyond highest sequence number received **in order**
- Ex. Sequence number = 1001. Sender sends 500 bytes, so receiver acknowledges with ACK number...
    1. 501
    2. 1002
    3. 1500
    4. **1501**
    5. 1502
    - Current sequence goes from 1001 to 1500, so next will be 1501
- Next sequence to send by sender is...
    1. 1500
    2. 1501
    3. **1501**

Establishing Connection
- **Three-Way Handshake**
    - Each side notifies other of starting sequence number it will use for sending
    - Each side acknowledges other's sequence number
        - **SYN-ACK**: Acknowledge sequence number + 1
    - The third segment may piggyback some data  
        ![img]({{ site.url }}/assets/4c03/ec.PNG)
    - Called three-way handshake since three messages are involved
- TCP State Diagram: Connection Setup
    ![img]({{ site.url }}/assets/4c03/tcpsd.PNG)
    - Diagram transitions shown as action on top and message below
    - HTTP is stateless so it doesn't need something like this, but TCP does
    - Important to differentiate from client to server
        - Client follows right path down, while server follows left

Tearing Down Connection
- Either side can initiate tear down
    - Send FIN signal
    - "I'm not going to send any more data"
- Other side can continue sending data
    - Half open communication
    - Must continue to acknowledge
- Acknowledge FIN
    - Acknowledge last sequence number + 1
- Diagram here  
    ![img]({{ site.url }}/assets/4c03/td.PNG)

L14: TCP Cont.
===

Connection Tear-Down State Diagram  
    ![img]({{ site.url }}/assets/4c03/teard.PNG)
- Left path is the client/server that initiated the teardown, called an **active close**
    - Each stop waits for their given ACKs
- The right path is taken by the client/server that did not initiate the teardown, called a **passive close**
- Sequences depend on messages being sent

TCP Window Control  
    ![img]({{ site.url }}/assets/4c03/tcpwc.PNG)
- Helps to manage ordered sending
- This window will move once the leftmost side of the sent data is acknowledged by the receiver

TCP Reliable Data Transfer
- TCP creates reliable data transport services on top of IP's unreliable service
    - Implements **pipelined segments**, sends multiple sections without waiting on acknowledgements, sends up to the buffer of the window
- In one round trip time (RTT), a maximum of BW * RTT bits can be sent to fill up the pipe
    - Problem is we don't always know the bottleneck bandwidth, so TCP needs to have a method to estimate this, which is the job of congestion control (talked about in future)
- TCP uses **cumulative ACKs**, giving ACKs back to the sending host whenever a message is received
    - It may be lost if a new packet is sent on top of where the acknowledgement was supposed to be read, but the ACK to the new message would let the sending host know that they received both the new and old segments
    - This makes losses tolerable since they can be recognized
- Also implements **retransmission triggered by timeout**
    - After a packet is sent without acknowledgement for the packet, indicating a possibility of a lost packet, the sender can resend the packet
    - Not very efficient because no data is sent while waiting for timeout

L15: TCP Cont. 2
===

TCP Reliable Data Transfer
- **Retransmissions triggered by duplicate acks**
    - Sending the same ACK if a sequence jump occurs, indicating that the ACK'd sequence has been lost 
    ![img]({{ site.url }}/assets/4c03/tcpda.PNG)
- Retransmission timeout issues
    - If the sender hasn't received an ACK by timeout, retransmit the first packet in the window, restart timer
    - How do we pick a timeout value?
        - Base it on the RTT
    - Example of issues:  
        ![img]({{ site.url }}/assets/4c03/tcpto.PNG)
- How do we estimate RTT?
    - **SampleRTT**: Measured time from segment transmission until ACK receipt
        - Ignore retransmissions since we cannot be sure that the ACK was from the first or second transmission
        - Causes a lot of fluctuation in RTT measuring in comparison to estimated value
            - We want something smoother, can look to average several recent measurements, not just the current SampleRTT
    - **Estimated RTT**: Exponential weighted moving average, calculated as:  
        $$(1-\alpha)*\text{EstimatedRTT} + \alpha * \text{SampleRTT}$$
        - Influence of past sample decreases exponentially fast, more data evens it all out
        - Typical value of $\alpha = 0.125$, since we want historical data to weigh more than new data
- So, to set the timeout value...
    - Use the EstimatedRTT plus a "safety margin"
        - Large variation in EstimatedRTT means larger safety margin
    - First estimate of how much SampleRTT deviates from EstimatedRTT:  
        $$DevRTT = (1 - \beta) * \text{DevRTT} + \beta * | \text{SampleRTT} - \text{EstimatedRTT}|$$
        - Typically, $\beta = 0.25$
    - Then set the timeout interval to:  
        $$\text{TimeoutInterval} = \text{EstimatedRTT} + 4*\text{DevRTT}$$
        - Kind of like mean plus 4 times standard deviation, can calculate probability RTT is in the range

TCP Receiver Event/ACK Generation  
    ![img]({{ site.url }}/assets/4c03/tcpre.PNG)

TCP Congestion Control 
- Congestion Detection
    - Loss event: timeout or 3 duplicate acks
    - TCP sender reduces rate (congestion window) after loss event
- Rate adjustment (Probing)
    - Slow start
    - Additive Increase and Multiplicative Decrease (AIMD)
        - Looks like a saw wave of increasing and decreasing, never flatlining
        - Always adapting to behaviors
    - Conservative after timeout events
    - Utilizes packet loss events as evidence for congestion
- Does packet loss equal congestion?

L16: TCP Congestion Control Mechanisms
===

Congestion Window (`cwnd`)
- Limits how much data can be in transit
- Implemented in a number of bytes  
    ![img]({{ site.url }}/assets/4c03/cwnd.PNG)
    - Note that `rwnd` is the receive window, used for the purpose of flow control
    - Using `rwnd` and `cwnd`, we can effectively control the rate which data is sent

Self-Clocking
- If we have a large window, ACKs "self-clock" the data to the rate of the bottleneck link
- Observe: received ACK spacing is around equal to L/bottleneck bandwidth  
    ![img]({{ site.url }}/assets/4c03/sclock.PNG)

Phases of TCP Congestion Control
- Congestion Window (`cwnd`): Initial value is 1 MSS (maximum segment size) counted in bytes
- Slow-start threshold value (`ss_thresh`): Initial value is the receive window size
- **Slow Start** (getting to equilibrium)
    - `cwnd` < `ss_thresh`
    - Want to find this very very fast, not waste any time
- **Congestion Avoidance**
    - `cwnd` >= `ss_thresh`
    - Probe the available bandwidth more gently
    - React to network conditions

TCP: Slow Start
- **Goal**: Discover *roughly* the proper sending rate quickly
- Whenever starting traffic on a new connection, or whenever increasing traffic after congestion (timeout) was experienced:
    - Initial `cwnd`= 1MSS
    - Each time a segment is acknowledged, increment `cwnd` by one MSS
- Continue until...
    - Reach `ss_thresh`
    - Packet loss
- Illustration:  
    ![img]({{ site.url }}/assets/4c03/ssill.PNG)
    - The congestion window size grows very rapidly
    - TCP slows down the increase of `cwnd` when `cwnd` >= `ss_thresh`
- Observe:
    - Each ACK generates two packets
    - Slow start increases rate exponentially fast (doubled every RTT!)

L17: TCP Congestion Control Mechanisms Cont.
===

Congestion Avoidance (After Slow Start)
- Slow Start figures out roughly the rate at which the network starts getting congested
- **Congestion Avoidance** continues to react to network conditions
    - Probes for more bandwidth, increase `cwnd` if more bandwidth available
    - If congestion is detected, aggressively cut `cwnd`
    - How?

TCP Multiplicative Decrease and Additive Increase (AIMD)
- **Multiplicative Decrease**: Cut `cwnd` in half after loss event
- **Additive Increase**: Increase `cwnd` by 1 MSS every RTT in the absence of loss events: *probing*  
    ![img]({{ site.url }}/assets/4c03/aimd.PNG)
    - Makes a sawtooth behaviour
- Why do we use AIMD?
    - Why not subtractive decrease or multiplicative increase or anything else?
    - Want to make sure network available bandwidth is being utilized
        - Can have everyone sending at a very slow rate, making sure messages send properly, but not using full possible bandwidth
    - Want to implement *fairness*
        - Can't have a "police force" to manage sending, need to rely on TCP to manage it
        - Note that fairness does not always equal efficiency
        - Ideally want max bandwidth split between all hosts
        - Will increase and decrease connections until they reach "sweet spot"
        - Sweet spot a bit more of an asymptote, not going to be able to get there, but will get really close!
        - Once rates equalize we assume fair share
- Example of slow start and congestion avoidance given in slides
    ![img]({{ site.url }}/assets/4c03/ssca.PNG)

Responses to Congestion (Loss)
- There are algorithms developed for TCP to respond to congestion
    - **TCP Tahoe**: Timeout only, start from slow start
    - **TCP Reno**: Tahoe + fast retransmit and fast recovery
        - Most end hosts today implement TCP Reno
- Many more exist as well
    - TCP Vegas (research: use timing of ACKs to avoid loss)
    - TCP SACK (future deployment: selective ACK)

"Sawtooth Behaviour" - TCP Tahoe
- Upon timeout, cut `ss_thresh` by 1/2 and `cwnd` = 1 MSS
- Go to slow start phase  
    ![img]({{ site.url }}/assets/4c03/tcpt.PNG)

TCP Reno
- Problem with Tahoe: If a segment is lost, there is a long wait until timeout
- Reno adds a fast retransmit and fast recovery mechanism
    - Upon receiving 3 duplicate ACKs, retransmit the presumed lost segment (fast retransmit)
    - But do not enter slow-start, instead enter congestion avoidance ("fast recovery")
- Slow start only once per session (if no timeouts)
- In steady state, `cwnd` oscillates around the ideal window size  
    ![img]({{ site.url }}/assets/4c03/tcpr.PNG)

L18: TCP Final Notes & IP Layer
===

Tahoe vs Reno Example
- Initial `ss_thresh` = 8 MSS
- Assume the first two packets are lost in round 8 and there is no other loss after, what is the `ss_thresh` and `cwnd` in round 9?  
    ![img]({{ site.url }}/assets/4c03/trex.PNG)
- Round 9:  
    `ss_thresh` = 12/2 = 6  
    Tahoe `cwnd` = 1  
    Reno `cwnd` = 6  
    ![img]({{ site.url }}/assets/4c03/trex9.PNG)

Final Comparison Table  
    ![img]({{ site.url }}/assets/4c03/tcpcomp.PNG)

Summary
- **Distinguish algorithms and protocols!**
- Multiplexing and demultiplexing
    - Port number
- Error detection/recovery
    - Internet checksum (inclusion of pseudo header)
    - Stop-n-Wait, Go-Back-N, selective repeat
- Connection establishment/termination
- Flow control
- Congestion control
    - Sliding window
    - AIMD
    - Slow start
    - Fast retransmit and recovery

IP Layer
===

Outline
- Intro
- What's inside a router
- IP: Internet Protocol
    - Datagram format
    - IPv4 addressing
    - IPv6
    - ICMP
- Routing algorithms

Network Layer
- Transport segment from sending to receiving hosts
- On sending side, encapsulates segments into **datagrams**
- On receiving side, delivers segments to transport layer
- Network layer protocols in **every** host, router
- Router examines header fields in all IP datagrams passing through it
- Internet is structured as a "network of networks"
    - Messages pass through many networks before being delivered to their end goal  
        ![img]({{ site.url }}/assets/4c03/nwpath.PNG)

IPv4 Address
- **IPv4 Address**: Unique 32-bit number associated with a host, router interface
- Represented with the **dotted-quad** notation (eg. 200.23.16.5)
    - Break down the 32 bits into four parts, each with one byte, making it a bit more human readable  
        ![img]({{ site.url }}/assets/4c03/ipv4.PNG)
- **Interface**: Connection between host/router and physical link
    - Router's typically have *multiple* interfaces, while hosts typically have *one*

Subnets
- 32 bits are partitioned into prefix part and host part  
    ![img]({{ site.url }}/assets/4c03/ipvsub.PNG)
- Interdomain routing operates on the prefix
- Notation: `200.23.16.5/23` represents a subnet with a 23-bit prefix and $2^9$ host addresses

The Internet Network Layer
- Host, router network layer functions  
    ![img]({{ site.url }}/assets/4c03/net.PNG)

Router Architecture Overview
- Two key router functions:
    1. Run routing algorithms/protocols (RIP, OSPF, BGP)
    2. Forwarding datagrams from incoming to outgoing links
- Diagram  
    ![img]({{ site.url }}/assets/4c03/rout.PNG)
- Input Port Functions  
    ![img]({{ site.url }}/assets/4c03/inpp.PNG)

Forwarding Table
- Set of destination address ranges under each link interface
    - Does not have every address, and instead has a range, which each has its own outgoing link interface
    - Reduce complexity and therefore computation time
- **Longest prefix matching**: When looking for a forwarding table entry for a given destination address, use the longest address prefix that matches the destination address  
    ![img]({{ site.url }}/assets/4c03/lpm.PNG)

Switching Fabrics
- **Transfer packet** from input buffer to appropriate output buffer
- **Switching rate**: Rate at which packets can be transferred from input to output
    - Often measured as multiple of input/output line rate
    - *N* inputs: switching rate *N* times line rate desirable
- Three types of switching fabrics:  
    ![img]({{ site.url }}/assets/4c03/sf.PNG)
    - **Memory**: Need to write and read data to memory, not very efficient
    - **Bus**: Transfer data through bus to output, but can only transfer one, not very efficient either
    - **Crossbar (most popular)**: Max throughput of *N* packets, direct connection

Output Ports
- **Buffering** required when datagrams arrive from fabric faster than the transmission rate
- **Scheduling Discipline** chooses among queued datagrams for transmission (not always first one!)
    - Datagrams/packets can be lost due to congestion and/or lack of buffers
    - Priority scheduling - who gets best performance, network neutrality
- Example output port diagram
    ![img]({{ site.url }}/assets/4c03/outport.PNG)

