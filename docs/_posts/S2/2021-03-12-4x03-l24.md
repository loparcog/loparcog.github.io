---
layout: post
title:  "4X03 - Lecture 24"
date:   2021-03-12 11:30:00 -0500
categories: 4X03
---

Intro to Deep Learning
===

Base Diagrams:  
    ![img]({{ site.url }}/assets/4x03/dl1.PNG)

Input (L1)
- Input values to the functions
    - Can be values, noise, etc.

Activation Functions (L2)
- **Sigmoid**: This function is given as:  
    $$\sigma(x) = \frac{1}{1+ e^{-x}}$$
    - Scales to a number from 0 to 1, goes to 1 when very large and 0 when small
    - $\sigma(0) = 0.5$
- Helps to avoid "vanishing gradient" and ensure values stay distinct

Neurons (L3)
- Each neuron takes input from all nodes in L2
- Neuron 1 combines the inputs as follows:
    $$\sigma(w_{11} a_1 + w_{12} a_2 + b_1)$$
    - *w* values are weights attached to the input and *b* is a **bias**
    - Activation function also applied to this also
- Each neuron follows this same formula, having their own weights and biases
- Each of these weights, inputs, and biases can be given as matrices:  
    ![img]({{ site.url }}/assets/4x03/dl2.PNG)
    - Output is now in vector form
- Overall (for this model), this layer takes two inputs and gives three outputs 

Notation
- Denote the input by *x*
- At layer *i*, $w^{[i]}, b^{[i]}$
- Output at layer 2 is:  
    $$\sigma(w^{[2]}x + b^{[2]}) = a^{[2]}$$
- Output at layer 3 is:  
    $$\sigma(w^{[3]}a^{[2]} + b^{[3]}) = a^{[3]}$$
- Output at layer 4 is:  
    $$\sigma(w^{[4]} a^{[3]} + b^{[4]}) = a^{[4]}$$
- Each layer has its own matrix of weights and vector of biases
- These formulas can be recursively shown as well:  
    ![img]({{ site.url }}/assets/4x03/dl3.PNG)
    - All of this is then defined as $F(x)$
- Assume we have determined $w^{[i]}, b^{[i]}$, and we have the following:  
    $$\tilde{x} = \begin{bmatrix} 0.1 \\ 0.5 \end{bmatrix}, F(\tilde{x}) = \begin{bmatrix} 0.4 \\ 0.6 \end{bmatrix} \rightarrow B$$
    - Function result shows classification in group *B*
- The machine is essentially learning the weights and biases
    - ML is about finding these parameters
- To know how many neurons and such is mainly through experimentation
- **Training data** is data where $x^{[i]}$ is given, and $y(x^{[i]})$ has a constant result
    - For example for a classification system, a vector [1 0] if in *A* and a vector [0 1] if in *B*
    - We want the system to output same or almost the same as the actual result
- This system breaks down into a nonlinear least squares problem
- For training of $N=10$, we want to essentially minimize the following:  
    $$\frac{1}{10} \sum_{i=1}^{10} \frac{1}{2} \vert \evrt y(x^{[i]} - F(x^{[i]})) \vert \vert_2^2

Next Time
- Steepest descent
- Stochastic gradient descent