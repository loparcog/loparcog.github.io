---
layout: post
title:  "4E03 - Lecture 28"
date:   2020-11-19 16:30:00 -0400
categories: 4E03
---

Scheduling and the Impact of Variability
===

M/G/1 Queue
- Arrivals follow a Poisson process, rate $\lambda$
- Processing times follow a general distribution, mean $1/\mu$ and second moment $E[S^2]$
- Processing is FCFS
- Performance
    - Suppose that $E[S^2]$ is large
    - Large proportion of jobs that are smaller than $1/\mu$, small proportion of jobs that are significantly larger than $1/\mu$
    - Probability that the processor is busy is still $\rho = \lambda/\mu$, so the probability that an arriving job sees a busy processor is also $\rho$
    - More likely to see a very large job being processed
- Analysis
    - Derivation in book of mean response time
    - Pollaczek-Khinchin formula:  
    $$E[T] = \frac{\lambda E[S^2]}{2(1-\rho)} + \frac{1}{\mu}$$  
    - Performance degrades proportionately with second moment (and hence variance)
    - When $\rho$ is close to 1, the impact of $\rho$ is still the most significant factor
    First term also essentially $E[T_Q]$

Variability Reduction
- System designers usually focus on reducing $1/\mu$ (mean processing time)
- M/G/1 queue: all other things being equal, reducing variability improves performance (can be quite significant)
    - However, there is often a tradeoff - reducing variability often increases mean processing time
    - Can a tradeoff be beneficial? (Spoiler: yes)
- Can rewrite Pollaczek-Khinchin formula as:  
$$E[T] = \sigma_s^2 * \frac{\lambda}{2(1-\rho)} + \frac{1}{\mu} * \frac{2 - \rho}{2(1 - \rho)}$$  
where $\sigma_s^2$ is the variance of the processing times
    - This shows the tradeoff directly, and can be used to calculate differences
- Example on some numbers
    - $\lambda = 100, \rho = 150$  
    - $\sigma_s^2 = \frac{1}{\mu^2}$ (exponentially) yields $E[T] =0.02$
    - Getting the variance down to 0, $\sigma_s^2 = 0$ and $E[T] =0.02$, then $\mu = 131$
        - Bringing the variance down to 0 actually slows down the system
    - Reducing mean processing time can also increase variability too
- Various frameworks for parallel computing/data analytics
    - Variance of individual processing times (response times) can cause issues
- Maximum processing time
    - Completion time is the maximum of the processing times
    - Explicit expression for mean, variance, etc. difficult
    - For $\mu = 1$$ simulations give mean completion times of 1.50 (N = 2), 2.29 (N = 5), etc.
    - If processing times consistant (not random), completion time is always 1, independent of *N*
    - Can show as variance increases, mean completion time increases
- Practical approaches
    - Straggler detection/mitigation
    - Also helps with issues when means are different
- Scheduling
    - Can moving away from FCFS mitigate the impact of variance?
    - Yes, it can!
- **Processor Sharing (PS)** - If there are *n* jobs in the processor, it (virtually) splits itself into *n* processors, each working at rate $\mu/n$
- Idealization of time slicing (let time slices go to zero, no switching overhead)
- Derivation in book of mean response time:  
$$E[T] = \frac{1/\mu}{1-\rho}$$  
$$= \frac{1}{\mu - \lambda}$$
- Implications
    - Mean response time does **not** depend on variance
    - (Very much) preferred if variance is large
    - If processing times less variable than an exponential distribution, FCFS preferred
- Performance
    - If congested, large jobs can only receive a small fraction of processor's capacity
    - Explains why it may perform well with high variance, but does not explain why performance is independent of variance
- **Preemptive LCFS** - Preemptive priority is given to the most recently arrived job
- Mean response time:  
$$E[T] = \frac{1/\mu}{1 - \rho}$$  
- Arriving small jobs do not get stuck behind large jobs
    - Some small jobs may be unlucky and get stuck behind an arriving large job, however the proportion of jobs for which this happens is very small
- Not at all obvious that PS and LCFS should result in the same mean response time