---
layout: post
title:  "3Y03 - Week 7 Notes"
date:   2020-10-18 12:00:00 -0400
categories: 3Y03
---

Independent Random Variables
- Recall two events $A,B$ are said to be independent if:  
$$P(A \cap B) = P(A) P(B)$$
    - This concept will motivate a definition of independence for CRV's
- Let $X,Y$ be CRV's with joint pdf $f_{X,Y}(x,y)$. We say that $X,Y$ are *independent* iff:  
$$f_{X,Y}(x,y) = f_X(x)f_Y(y)$$  
where $f_X$ and $f_Y$ are the marginal density functions of $X$ and $Y$
- If $X, Y$ are independent and jointly distributed on a region $R \subseteq \mathbb{R}^2$, then $R$ must be rectangular ($R = I_1 * I_2$ for intervals $I_1, I_2$) (Also note the converse is false). In this case:  
$$1 = \int \int_{I_1 * I_2} f_{X,Y}(x,y) dx dy$$  
$$= (\int_{I_1} f_X(x) dx)(\int_{I_2} f_Y(y) dy)$$

Covariance
- Suppose $X,Y$ are CRV's that are jointly distributed with joint pdf $f_{X,Y}(x,y)$
- For any "nice" function $h(x,y)$, $h(X,Y)$ is a random variable, and we have the expected value of $h(X,Y)$ is:  
$$E(h(X,Y)) = \int \int_{\mathbb{R}^2} h(x,y) f_{X,Y}(x,y) dx dy$$
    - We can use this to construct a measure of *linear relationship* between $X$ and $Y$
- Given $X, Y$ CRV's with joint pdf $f_{X,Y}(x,y)$, the **covariance** of $X$ and $Y$ is defined to be:  
$$\sigma_{XY} := E(XY) - E(X)E(Y)$$
- Note that if X and Y are independent, then:  
$$E(XY) = E(X)E(Y)$$
- In general, if $X,Y$ are independent, then $\sigma_{XY} = 0$
    - If the covariance is 0 though, it does not imply $X,Y$ are independent, only that there is no linear relationship

Correlation
- The **correlation** between random variables $X,Y$, denoted as $\rho_{X,Y}$, is:  
$$\rho_{X,Y} = \frac{cov(X,Y)}{\sqrt{V(X)V(Y)}} = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}$$
- For any two random variables $X,Y$, $-1 \leq \rho_{XY} \leq 1$
- If the points in the joint probability distribution of $X,Y$ that receive positive probability tend to fall along a line of positive/negative slope, $\rho_{XY}$ is near 1
- Two random variables with nonzero correlation are said to be correlated
- If $X,Y$ are independent random variables, then:  
$$\rho_{XY} = \sigma_{XY} = 0$$

Linear Functions of Random Variables
- **Linear Function**: Given random variables $X_1, X_2, ..., X_p$, and constants $c_0, c_1, c_2, ..., c_p$:  
$$Y = c + 0 + c_1 X_1 + c_2 X_2 + ... + c_p X_p$$  
is a linear function of $X_1, X_2, ..., X_p$
- **Mean of a Linear Function**: If $Y = c + 0 + c_1 X_1 + c_2 X_2 + ... + c_p X_p$, then:  
$$E(Y) = c_0 + c_1 E(X_1) + c_2 E(X_2) + ... + c_p E(X_p)$$
- **Variance of a Linear Function**: If $X_1, X_2, ..., X_p$ are random variables, and $Y = c + 0 + c_1 X_1 + c_2 X_2 + ... + c_p X_p$, then in general:  
$$V(Y) = c_1^2 V(X_1) + c_2^2 V(X_2) + ... + c_p^2 V(X_p) + 2 \sum \sum_{i \lt j} c_i c_j cov(X_i, X_j)$$
    - If $X_1, X_2, ..., X_p$ are independent, then:  
    $$V(Y) = c_1^2 V(X_1) + c_2^2 V(X_2) + ... + c_p^2 V(X_p)$$
- **Mean and Variance of an Average**: If $\bar{X} = (X_1 + X_2 + ... + X_p) / p$, with $E(X_i) = \mu$ for $i = 1,2,...,p$, then:  
$$E(\bar{X}) = \mu$$
    - If $X_1, X_2, ..., X_p$ are also independent with $V(X_i) = \sigma^2$ for $i =1,2,...,p$, then:  
    $$V(\bar{X}) = \frac{\sigma^2}{p}$$
- **Reproductive Property of the Normal Distribution**: If $X_1, X_2, ..., X_p$ are independent normal random variables with $E(X_i) = \mu_i$ and $V(X_i) = \sigma_i^2$, for $i = 1,2,...,p$, then:  
$$Y = c + 0 + c_1X_1 + c_2X_2 + ... + c_pX_p$$  
is a normal random variable with:  
$$E(Y) = c_0 + c_1 \mu_1 + c_2 \mu_2 + ... + c_p \mu_p$$  
and  
$$V(Y) = c_1^2 \sigma_1^2 + c_2^2 \sigma_2^2 + ... + c_p^2 \sigma_p^2$$

Numerical Summaries of Data
**Sample Mean**: If the *n* observations in a sample are denoted by $x_1, x_2, ..., x_n$, the sample mean is:  
$$\bar{x} = \frac{x_1 + x_2 + ... + x_n}{n} = \frac{\sum_{i=1}^n x_i}{n}$$
    - The sample mean is the average value of all observations in the data set. Usually, these data are a sample of observations that have been selected from some larger population of observations
    - Recall, any populations based on future or hypothetical events are conceptual/hypothetical populations, since they don't physically exist
- **Sample Variance and Standard Deviation**: If $x_1, x_2, ..., x_n$ is a sample of *n* observations, the sample variance is:  
$$s^2 = \frac{\sum_{i=1}^n (x_1 - \bar{x})^2}{n-1}$$  
    - The sample standard deviation, $s$, is the positive square root of the sample variance
- **Sample Range**: If the *n* observations in a sample are denoted by $x_1, x_2, ..., x_n$, the sample range is:  
$$r = \max(x_i) - \min(x_i)$$

Diagrams
- **Stem-and-Leaf Diagrams**: A good way to obtain an informative visual display of a data set $x_1, x_2, ..., x_n$, where each number $x_i$ consists of at least two digits. To construct:
    1. Divide each number $x_i$ into two parts: a stem, consisting of one or more of the leading digits, and a leaf, consisting of the remaining digits
    2. List the stem values in a vertical column
    3. Record the leaf for each observation beside its stem
    4. Write the units for stems and leaves on the display
    - Can also divide data into more than two parts, four can work as well. $q_1$ has approx 25% of values below it, $q_2$ approx 50%, and so on
    - In general, the $100kth$ percentile is a data value such that approx $100k%$ of the observations are at or below this value, and approx $100(1-k)%$ of them are above it
    - We can use the **interquartile range (IQR)**, defined as $IQR = q_3 - q_1$, as a measure of variability
- **Frequency Distributions and Histograms**: More compact summary of data than a stem-and-lead diagram. To construct a frequency distribution, we must divide the range of data into intervals, usually called class intervals/cells/bins. Choosing the number of bins approx equal to the square root of the number of observations works well in practice
    - Relative frequencies found by dividing the observed frequency in each bin by the total number of observations
- The histogram is a visual display of the frequency distribution. To construct:
    1. Label the bin boundaries on a horizontal scale
    2. Mark and label the vertical, scale with the frequencies or the relative frequencies
    3. Above each bin, draw a rectangle where height is equal to the frequency (or relative frequency) corresponding to that bin
    - Sometimes unequal bin widths will be used in a histogram. When this is the case, the rectangle's area should be proportional to the bin frequency. This implies the rectangle height should be:  
    $$\text{Rectangle Height} = \frac{\text{Bin Frequency}}{\text{Bin Width}}$$
- **Box Plots**: A graphical display that simultaneously describes several important features of a data set, such as center, spread, departure from symmetry, and identification of unusual observations or outliers:  
![img]({{ site.url }}/assets/3y03/bplot.png)