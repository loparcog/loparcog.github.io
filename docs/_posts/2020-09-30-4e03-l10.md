---
layout: post
title:  "4E03 - Lecture 10"
date:   2020-09-30 16:30:00 -0400
categories: 4E03
---

Recall
- Forced Flow Law  
$\lim_{t \rightarrow \inf} \frac{C_i(t)}{t} = \lim_{t \rightarrow \inf} \frac{C_0(t)}{t} E[U_i]$  
$C_i(t) = A_i(t) - N_i(t)$
    - The first equation is ONLY true if:  
    $\lim_{t \rightarrow \inf} \frac{N_i(t)}{t} = 0$
-Little's Law  
$E[N_i] = X_i E[T_i]$
    - Under the assumption that everything is stable (\lim_{t \rightarrow \inf} \frac{N_i(t)}{t} = 0$), $\lambda_i = X_i$

Ex1: A computer system has one CPU and two disks. After monitoring the system for 2hrs, the following observations were made:  
- The utilization of the CPU was 43%
- The utilization of the first disk was 66%
Each transaction AAA (finish)
- a) AAA  
SOLN:  
$\rho_{disk1} = 0.66$  
$E[D_{disk1}] = (0.02)(5) = 0.1$  
$X = \frac{0.66}{0.1} = 6.6/\text{sec}$
- b) AAA  
$P_{disk2} = X E[D_{disk2}]$  
$= (6.6)(0.02)(6)$  
$= 0.792$
- c)AAA  
$E[D_{CPU}] = \frac{P_{CPU}}{X}$  
$= \frac{0.43}{6.6} = 0.065 \text{sec}#

Ex2: Suppose we have *M* clients that submit requests to three single-server nodes in series (a client request needs to be processed by three servers in order). The mean demands at the three nodes are:  
$E[D_a] = 2, E[D_b] = 3, E[D_c] = 1$  
The average thinking time of a client is $E[Z] = 30$ AAA (check)
- The throughput is bounded above by:  
$\min(\frac{1}{3})$  
and the mean response time is bounded below by:  
$\max(6, 3M - 30)$
    - One can see that these bounds both have break points at $M* = 12$. This value is often known as the number of clients that the system can support. Below this number, performance scales well, but above this number, performance degrades considerably
- **Load Balancing**
    - Suppose that were free to choose $E[D_a], E[D_b], E[D_c]$ to be any (nonnegative) value. What would be a good choice?
    - A good choice would be 2, 2, 2, since this minimizes $D_{max}$, and max throughput is now up from 1/3 to 1/2
        - $M*$ also increases

Ex3: A database server receives requests from 50 clients. Each request to the DB server requires the five reads to be made on average from the server's single disk. The average read time is 9ms of CPU time to be processed. Consider the following three proposed design changes:
1. The average number of disk reads per access can be reduced from 5 to 2.5
2. The disk is replaced by one 60% faster
3. The CPU speed is doubled
If you were to do one of these, which would you do? If you were to do two of them?
- Some ideas are:
    - Determine $D_{max}$
    - Decrease $D_{max}$ as much as possible
    - Decreasing other E[D_i] "does not" help
        - Would help partially but not much, focus on the bottleneck
- To start, we can calculate:  
$E[D_{CPU}] = 15$  
$E[D_{disk}] = (5)(9) = 45$
    - The disk is limiting the performance, so we should look at fixing that, ignore design change 3 and look @ 1/2
- If we did design 1:  
$E[D_{disk}] = (2.5)(9) = 22.5$
- If we did design 2:  
$45 * ? = 28.1$

