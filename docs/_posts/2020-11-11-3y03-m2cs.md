---
layout: post
title:  "3Y03 - Midterm 2 Cheat Sheet"
date:   2020-11-11 20:00:00 -0400
categories: 3Y03
---

Notes from Poisson Distribution to Point Estimators
===

Poisson Distribution
---

- Used to model number of events occurring within a given time interval
    - Can also be used for number of events in a distance, area, volume, any sort of **continuous** domain
- If *X* is a **poisson random variable**, where *x* is the discrete number of occurrences and $\lambda$ is the average number of events per time interval, with *T* as the total time, then the functions are given as follows
    - pmf:  
    $f(x) = \frac{e^{-\lambda}(\lambda)^x}{x!}$
    - avg:  
    $E(X) = \lambda T$
    - var:  
    $V(X) = \lambda T$

CRV
---

- Probability Density Functions
    - **Continuous Random Variable**: A random variable with an interval (finite or infinite) with real numbers in its range
    - For a continuous random variable *X*, a **probability density function** is a function such that:
        1. $f(x) \geq 0$
        2. $\int_{-\infty}^{\infty} f(x) dx = 1$
        3. $P(a \leq X \leq b) = \int_a^b f(x) dx =$ area under f(x) fro *a* to *b* for any *a* and *b*
    - **Histogram**: An approximation to a probability density function
        - An example would be for *X* as a continuous random variable, and any *a* and *b*:  
        $P(a \leq X \leq b) = P(a \lt X \leq b) = P(a \leq X \lt b) = P(a \lt X \lt b)$
- Cumulative Distribution Function
    - For a continuous random variable *X*, the **cumulative distribution function** is:  
    $F(x) = P(X \leq x) = \int_{-inf}^x f(u) du$  
    (For $-\infty \lt x \lt \infty$)
        - The probability density function can be determined from the cumulative distribution function by differentiating:  
        Given F(x), $f(x) = \frac{dF(x)}{dx}$, as long as the derivative exists
        - We can also get full ranges from this:  
        $$P(a \leq X \leq b) = \int_a^b f(u) du$$
- Mean/Variance of a Continuous Random Variable (CRV)
    - Suppose that *X* is a continuous random variable, with probability density function $f(x)$
        - The mean/expected value of *X* is:  
        $\mu = E(X) = \int_{-\infty}^{\infty} x f(x) dx$
        - The variance of *X* is:
        $$\sigma^2 = V(X) = \int_{-\infty}^{\infty} (x-\mu)^2 f(x) dx = \int_{-\infty}^{\infty} x^2 f(x) dx - \mu^2$$
    - Furthermore, we can compute the expected value with a random variable function as follows:  
    $E[h(X)] = \int_{-\infty}^{\infty} h(x) f(x) dx$
        - In the special case that $h(X) = aX + b$, for any constants *a* and *b*, then:  
        $E[h(X)] = aE(X) + b$

CRV Distributions
---

- **Continuous Uniform Distribution**
    - A CRV with pdf:  
    $$f(x) = \frac{1}{b-a}, a \leq x \leq b$$  
    is a continuous uniform random variable
    - This also gives us the functions:  
    $$F(x) = P(x \leq X) = \frac{x-a}{b-a}$$
    $$E(X) = \frac{a+b}{2}$$  
    $$V(X) = \frac{(b-a)^2}{12}$$
- **Normal Distribution**
    - A CRV with pdf:  
    $$f(x) = \frac{1}{\sqrt{2 \pi \sigma}} e^{\frac{-(x-\mu)^2}{2 \sigma^2}}, -\infty \lt x \lt \infty$$  
    is a normal random variable, with distribution $N(\mu, \sigma^2)$
    - If the distribution is $N(0,1)$, then it is a **standard normal random variable**, denoted as $Z$, and gives us:  
    $$\Phi(z) = P(Z \leq z)$$
    - We can standardize any normal random variable with $E(X) = \mu, V(X) = \sigma^2$:  
    $$Z = \frac{X - \mu}{\sigma}$$  
    We can also do the same for any values:  
    $$z = \frac{x - \mu}{\sigma}$$  
    and then plug the $z$ value into the table to get an associated probability, noted as $\Phi z$
    $$P(X \lt x) = P(Z \lt z) = \Phi z$$
- **Normal Approximation to the Binomial and Poisson Distributions**
    - If $X$ is a **binomial random variable** with parameters *n* and *p*:  
    $$Z = \frac{X - np}{\sqrt{np(1-p)}}$$  
    is approximately a standard normal random variable
    - To approximate with a binomial random variable with a normal distribution, a **continuity correction** is applied as follows:  
    $$P(X \leq x) = P(X \leq x + 0.5)$$  
    $$\approx P(Z \leq \frac{x + 0.5 - np}{\sqrt{np(1-p)}})$$  
    $$P(x \leq X) = P(x - 0.5 \leq X)$$  
    $$\approx P(Z \leq \frac{x - 0.5 - np}{\sqrt{np(1-p)}})$$  
    **NOTE:** This approximation is **ONLY** good for $np \gt 5, n(1-p) \gt 5$
    - If $X$ is a **Poisson random variable** with $E(X) = V(X) = \lambda$:  
    $$Z = \frac{X - \lambda}{\sqrt{\lambda}}$$  
    Is approximately a standard normal random variable
    - The same **continuity correction** applies, and this is **ONLY** good for $\lambda \gt 5$
- **Exponential Distribution**
    - The random variable $X$ that equals the distance between successive events from a Poisson process, with mean number of events $\lambda \gt 0$ per unit interval, is an **exponential random variable** with parameter $\lambda$. The pdf of $X$ is then:  
    $$f(x) = \lambda e^{- \lambda x}, 0 \leq x \lt \infty$$
    - We also get the following values:  
    $$F(X) = 1 - e^{-\lambda x}$$  
    $$E(X) = \frac{1}{\lambda}$$  
    $$V(X) = \frac{1}{\lambda^2}$$
    - This distribution also supports the **memoryless property**, stating that the probability of an event does not depend on the time that has already passed or time since the last event

Joint Probability Distributions
---

- **ONLY** for CRV's
- Let $X,Y$ be two CRV's, which may or may not be related to one another. The **joint probability density function** of $X,Y$ is a function $f_{X,Y}(x,y)$ satisfying the following properties:
    1. $$f_{X,Y} \gt 0, \forall x,y \in \mathbb{R}$$
    2. $$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X,Y}(x,y) dx dy = 1$$
    3. For any region in $A \subseteq \mathbb{R}^2$:  
    $$P((X,Y) \in A) = \int \int_A f_{X,Y}(x,y) dx dy$$
- Note the mean of the joint probability distribution can be found with:  
$$E(XY) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xy f_{X,Y}(x,y) dx dy$$  
- We can also divide this function in two, getting the **marginal probability densities** as follows:  
$$f_X(x) = \int_{R_Y(y)} f_{X,Y}(x,y) dy$$  
$$f_Y(y) = \int_{R_X(x)} f_{X,Y}(x,y) dx$$  
where $R_X(x)$ is the range of all values $X$ is defined on, and same with $Y$
- We can then compute the expected value and variance of each as follows:  
$$E(X) = \int_{-\infty}^{\infty} x f_X(x) dx = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x f_{X,Y}(x,y) dx dy$$  
$$V(X) = E((x - \mu_X)^2) E(X^2) - E(X)^2$$  
This can be some similarly for $Y$
- We can say $X,Y$ are **independent** iff:  
$$f_{X,Y}(x,y) = f_X(x) f_Y(y)$$
    - If $X,Y$ are independent and jointly distributed on region $R$, then $R$ must be rectangular
    - $R = I_1 * I_2$ for intervals $I_1, I_2$, giving us:  
    $$1 = \int_{I_2} \int_{I_1} f_{X,Y} (x,y) dx dy$$  
    $$= (\int_{I_1} f_X(x) dx)(\int_{I_2} f_Y(y) dy)$$
- We can get the expected value of a function with two random variables $h(X,Y)$ as follows:  
$$E(h(X,Y)) = \int \int h(x,y) f_{X,Y}(x,y) dx dy$$
- We can also measure the linear relationship, or **covariance** of $X$ and $Y$, given as:  
$$cov(X,Y) = \sigma_{XY} = E(XY) - E(X)E(Y)$$
    - If $X,Y$ are independent, then $\sigma_{XY} = 0$, but the reverse is **not true**
- The **correlation** between two random variables $X,Y$, denoted as $\rho_{XY}$ is:  
$$\rho_{XY} = \frac{cov(X,Y)}{\sqrt{V(X)V(Y)}} = \frac{\sigma_{XY}}{\sigma_X\sigma_Y}$$
    - If the covariance is positive/negative/zero, then the correlation is also positive/negative/zero, but the values can range to:  
    $$-1 \leq \rho_{XY} \leq 1$$  
    so independent variables would also have $\rho_{XY} = 0$

Linear Functions of Random Variables
---

- Given random variables $X_1, X_2, ..., X_p$ and constants $c_0, c_1, c_2, ..., c_p$:  
$$Y = c_0 + c_1 X_1 c_2 X_2 ... c_p X_p$$  
is a **linear function** of $X_1, X_2, ..., X_p$
- The **mean of a linear function** can be calculated as:  
$$E(Y) = c_0 + c_1 E(X_1) + c_2 E(X_2) + ... + c_p E(X_p)$$
- Similarly, the **variance of a linear function** can be calculated as:  
$$V(Y) = c_1^2 V(X_1) + c_2^2 V(X_2) + ... + c_p^2 V(X_p) + 2 \sum \sum_{i \lt j} c_i c_j cov(X_i, X_j)$$
    - However, if $X_1, X_2, ..., X_p$ are *independent*, then:  
    $$V(Y) = c_1^2 V(X_1) + c_2^2 V(X_2) + ... + c_p^2 V(X_p)$$
- **Mean of an Average**: If $\bar{X} = (X_1 + X_2 + ... X_p)/p$, with $E(X_i) = \mu$ for $i = 1, 2, ..., p$:  
$$E(\bar{X}) = \mu$$
- **Variance of an Average**: If $X_1, X_2, ..., X_p$ are also independent, with $V(X_i) = \sigma^2$ for $i = 1, 2, ..., p$:  
$$V(\bar{X}) = \frac{\sigma^2}{p}$$
- **Reproductive Property of the Normal Distribution**: If $X_1, X_2, ..., X_p$ are independent normal random variables with $E(X_i) = \mu_i$ and $V(X_i) = \sigma_i^2$ for $i = 1, 2, ..., p$, then:  
$$Y = c_0 + c_1X_1 + c_2 X_2 + ... + c_p X_p$$  
is a **normal random variable**, with:  
$$E(Y) = c_0 + c_1 \mu_1 + c_2 \mu_2 + ... + c_p \mu_p$$  
$$V(Y) = c_1^2 \sigma_1^2 + c_2^2 \sigma_2^2 + ... + c_p^2 \sigma_p^2$$
- NOTE: You can also use this for equations like the following:  
$$P(\bar{X}_1 - \bar{X}_2 \geq x) = P(\frac{(\bar{X}_1-\bar{X}_2) - (\mu_1 - \mu_2)}{\sqrt{\sigma^2_1/p + \sigma_2^2/p}} \geq \frac{(x) - (\mu_1 - \mu_2)}{\sqrt{\sigma^2_1/p + \sigma_2^2/p}})$$

Numerical Summaries of Data
---

- **Sample Mean**: If a sample of *n* values are taken from a population, with each sample denoted as $x_1, x_2, ..., x_n$, then the sample mean is:  
$$\bar{x} = \frac{x_1 + x_2 + ... + x_n}{n}$$
- **Sample Variance**: If $x_1, x_2, ..., x_n$ is a sample of *n* observations, the sample variance is:  
$$s^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}$$
    - Similarly, you can get the **sample standard deviation** using the positive root
- **Sample Range**: The sample range of *n* observations is given as:  
$$r = max(x_i) - min(x_i)$$

Descriptive Statistics (Graphing)
---

- **Stem-and-Leaf Diagrams**
    - Steps to construct a stem-and-leaf diagram:  
        1. Divide each number $x_i$ into two parts: a stem, consisting of one or more leading digits, and a leaf
        2. List the stem values in a vertical column, and record all the leafs beside it
        4. Write the units for stems and leaves on the display
    - We can use this to divide data into quartiles, with $q_1$ having approx 25% below it, $q_2$ having 50%, and $q_3$ having 75%
        - We can find these values using the following:  
        $q_1 = \text{median between start value and median}$  
        $q_2 = \text{median value}$  
        $q_3 = \text{median between median and end value}$  
        - An outlier is an observation that passes $q_1 - 1.5(IQR)$ or $q_3 + 1.5(IQR)$
    - In general, the $n$th percentile is a data value such that $n$% of observations are below it, and $1-n$% are above
    - We can use the **interquartile range (IQR)** as a measure of variability:  
    $$IQR = q_3 - q_1$$

- **Frequency Distributions and Histograms**
    - More compact summary of data than a stem-and-lead diagram. To construct a frequency distribution, we must divide the range of data into intervals, usually called class intervals/cells/bins. Choosing the number of bins approx equal to the square root of the number of observations works well in practice
        - Relative frequencies found by dividing the observed frequency in each bin by the total number of observations
    - The histogram is a visual display of the frequency distribution. To construct:
        1. Label the bin boundaries on a horizontal scale
        2. Mark and label the vertical, scale with the frequencies or the relative frequencies
        3. Above each bin, draw a rectangle where height is equal to the frequency (or relative frequency) corresponding to that bin
        - Sometimes unequal bin widths will be used in a histogram. When this is the case, the rectangle's area should be proportional to the bin frequency. This implies the rectangle height should be:  
        $$\text{Rectangle Height} = \frac{\text{Bin Frequency}}{\text{Bin Width}}$$
- **Box Plots**
    - The following image helps to explain the plot:  
    ![img]({{ site.url }}/assets/3y03/boxplot.PNG)

- **Probability Plots**
    - Let $x_1, x_2, ..., x_n$ be a given data set, and $x_{(1)}, x_{(2)}, ..., x_{(n)}$ be the same data set, ordered from smallest to largest
    - Let $z_1, z_2, ..., z_n$ be the value of $z$ with the property that:  
    $$\Phi(z_i) = \frac{i-0.5}{n}$$
        - We can use that fraction calculation to match with the closest value from the normal distribution table, getting a $z$ value, which we will use as the y values of the graph
    - Plot the $(x_i, z_i)$ pairs onto a graph and check for correlation
        - We can then do an **Anderson-Darling Normality Test** to see if the population is normally distributed
        - If the p-value is **less than 0.05**, then we can conclude it **does not** follow a normal distribution
        - If the p-value is **greater than 0.10**, we can conclude that it **does** follow a normal distribution
        - If the p-value is **between 0.05 and 0.10**, then the results are inconclusive
    - By eye, we're just looking that all the points are on the line or somewhere close

Point Estimators
---

- A major component of statistical inference is called **parameter estimation**
    - A **parameter**, $\theta$, is any numerical feature of a population/dataset (mean, variance, etc.)
- A **point estimate** of some population parameter $\theta$ is a single numerical value $\hat{\theta}$ of a statistic $\hat{\Theta}$. The statistic $\hat{\Theta}$ is called the **point estimator**
    - An example of this could be how a sample mean $\bar{x}$ is a point estimator of the population mean $\mu$, that is:  
    $$\hat{\mu} = \bar{X}$$
- The random variables $X_1, X_2, ..., X_n$ are a **random sample** of size *n* if:
    1. The $X_i$'s are independent random variables
    2. Every $X_i$ has the same probability distribution
- A **statistic** is any function of the observation in a random sample
- A few sample statistics can be seen as follows:
    - Sample mean:  
    $$\bar{X} = \frac{X_1+X_2+...+X_n}{n}$$
    - Sample variance: $S^2$
    - Sample standard deviation: $S$
- The probability distribution of a statistic is called a **sampling distribution**
    - Depends on many factors like sample size, sample method, distribution of $X$, but can be helped with the central limit theorem
- **Central Limit Theorem**: If $X_1, X_2, ..., X_n$ is a random sample of size *n* taken from a population (finite/infinite) with mean $\mu$ and finite variance $\sigma^2$, and if $\bar{X}$ is the sample mean, the limiting form of the distribution is:  
$$Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}}, n \to \infty$$
    - This means that for any *z*:  
    $$P(\frac{\bar{X} - \mu}{\sigma/\sqrt{n}}) = \phi(z)$$
    - **NOTE**: To use this, you will need $n \geq 30$, unless the distribution is ""nice enough"" (eg. symmetric, unimodal), in which you can use $n \geq 5$
- **Unbiased Estimators**
    - The point estimator $\hat{\Theta}$ is an **unbiased estimator** for the parameter $\theta$ if  
    $$E(\hat{\Theta}) = \theta$$  
    otherwise, the estimator is biased, and the **bias** of the estimator is given by:  
    $$E(\hat{\Theta}) - \theta$$
    - Note that the sample variance:  
    $$S^2 = \frac{\sum_{i=1}^n (X_i - \bar{X})^2}{n-1}$$ 
    is an unbiased estimator of variance
        - $S = \sqrt{S^2}$ however, is **NOT** an unbiased estimator of standard deviation, but is still pretty good
- **Variance of a Point Estimator**
    - If we consider all unbiased estimators of $\theta$, the one with the smallest variance is called the **minimum variance unbiased estimator (MVUE)**
        - This should always be chosen as the point estimator
        - Also, note that if $X_1, X_2, ..., X_n$ is a random sample of size *n* from a normal $\bar{X}$ is **always** the MVUE of $E(X)$
- **Standard Error of an Estimator**
    - The **standard error** of an estimator $\hat{\Theta}$ is its standard deviation, given by:  
    $$\sigma_{\hat{\Theta}} = \sqrt{V(\hat{\Theta})}$$
    - If the standard error involves unknown parameters that can be estimated, substitution of those values into $\sigma_{\hat{\Theta}}$ produces an **estimated standard error**, denoted by $\hat{\sigma_{\hat{\Theta}}}$
    - The standard error of the distribution of $\bar{X}$ is given by:  
    $$\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}$$
    - If we didn't know $\sigma$, but we knew the sample standard deviation *S*, we can get the estimated standard error by:  
    $$\hat{\sigma}_{\bar{X}} = \frac{S}{\sqrt{n}}$$
- **Mean Square Error of an Estimator**
    - The **mean squared error (MSE)** of an estimator $\hat{\Theta}$ of the parameter $\theta$ is defined as:  
    $$MSE(\hat{\Theta}) = E(\hat{\Theta} - \theta)^2$$
    - This can also be generalized to:  
    $$MSE(\hat{\Theta}) = V(\hat{\Theta}) + (\text{bias})^2$$
    - The **relative efficiency** of $\hat{\Theta}_2$ to $\hat{\Theta}_1$ is given as:  
    $$\frac{MSE(\hat{\Theta}_1)}{MSE(\hat{\Theta}_2)}$$
    