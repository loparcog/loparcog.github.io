---
layout: post
title:  "4AA4 - Final Notes"
date:   2020-12-08 20:00:00 -0400
categories: 4AA4
---

Real-Time Operating Systems (RTOS)
---

Classification of Real-Time (RT) Systems
- **Response Time**: Time interval between a stimulus/input and the corresponding result/output
    - A "timely" response to external stimuli is vital
- **Classification of RTS**
    - A **soft RTS** is one where performance is degraded but not *destroyed* by failure to meet a response-time constraint
        - Ex. Streaming videos, computer games, online chatting
    - A **firm RTS** is one where missing a few deadlines will not lead to total failure, but missing more than a few may lead to complete and catastrophic system failure
        - Ex. Manufacturing systems with robot assembly lines, coursework submissions
    - A **hard RTS** is one where failure to meet a single deadline may lead to complete and catastrophic failure
        - Ex. Mission critical systems, nuclear systems, medical applications such as pacemakers
    - Cost for missing deadlines increases from sort to hard
- **Terminology for RTS**
    - Multitasks (periodic, aperiodic tasks)
    - Schedulability (ability of tasks to meet hard deadlines)
    - Performance (response time, cost of missing deadlines)
    - Does a RTS mean a *fast* system?
        - No, as long as the tasks execute when needed, that's a real time system

System Call
- How does the OS deliver the services to user processes?
    - User processes request a service from the kernel by **making a system call**, utilizing the **library procedure**
        - This procedure puts params of the system call in suitable registers and then issues a **TRAP** instruction
        - Control is the passed onto the kernel, checking the validity of the params and performing the requested service
        - When finished, a code is put into a register to let the user know if a process passed or failed
        - Finally, a return from the TRAP instruction is executed, and control is passed back to the user process
    - **A user process becomes a kernel process when it executes a system call**

Kernel Module
- Two approaches to implement RTOS based on linux:
    1. Add a new layer of RT Kernel with full control of interrupts and processor key features
        - RTAI and RTLinux both use real-time kernels as the main kernel
    2. Make necessary changes in the Linux Kernel to make it suitable for real time applications (preempt RT)
        - Preempt RT is used on the myRIO devices, so the kernel threads are preemptable - a **kernel module** is a piece of code that can be loaded and unloaded into the kernel upon demand
        - Stops wasting memory and users don't need to rebuild and reboot the base kernel every time they require new functionality
- To interact with kernel modules on the system, use the following commands:
    - `insmod`: Insert a module
    - `rmmod`: Remove a module
    - `lsmod`: Show currently loaded modules
    - `modinfo`: Show information about a given Linux Kernel module

Kernel Module Development
- A real-time task running as a kernel module consists of three sections:
    1. `init_module()`: Invoked by `insmod` to prepare for later invocations of module's functions
        - Can be used to allocate required system resources, declare and start tasks, etc.
    2. Task-specific code (based on POSIX API)
    3. `cleanup_module()`: Invoked by `rmmod` to inform the kernel that the module's functions will not be called anymore
        - Good place to release all sys resources allocated during the lifetime of the module, stop and delete tasks, etc.
- A simple example of this can be seen below:

```C
#include <linux/module.h>
#include <linux/kernel.h>

int init_module(void){
  printk(KERN_INFO "Hello World\n");
  return 0;
}

void cleanup_module(void){
  printk(KERN_INFO "Goodbye Cruel World!\n");
}

```

- Compiling a kernel module is done using a make file, example in L3
    - Note, `init_module` can be any function, that function just needs to be tagged with `__init` (or `__exit` for cleanup), marked as static, and then declared at the bottom of the function with the `module_init()` function
    - Other macros also exist like `DRIVER_AUTHOR`, `DRIVE_DESC`, etc.
    - Example:

```C
#define DRIVER_AUTHOR "Giacomo Loparco"
#define DRIVER_DESC "4aa4_lab2_part3"

static int intval __initdata = 2;
static char *my_string = "dummy";

static int __init hello_init(void)
{
    printk (KERN_INFO  "Hello %s world number 2\n", my_string);
    return 0;
}

static void __exit hello_clean(void)
{
    printk (KERN_INFO "<1>Hello cruel world 2\n");
}


module_init(hello_init);
module_exit(hello_clean);

module_param(my_string, charp, 0000);
MODULE_PARM_DESC(my_string, "A character string");
```

- Command line arguments can be passed, but NOT with `argv` or `argc`
    - First, values need to be declared to store values passed on the command line
    - Next, they should be set up with the macro `module_param(name, type, permissions)`
    - At run time, `insmod` will fill up the variables with values passed

Thread vs Process
- A program can be divided into multiple tasks by creating multiple processes, which can be done in one of two ways.
- **Fork** (Process) 
    - The `fork()` command:
        - Creates a child process identical to its parent
        - Returns a value of 0 to the child process and returns the process ID of the child process to the parent process
    - There is a lot of overhead to create a new process, as everything is duplicated. Data space is not shared, so its harder to communicate
    - Variables initiated **before** `fork()` will be duplicated from the parent to child process. After `fork()`, a branch is needed to separate parent and child processes, for example:
    
```C
pid = fork();
if (pid == 0){
// Do child process work (fork process)
ChildProcess();
} else{
// Do parent process work (main process)
ParentProcess();
}
```

- `sleep(x);` is commonly used to make child/parent threads wait for one another
- **Threads**
    - Gives a more efficient way to implement a task
    - Multiple subtasks can be implemented as separate streams in a single process
    - The thread model breaks the memory space into two parts:
        1. Contains the program-wide resources such as global data and program instructions/code
        2. Information pertaining to the execution state of control stream, such as the PC and the stack  
    ![img]({{ site.url }}/assets/4aa4/threadex.png)
    - Advantages of threads
        - Shared address space, communication between threads is more efficient
        - Context switching between threads in the same process typically faster than processes
        - Much quicker to make a thread than process
        - Communication is possible through **pointers** to change variables in different threads, not possible with fork
        - Supported by POSIX (Portable OS interface)
    - Disadvantages of threads
        - Need of synchronization, global variables are shared between threads, so any modification to these or shared variables could be disastrous (deadlock, crashes)
        - Less secure, many library functions are not thread safe, multiple threads trying to access the same function can throw an error
        - Lack of robustness, if one thread crashes, the whole application crashes
    - Example:

```C
// Define the number of threads
#define NUM_THREADS 5

// Define a worker function
void  *print_hello(void *threadid)
{
    long tid;
    tid = (long) threadid;
    printf("Hello World! It's me, thread #%ld! \n", tid);
    pthread_exit(NULL);
}

// Go through main thread
int main (int argc, char *argv[])
{
    pthread_t threads[NUM_THREADS];
    int rc;
    long t;
    for (t = 0; t < NUM_THREADS; t++)
    {
        printf("In main: creating thread %ld \n", t);
        rc = pthread_create(threads[t], NULL, print_hello, (void *) t);
        if (rc)
        {
            printf("ERR: Return code from pthread_create() is $d \n", rc);
            return -1
        }

        // If you want to waitr for the thread to finish
        pthread_join(threads[t], NULL);
    }

    return 0
}
```
- Race Condition
    - An error condition in parallel programs, where the outcome of a program changes depending on what code is executed first in a set of thread/fork codes where either any program can run first or second
    - Causes the outcome to not be predictable or repeatable
    - Can be avoided by using thread-safe locks, like semaphores

Event-Driven Programming (NOT AS IMPORTANT)
- Normal functions would use sleeps to manage child and parent thread timings, but event-driven programming puts signals into use, acting on those signal events
    - Functions used given as follows:

```C
// Set a signal to a given function
signal(SIGUSR1, funcName1);
signal(SIGUSR2, funcName2);

// Get the current process ID
getpid();
// Get the parent process ID
getppid();

// Wait for a process @ ID pid to end
waitpid(pid, NULL, 0);

// Send a signal to a given process
kill(getppid(), SIGUSR1);
```

- Example:

```C
int main ()
{

pid_t pid;
if ((pid = fork()) == 0)
{
    int i;
    for(i=0; i<5; i++)
    {
        sleep(1);
        kill(getppid(), SIGUSR1);
    }
    exit(0);
}
else
{
    signal(SIGUSR2, handle_printSum);
    signal(SIGUSR1, handle_addRNGs);
    waitpid(pid, NULL, 0);
    //printf("The End!\n");
    exit(0);
}

}
```

- How can a race condition be solved in a uniprocessor system?
    - Disable preemptions when scheduling processes (only the process itself can voluntarily relinquish the CPU)
    - Use semaphores as **atomic** operation
- Linux Priority Levels and Nice Values
    - Linux has static priority ranged from 0 to 139, where 0 to 99 are reserved for real time kernel events, and 100 to 139 for users
    - Priority is represented as a **nice value (niceness)**, from [-20, 19], mapping to 100 to 139 (NOTE: the nice value **cannot map to below 100**. -20 is the lowest it goes)
        - The lower the nice value, the higher the priority of the process
        - By default, the priority value is 0
    - A process priority can be changed on linux using `nice -x <pname>`, setting pname's priority to x
    - Can also be done using C code, although a bit more work 
    - Threads can also be declared with a given priority value, allowing the system to know what to run first in a race condition
        - In general though, the parent process will *usually* run first
    - Functions are given as follows:

```C
// Set some default val
int which = PRIO_PROCESS;

// Get a given PID
pid = getpid();

// Get the priority of process at pid
getpriority(which, pid);
// Set the priority of process at pid
int newPriority = 10
setpriority(which, pid, newPriority)

// Alternatively, nice can be called to increase the current process
// Increase the current process priority by x
nice(x);
// NOTE: unprivileged users can only lower the process priority

// Thread priority can be handled with the following

// Set a param object
struct sched_param param;

// Also set some default vals
pthread_attr_t attr;
pthread_attr_init(&attr);

// Set priority to x
param.sched_priority = x

// Can be used for a given thread...
pthread_attr_getschedparam(&tattr, &param);
param.sched_priority = x;
pthread_attr_setschedparam(&tattr, &param)

// ... or in creating a thread
pthread_create(&thread, &tattr, functionName, arg);

// Finally, for fork, there is also sched_setscheduler
// ONLY ACCESSIBLE TO PRIVILEGED USERS
struct sched_param param;
param.sched_priority = 10;
if ( sched_setscheduler(pid, SCHED_FIFO, &param) == -1){
    perror("SetScheduler failed in parent process. \n")
}
```



Task Scheduling
---

Task Overview
- In real-time systems, several tasks execute **concurrently**, with each having its own **real-time constraint**
- **Periodic Tasks**: Inter-arrival times between two instances are almost the same
- **Sporadic Tasks**: Inter-arrival times between two consecutive instances differ widely (hard timing constraints)
- Temporal Parameters
    - In order to meet real-time reqs of hard real-time tasks, it's assumed many parameters of these tasks are known at all times. Some are:  
        - Number of tasks ($n$)
            - Many embedded systems have number of tasks fixed/limited
        - Arrival/Release time ($r_{i,j}$)
        - Absolute deadline ($d_i = r_{i,j} + D_i$)
            - Moment in time at which the job must be completed
        - Relative deadline ($D_i$)
            - Interval of time when the job should be run and complete
        - Execution time ($E_i$)
            - Counts with no other tasks sharing resources
                - Depends on processor speed, complexity of instructions
            - Actual amount of time required by a job to complete its execution
        - Response time ($R_i$)
            - Time span between task activation and completion
- Periods and Phases of Periodic Tasks
    - A **period** ($p_i$) of a periodic task $T_i$ is the *minimum* length of all time intervals between the release times of consecutive tasks
    - **Phase** of a task ($\phi_i$) is also known as the release time ($r_{i, j}$) of a task $T_i$
        - First instances of several tasks may be released simultaneously, they are called in phase and have a 0 phase
- A typical task model for periodic tasks:
    - All tasks in the task set are **strictly** periodic
    - The relative deadline of a task is equal to its period (if not specified)
    - All tasks are independence, no precedence constraints
    - No task has any non-preemptible section and the cost of preemption is negligible
    - CPU processing requirements are significant, while memory and I/O reqs are negligible
- Representations
    - A periodic task $T_i$ can be represented by a 4 tuple:  
    $\phi_i , P_i, e_i, D_i$
        - If using 3 tuple $P_i, e_i, D_i$, assume phase = 0  
        $(P_i, e_i, D_i) = (0, P_i, e_i, D_i)$
        - If using 2 tuple $P_i, e_i$, assume phase = 0 and $D_i = P_i$  
        $(P_i, e_i) = (0, P_i, e_i, P_i)$
- CPU Utilization
    - A measure of the percentage of non-idle processing, denoted as $U$. Calculated by symming the contribution of utilization factors for each (periodic or aperiodic) task. The utilization factor $u_i$ for a task $T_i$ with execution time $e_i$ and period $p_i$ is given by:  
    $$u_i = \frac{e_i}{p_i}$$
    - And for a system with n tasks, the overall system utilization is:  
    $$U = \sum_{i=1}^n u_i$$

Cyclic Executive (CE)
- Table-driven gives off-line static-schedule which specifies *exactly* when each job executes
    - Assumptions:
        - Parameters of jobs with hard deadlines known
        - Task scheduling is non-preemptive
    - Non-periodic work can be run during time slots not used by periodic tasks
    - Sophisticated algorithms can be used
- Consider the following 4 periodic tasks:  
$T_1 = (4; 1)$  
$T_2 = (5; 1.8)$  
$T_3 = (20; 1)$  
$T_4 = (20; 2)$  
    - What is the total utilization? How can we construct a schedule for this process?  
    - Utilization is given as $e_i/P_i$, so this system's total utilization is:  
    $$U = \sum_{i=1}^{n} u_i$$  
    $$= 1/4 + 1.8/5 + 1/20 + 2/20$$  
    $$= 0.76$$
    - Furthermore, a possible schedule for this would be:
    ![img]({{ site.url }}/assets/4aa4/totut.png)
- Hyperperiod (H)
    - The least common multiple (LCM) of the periods of all periodic tasks
    - The max number of arriving jobs in a hyperperiod is denoted as N, with  
    $$N = \sum_{i=1}^n \frac{H}{p_i}$$  
    where $p_i$ is the period of task *i*
        - In the example above, the hyperperiod H is 20 for the four tasks, and N=11
- Frames
    - We want scheduling decisions to be made at random intervals rather than arbitrary times
    - We can do this by dividing a hyperperiod into frames
        - Timing is enforced only at frame boundaries
        - Each task must fit within a single frame
        - Frame size is *f*
        - Number of frames per hyperperoid is $F = H/f$
    - Frame sizes have the following constraints:
        1. A job/instance must fit into a frame, so:  
        $$f \geq \max_{1 \leq i \leq n} e_i$$  
        for all tasks
            - We do this so non-preemptive tasks can finish executing within a single frame
        2. *H* must be evenly divided by *f*, so there are an integer number of frames in the hyperperiod
            - Done to keep the cyclic schedule table size small
        3. *f* should be sufficiently small, so that there should be a complete frame between the release and the deadline of every task
            - Done to schedule the task before the deadline is missing
            - This is also given by the equation for each task *i*:  
            $$2f - gcd(P_i, f) \leq D_i$$ 
            - Remember that in a two tuple, $D_i = P_i$ 
    - Back to the first example, what value should *f* take?
        - C1: $f \geq 2$
        - C2: *f* can be 1, 2, 4, 5, 10, or 20
        - C3: Only 2 works  
            - For 1:  
            $$2f - gcd(4, f) \leq 4$$  
            $$2(2) - 2 = 2$$  
            $$2(4) - 4 = 4$$  
            $$2(5) - 1 = 9$$  
            Therefore 2 and 4 works
            - For 2:  
            $$2f - gcd(5, f) \leq 5$$  
            $$2(2) - 1 = 3$$  
            $$2(4) - 1 = 7$$  
            Therefore only 2 works
            - You can go through the other numbers for completeness but we already know 2 is the only working frame size fo this problem by these two
        - Layout:  
        ![img]({{ site.url }}/assets/4aa4/fval.png)
- Task Slices
    - What if frame size constraints conflict/cannot be met?
        - Example being T = (4,1),(5,2,7),(20,5)
        - C1: $f \geq 5$
        - C2: $f \leq 4$
    - Solution is to "slice" a task into smaller sub tasks, ex:  
    $$T_3 = (20,5) := T_{31} = (20, 1), T_{32}=(20, 3), T_{33}=(20, 1)$$
    - Q: Why not split $T_3$ into $T_{31} = (20, 2), T_{32}=(20, 3)$
        - ANS:  
        $T_1$ with a period of 4 must be scheduled in each frame of size 4  
        $T_2$ with a period of 5 must be scheduled in 4 out of 5 frames  
        This leaves only 1 frame with 3 units of time for T3, other frames have only 1 unit of time and cannot have a job w/ execution time of 2
        - Splitting tasks is a pain and can be very error prone, so try to avoid it 
- Cycle Scheduling Decision Summary
    - Three decisions:
        1. Choose a frame size (consider the 3 constraints)
        2. If a suitable frame size was not found, break constraint 1 to select a smaller frame size to satisfy C2 and C3, and partition tasks into slices
        3. Place jobs/slices into frames
    - In general, these decisions are not independent
    - Try to partition a job into as few slices as necessary
- If a task is scheduled across multiple frames, we must slide it into subtasks
    - Potentially difficult, but if we don't allow the algorithm to split taks, the problem becomes NP-complete (?)
    - Analogy: Optimal bin packing becomes easy if we can split objects
- **Advantages**
    - Cyclic executives are very simple - you just need a table
        - Table makes the system very predicable, can validate and test with high confidence
    - No race condition/deadlock
    - Task dispatch is very efficient, just a function call
    - Lack of "scheduling anomalies"
- **Disadvantages**
    - Brittle, any change requires a new table to be computed
    - Number of frames (F) could be huge
        - Implies mode changes may have long latency
    - Release times of tasks must be fixed
    - Slicing tasks into smaller units is difficult and error prone
- **Overall**
    - CE is one of the major software architectures for embedded systems
    - Historically, CEs dominate safety-critical systems
    - Simplicity and predictability win
    - However, significant drawbacks (overhead, strong assumptions)
        - Finding a schedule may require significant offline computation

Rate Monotonic (RM)
- Shorter period tasks get higher priority
- RM Assumptions
    - Tasks are running on a uniprocessor system
    - Task are preemptive
    - There is no OS overhead for preemption
- Rate Monotonic (RM) Scheduling Algorithm
    - Static-priority preemptive approach, and one of the most popular algorithms
    - At any time instant, an RM scheduler executes the instance of the ready task that has the highest priority
    - The priority of a task is inversely related to its period, lower period has higher priority
        - If same period, priority selected at random
- Ex: Given the set of periodic tasks:  
$T_1$: 0, 5, 10, 15, 20, ...  
$T_2$: 1, 5, 9, 13, 17, ...  
$T_3$: 2, 22, 42, 62, ...  
    - Can use a 4-tuple to represent each as well:  
    $T_1 (0, 5, 2, 5)$  
    $T_2 (1, 4, 1, 4)$  
    $T_3 (2, 20, 2, 20)$  
- Can RM make the tasks meet their deadlines?  
SOLN:  
The tasks can be displayed as follows:  
![img]({{ site.url }}/assets/4aa4/tsq4.png)
- Difference between CE and RM
    - $T_1(2, 1)$ and $T_2(3, 1)$  
    What will happen if an instance (say the second) of $T_1$ is delayed for 1 time unit?
    - With CE (f=1):  
    $T_1, T_2, T_1, T_2, T_1, |$ (no late arrival)  
    $T_1 T_2 ? T_2 T_1 |$ (deadline missed when 2nd instance of $T_1$ is late)
    - With RM:  
    $T_1, T_2, T_1, T_2, T_1, |$ (no late arrival)  
    $T_1 T_2 | T_1 T_1 T_2$ (all deadlines met when the 2nd instance of $T_1$ is late)
- Schedulability Test
    - Determining if a specific set of tasks satisfying certain criteria can be successfully scheduled (completing execution of every task by its deadline) using a specific scheduler
    - This test is often done @ compile time, before the computer system and its tasks start execution
- Optimal Scheduler
    - One which may fail to meet the deadline of a task, **only if** no other scheduler can meet it
    - Note that "optimal" in real-time scheduling does not mean "fastest average response time" or "shortest average waiting time", aimed at getting tasks to meet their deadlines
- Schedulability Test for RM (Test 1)
    - There are *n* periodic processes, independent and preemptable
    - $D_i \geq p_i$ for all processes
    - Periods of all processes are integer multiples of each other
    - A **necessary and sufficient condition** for such tasks to be scheduled on a uniprocessor system using RM is the following algorithm:  
    $$U = \sum_{i=1}^n \frac{e_i}{p_i} \leq 1$$
- Schedulability Test for RM (Test 2)
    - If the tasks have arbitrary periods, a *sufficient* but not necessary schedulability condition is:  
    $$U \leq n(2^{1/n} - 1)$$  
    Task sets with a utilization smaller than $n(2^{1/n} - 1)$ are schedulable by RM algorithm
    - For different n values, we get...  
    $$n = 1 \rightarrow U \leq 1$$  
    $$n = 2 \rightarrow U \leq 0.824$$  
    $$n = \inf \rightarrow U \leq 0.693$$  
        - Proof for infinity case is in the slides
- Schedulability Test for RM (Test 3)
    - A *sufficient and necessary* condition for scheduability by RM algorithm can be derived as follows:
        - Consider a set of tasks $(T_1; T_2; ...; T_i)$, with $(p_1 \lt p_2 \lt ... \lt p_i)$
        - __Assume all tasks are in phase__
        - The moment $T_1$ is released, the processor will interrupt anything else it is doing and start processing this task as it has the highest priority (lowest period). Therefore the only condition that must be satisfied to ensure that $T_1$ can be feasibly scheduled is that:  
        $$e_1 \leq p_1$$
        - This is clearly a necessary and sufficient condition
        - The task $T_2$ will be executed successfully if its first iteration can find enough time over the time interval $(0; p_2)$, that is not used by $T_1$ 
            -$p_2$ is the period of $T_2$ and the first instance of $T_2$ must complete before the second instance arrives
        - Suppose $T_2$ finishes at *t*. The total number of instances of task $T_1$ released over the time interval $[0; t)$ is $[\frac{t}{p_1}]$
        - If $T_2$ is to finish at *t*, then every instance of task $T_1$, released during time interval $(0; t)$, must be completed, and in addition, there must be $e_2$ time available for execution of $T_2$, ie. the following condition must be satisfied:  
        $$t = [\frac{t}{p_1}e_1 + e_2]$$ 
- How do we find such *t* for $T_2$?
    - Note: Every interval has an infinite number of points, so we cannot exhaustively check for all possbible *t*
    - Consider $[\frac{t}{p_1}]$, it only changes at multiples of $p_1$, with jumps of $e_1$. So if we can find an integer k, such that time  
    $$t = k * p_1 \gt k * e_1 + e_2$$  
    and,  
    $$k*p_1 \leq p_2$$  
    we have the necessary and sufficient condition for $T_2$ to be schedulable under the RM algorithm
- Consider task $T_3$
    - It is sufficient to show that the first instance of $T_3$ completes before the arrival of its next instance at $p_3$. If $T_3$ completes its execution by *t*, then by an argument similar to that for $T_2$, we must have:  
    $$t = [\frac{t}{p_1}]e_1 + [\frac{t}{p_2}]e_2 + e_3$$  
    - $T_3$ is schedulable iff there is some $t \in (0; p_3)$ such that the above condition is satisfied
    - Also remember that the right side of the equation only changes for multiples of $p_1$ and $p_2$
- General Statement for Test 3
    - The time demand function for task $i (1 \leq i \leq n)$ is given as:
    $$\omega_i (t) = \sum_{k=1}^i [\frac{t}{p_k}] e_k \leq t, 0 \leq t \leq p_i$$  
    This holds for **any time instant t** chosen as follows:  
    $$t = k_jp_j, (j=1, ..., i)$$  
    and  
    $$k_j = 1, ..., [\frac{p_i}{p_j}]$$  
    iff the task $T_i$ is RM-schedulable

Deadline Monotonic (DM)
- Shorter relative deadlines get higher priority
- Also a fixed priority scheduler
- If every task has the period equal to the relative deadline this is the same as RM
    - For arbitrary deadlines though, DM performs better than RM
- DM may find a solution where RM cannot, but **if DM always fails then RM always fails**

Earliest Deadline First (EDF)
- This algorithm uses **dynamic-priority scheduling**, where task priorities change depending on the closeness of their absolute deadlines
- Processor always executes the task whose absolute deadline is the earliest
    - Absolute deadline is the arrival time plus the relative deadline of a task
- If more than one task has the same absolute deadline, EDF randomly selects one
- EDF is an **optimal uniprocessor scheduling algorithm**, if EDF cannot schedule a task set then there is no other algorithm that can
- It is more flexible and has better utilization than RM
    - However, timing behavior of RM is more predictable
    - In case of overload, RM is stable, same lower priority tasks miss deadlines and there is no effect on higher priority tasks. With EDF, it is difficult to predict which tasks will miss their deadlines during overloads. Also, a late task that has already missed its deadline has a higher priority than a task whose deadline is still in the future
- Schedulability Test for EDF (Test 1)
    - A set of *n* periodic tasks, each of whose **relative deadline is equal to or greater than its period**, can be feasibly scheduled by EDF iff:  
    $$\sum_{i=1}^n \frac{e_i}{p_i} \leq 1$$
- Schedulability Test for EDF (Test 2)
    - No simple test is available in the case **where the relative deadlines are not equal to or greater than their periods**. In such cases the best course of action is to develop a schedule using EDF to see if all deadlines are met over a given interval of time. A sufficient condition for such cases is:  
    $$\sum_{i=1}^n \frac{e_i}{\min(D_i, p_i)} \leq 1$$
    - Note, this is only a sufficient condition. If it fails, the task set may or may not be EDF schedulable
        - If $D_i \geq p_i$ it reduces to Test 1 above
        - If $D_i \leq p_i$ it is only a sufficient condition

Priority Inversion
- Critical sections
    - When two or more processes are competing to use the same resource, a **race condition** occurs
    - Resources can only be used by one task at a time under **mutual exclusion**
    - Use of a resource cannot be interrupted under **serially reusable**
    - Code that interacts with serially reusable resources is in its **critical section**
        - We can avoid race conditions by making sure no two processes/threads enter their critical sections at the same time
        - Can be done with mutex locks
- **Priority Inversion** is when a higher priority task is blocked by a lower priority task due to resource contention
    - Comes when trying to access shared resources guarded by mutex or semaphores and accessing non-preemptive subsystems like storage and networks
- Non-preemptivity of resource allocation can also cause deadlocks
- Can cause timing anomalies, issues for tasks to meet their deadlines

Non-Preemptive Critical Section Protocol (NPCS)
- Schedule all critical sections non-preemptively
    - **While a task holds a resource, it executes at a priority higher than the priorities of all other tasks**
- Once the blocking critical section is complete, no lower priority task can get the processor or resource until the higher priority task completes
- **Advantages**
    - Does not need prior knowledge about resource requirements of tasks
    - Simple to implement
    - Can be used in both static and dynamic priority schedulers
    - Good protocol when most critical sections are short and most tasks conflict with one another
- **Disadvantages**
    - A task can be blocked by a lower priority task for a long time even without a resource conflict

Priority Inheritance Protocol (PIP)
- If a lower priority task blocks a higher priority task due to resource contention, the lower priority task is executed first
    - Otherwise, priority of the task is the same while it is in its critical section
    - Essentially, **priority of a task is only increased when a higher priority task needs a resource it is currently using**
- Once the critical section using the resource is complete, the priority goes back to its original level
- Attains most of the advantages of NPCS while avoiding the disadvantage
    - However, this does **not avoid deadlock**, different tasks can all be waiting on other task's currently held resources

Priority Ceiling Protocol (PCP)
- Extend priority inheritance protocol to prevent deadlocks and further reduce blocking time
- Assumptions:
    1. Assigned priorities of all jobs are fixed **(static priority ONLY)**
    2. Resource requirements of all the tasks that will request a resource *R* is known
- Priority ceiling of a resource *R*:
    - ceiling(*R*) = highest priority among all the tasks that request *R*
    - Each resource has the fixed priority ceiling
- Priority ceiling of a system:
    - At any given time a set of resources are being used, the highest priority ceiling of this resource set is called the priority ceiling of the system
- Resource allocation rule in PCP
    - If *R* is held by another task, the request fails and the requesting task is blocked
    - If *R* is free then:
        1. If the requesting task's priority is **higher** than the current system priority ceiling, *R* is allocated to it
        2. If the requesting task's priority is **lower** than the current system priority ceiling, the request is denied and the task is blocked
        - **Except** if the requesting task is holding the resource(s) whose priority ceiling(*R*) is equal to the priority ceiling of the system, in which case the resource is allocated to the requesting task
- Priority inheritance rule in PCP
    - When a task $T_1$ gets blocked by $T_2$, $T_2$ inherits the priority of $T_1$ *or* the ceiling of the allocated resources (both is also okay, but may result in a different schedule)
    - $T_2$ executes at the inherited priority until it releases every resource whose priority ceiling is equal to or higher than the inherited priority of $T_2$. At this time, the priority of $T_2$ returns to the priority it held when it acquired the resoure *R*
- Comments
    - Priority ceiling of a resource is fixed since we assume we know all resource requests. However, priority ceiling of the system is dynamically changed
    - The priority of a task is updated when a high-priority task $T_1$ is blocked by a low priority task $T_2$, in which $T_2$'s priority is updated to the priority ceiling of the allocated resource
    - Only when a task has higher priority than the system's priority ceiling, the task will acquire the given resource if available
    - A task can be blocked by **at most one** critical section
    - Higher run-time overhead than PIP, but avoids deadlock


Control Systems
---

Control System Review
- What is a control system?
    - Desired output, desired performance with specified input
    - Performance: transient response and steady state error
- Types of systems:
    - Open loop
    - Closed loop
    - Multi-loop
- Time Doman vs Frequency Domain
    - Input and output are in the time domain, we use the laplace transform to invesitgate system behavior in the frequency domain

Laplace and Inverse Laplace Transform
- Laplace transforms from the time domain to the frequency domain, transforming differential equations into algebraic equations and convolution into multiplication
- Laplace Transform:  
$$F(s) = \int_0^\infty f(t) e^{-st} dt$$
- Inverse Laplace Transform:  
$$f(t) = \frac{1}{2\pi j} \int_{c - j \infty}^{c + j \infty} F(s) e^{st} ds$$
- Table of Laplace and Inverse Laplace Transforms can be found [at this link](http://tutorial.math.lamar.edu/pdf/Laplace_Table.pdf)
- Note, for derivates and integrals:  
$$\mathcal{L}[f'(t)] = sF(s) - f(0), \mathcal{L}[f'(t)] = s^2 F(s) - s f(0) - f'(0)$$  
$$\mathcal{L} \left[\int_0^t f(t) dt \right] = \frac{F(s)}s$$

Frequency Response
- For a sinusoidal input, the output of a linear system is also a sinusoidal. However, the output will have a **different magnitude** and will be subject to a **phase shift**:  
$$V(t) = \sin(\omega t) \to G(s) \to vel(t) = a \sin(\omega t + \phi)$$
- We can also replace $s$ with $jw$ in the transfer function $G(s)$ to get $G(jw)$
    - Allows for calculation of gain and frequency
    - Can also transfer a Laplace transform into a Fourier transform

Time Response and Parameter of the System
- Why is time response important?
    - Finding steady state, steady state error, etc.
- Consider the following system:  
$$Y(s) = \frac{s+2}{s(s+5)} = \frac{2/5}{s} + \frac{3/5}{s+5}$$  
$$y(t) = \frac25 + \frac35 e^{-5t}$$
- The output response of the system consists of:  
    1. A **natural/transient** response:  
    $$\frac35 e^{-5t}$$  
    2. A **forced/steady state** response:  
    $$\frac25$$
- Poles and Zeros
    - In the previous example:  
    $$Y(s) = \frac{s+2}{s(s+5)}$$  
    $s = 0, s = -5$ are **poles** and $s = -2$ is the **zero** of the transfer function
    - The pole at the origin generated a step function at the output, while the pole at $-5$ generated the transient response term $e^{-5t}$
        - The further to the left a pole is on the negative real axis, the faster the exponential transient response will decay to 0
    - The zeros and poles generate the aplitude for both the forced and natural responses

Transfer Functions and Characteristic Equation of a System
- Transfer function AAA
- The **characteristic equation** is setting the denominator of the closed-loop transfer function to 0

First Order Systems **(not on list)**
- A first-order system will use the following equations:  
$$X(s) = \frac1s, G(s) = \frac{a}{s+a}$$
- The output of a general first order system to a step input results in a transfer function:  
$$Y(s) = X(s) G(s) = \frac{a}{s(s+a)}$$
- The result in the time domain is given as:  
$$y(t) = 1 - e^{-at}$$  
where $a$ is the only parameter that affects the output:  
$$t = \frac1a, y(t) = 0.63$$  
$1/a$ is the time constant of the response, and is the time it takes for the step response to rise to 63% of its final value
- **Rise Time ($T_r$)**: Time to go from 0.1 to 0.9 of its final value:  
$$T_r = \frac{2.2}a$$
- **Settling Time ($T_s$)**: Time for the response to reach and stay within 2% of its final value:  
$$T_s = \frac4a$$

Second Order Systems **(not on list)**
- Most real world systems are not first order systems. A general second order system is defined by the transfer function:  
$$G(s) = \frac{b}{s^2 + as + b}$$
- Find the poles of this transfer function to example the behaviour of the output response. Using the quadratic formula:  
$$s_1, s_2 = \frac{-a \pm \sqrt{s^2 - 4b}}2$$
    - Note, if $a = 0$, the poles will have only an imaginary part $\pm jw$ and by definition the **natural frequency** $w_n = \sqrt{b}$ is the frequency of oscillation of this system
- The complex poles have a real part $\sigma = -a/2$
- The magnitude of $\sigma$ is called the "exponential decay frequency" and $w_n$ the "natural frequency". We define the **damping ratio/coefficient**, $\zeta$, as:  
$$\zeta = \frac{\text{Exponential decay frequency}}{\text{Natural frequency}}$$  
$$\zeta = \frac{\sigma}{w_n} = \frac{a/2}{w_n}$$  
this equation gives us:  
$$a = 2 \zeta w_n$$
- The general second order transfer function can now be written as:  
$$G(s) = \frac{w_n^2}{s^2 + 2 \zeta w_n s + w_n^2}$$  
$$s_1, s_2 = - \zeta w_n \pm w_n \sqrt{\zeta^2 - 1}$$
- Observations
    - Two imaginary poles at $\pm j \omega_n : \zeta = 0$ **(undamped)**
    - Two complex poles at $\sigma_d \pm j \omega_n : 0 \lt \zeta \lt 1$ **(underdamped)**
    - Two real poles at $\sigma_1 : \zeta = 1$ **(critically damped)**
    - Two real poles at $\sigma_1, \sigma_2 : \zeta \gt 1$ **(overdamped)**
- **Settling Time ($T_s$)**: Time for the response to reach and stay within 2% of its final value:  
$$T_s = \frac4{\delta \omega_n}$$
- **Peak Time ($T_p$)**: Time required to reach the first or max peak:  
$$T_p = \frac{\pi}{\omega_n \sqrt{1 - \zeta^2}}$$
- **Percent Overshoot (%OS)**: The amount that the waveform overshoots the steady state of final value at peak time:  
$$\%OS = \frac{c_{max} - c_{final}}{c_{final}}$$

Final Value Theorem
- If a system is stable and has a final constant value, then this theorem can be used to find the steady state value without solving for the system's entire response:  
$$\lim_{t \to \infty} x(t) = \lim_{s \to 0} s X(s)$$
- Whether a given function has a final value or not depends on the location of the poles of its transfer function
    - If there are poles in the right hand side of the s-plane or if there are pairs of complex conjugate poles on the imaginary axis, the final value **does not exist**

Location of Poles and Root Locus **(not on list)**
- In designing a controller, the requirements of the user must be considered. These may include:
    - Rise time
    - Settling time
    - Steady-state accuracy
- **Root Locus** is a design and analysis mathod that **tells us how the roots change when a parameter such as a gain varies**
- Rules for Sketching Root Locus
    1. **Number of Branches**: Equal to the number of closed-loop poles, which is equal to the number of finite open loop poles or the number of finite open loop zeros
    2. **Symmetry**: The root locus is symmetrical about the real (x) axis
    3. **Starting and Ending Points**: The root locus begins at the finite and infinite poles of the open loop transfer function $G(s) H(s)$, and ends at the finite and infinite open loop zeros of $G(s) H(s)$
    4. **Behaviour at Infinity**: The root locus approaches straight lines as asymptotes as the locus approaches infinite. Further, the equation of the asymptotes is given by the real axis intercept $\sigma_a$ and angle $\theta_a$ as follows:  
    $$\sigma_a = \frac{\sum \text{finite poles} - \sum \text{finite zeros}}{\# \text{finite poles} - \# \text{finite zeros}}$$  
    $$\theta_a = \frac{(2k+1)\pi}{\# \text{finite poles} - \# \text{finite zeros}}$$  
    where $k = 0, \pm1, \pm2, \pm3$, and the angle is given in radians w/r/t the positive extension of the real axis
    5. **Breakaway or Break-In Points**: The breakaway/break-in points are located at the roots of the following equation:  
    $$\frac{d[G(s) H(s)]}{ds} =0$$

PID Controller and Software Implementation
- **Proportional Control** action is obtained when we multiply the error signal by a constant gain:  
$$K_p e(t) = K_p [u(t) - y(t)]$$
- **Integral Control** action adds a compensator term $G_c = K_I/s$ to the system, forcing steady state error for a step input to be 0
- **Proportional-Integral (PI) Control** combines a proportional and integral controller in one system
- **Derviative Control** adds the compensator $G_c = K_D s$, introducing an open loop zero
    - As $K_D$ increases, the system may not be stable
- **Proportional/Integral/Derivative (PID) Control** is a combination of all 3 control types making the terms:  
$$G_c(s) = K_P + \frac{K_I}{s} + K_D s$$
    - Proportional Gain $K_P$: Steady state system error is reduced by increasing $K_P$
    - Integral Gain $K_I$: We can use integral action to reduce steady state error to 0, with a trade-off to stability and dynamic performance
    - Derivative Gain $K_D$: Settling/peak time and damping in our systems directly affected

Sampling (Analog to Digital Converter) **(not on list)**
- Conversion takes place repetitively at instants of time that are *T* seconds apart, *T* is called the sampling period and $1/T$ is the sampling rate in cycles per second
- Accuracy of the digital signal depends on the number of bits used to represent the samples
- **Quantization Error**: Approximations can be measured at certain levels, and analog signals that fall between the values will need to be approximated to the nearest binary value. If we use 3 bits, 8 levels to divide the maximum analog voltage *M*, we would have each section with *M*/8 volts. The quantization error can be calculated as:  
$$\frac12 * \frac{M}{2^n} = \frac{M}{2^{n+1}}$$  
where *n* is the number of bits used for digitization
    - The **resolution** of an A/D converter is the minimum value of the output that can be represented as a binay number:  
    $$\frac{M}{2^n}$$
- Input signals are available only at sample intervals of time, thus the reference input *r* is a sequence of sample values $r(kT)$ instead of $r(t)$
    - A sampler is basicaly a switch that closes every *T* seconds for one instant:  
    $$r(kT) \delta(t - kT)$$
- If we sample a signal $r(t)$ to obtain $r^*(t)$, we can get:  
$$r^*(t) = \sum_{k = 0}^{\infty} r(kT) \delta(t - kT), t \gt 0$$
- Using the Laplace Transform, we get:  
$$R^* (s) = \mathcal{L}(r^*(t)) = \sum_{k = 0}^\infty r(kT) e^{-ksT}$$

Z-Transform and Inverse Z-Transform
- If we define the $z = e^{sT}$, then we get the **z-Transform Function of Sampled Data**, defined as:  
$$Z(r(t)) = Z(r^*(t)) = \sum_{k=0}^\infty r(kT) z^{-k}$$
- In general, **the z-Transform of a function f(t) is defined as:**  
$$Z(f(t)) = F(z) = \sum_{k=0}^\infty f(kT) z^{-k}$$
- **Zero Order Hold (D2A Converter)**: A device that holds the sampled signal $r(t)$ to a constant value for the duration of the sampling period. The transfer function of a ZoH is:  
$$\mathcal{L}(u(t) - u(t-T)) = \frac1s - \frac{e^{sT}}s$$
- The **Inverse z-Transform** has two methods:
    1. **Power Series Method**: If $G(z)$ is expressees as the ratio of two polynomials in *z*, we can write $G(z)$ in a power series:  
    $$G(z) = a_0 + a_1 z^{-1} + a_2 z^{-2} + ...$$
    2. **Partial-Fraction Expression Method**: Write $G(z)$ in partial fraction expression and use the following equation to find $g(k)$:  
    $$\frac{z}{z-a} = 1 + az^{-1} + a^2 z^{-2} + ...$$

Stability Range in Z-Plane and Mapping from s-Plane to z-Plane
- From continuous control, we know that: the region of stability is the left half of the s-plane. We know how to transform $G(s)$ to $G(z)$, where does the region of stability lie on the z-plane?
- By definition $z = e^{sT}$. Let $s = \alpha + j \omega$, then:  
$$z = e^{sT} = e^{T(\alpha + j \omega)} = e^{\alpha T} e^{j \omega T}$$  
$$z = e^{\alpha T}(cos(\omega T) + j*sin(\omega T)) = e^{\alpha T} \angle \omega T$$
- Each region on the s-plane is put into a corresponding region on the z-plane
- We assume $s = \alpha + j \omega$
    - If $\alpha$ is positive, $e^{\alpha T} \gt 1$. Thus, the points on the positive real axis of the s-plane are points outside the unit circle on the z-plane
    - If $\alpha$ is negative, $e^{\alpha T} \lt 1$. Thus, the points on the negative real axis of the s-plane are points inside the unit circle on the z-plane
    - Points on the $j \omega$ axis, $\alpha$ has 0 values, so $e^{\alpha T} = 1$. Thus, points on the $j \omega$ axis of the s-plane map into points on the unit circle of the z-plane  
    ![img]({{ site.url }}/assets/4aa4/zps1.PNG)
- Vertical lines in the s-plane map to lines of circles with constant radius in the z-plane:  
![img]({{ site.url }}/assets/4aa4/zps2.PNG)
- Horizontal lines in the s-plane map to lines of constant angle in the z-plane:  
![img]({{ site.url }}/assets/4aa4/zps3.PNG)

Stability of a Digital Control System
- The system is **stable** if all poles of the closed loop transfer function are inside the unit circle on the z-plane
- The system is **unstable** if any of the poles are outside the unit circle, and/or more than one pole is on the unit circle
- The system is **marginally stable** if one of the poles is on the unit circle and all other poles are inside the unit circle
- Pole values can change for different values of *T*, making the system only stable in a certain range of values of *T*

Root Locus in Z-Plane
- Determines the location of roots of the characteristic equation of a closed loop control system as the overall system gain (often denoted by *K*) varies
- Steps to plot closed-loop poles:
    1. Derive the open loop function $K \bar{GH}$
    2. Factor the numerator and denominator to get the open loop zeros and poles
    3. Plot the roots of $1 + K \bar{GH} = 0$ into z-Plane as *K* varies
    - Note that while construction rules for the z-Plane are identical with those for the s-Plane, **the interpretation is different**
    4. AAA

S/Z-Transform Table  
![img]({{ site.url }}/assets/4aa4/zps5.PNG)

Approximation of Continuous Control System to Discrete System & Implement a Discrete Controller from Z-Transform
- Gonna be honest left these ones in the slides, this content is already a pain as is

Important Notes
===

Terminology
- $X(s)$: System Input
- $Y(s)$: System Output
- $G(s)$: Transfer Function
- $H(s)$: Feedback Function
- $\mathcal{L}(f(t))$: Laplace Transform (time domain to frequency somain)
- $\mathcal{L}^-{1}(F(s))$: Reverse Laplace Transform
- **Characteristic Equation**: Setting the denominator of the closed-loop transfer function to 0 ($1 + G(s)H(s)$)

First-Order Systems
- $Y(s) = X(s)G(s)$
    - $X(s) = 1/s$: Input
    - $G(s)$: Transfer Function  
    $$\frac{a}{s+a}$$
- $Y(s) = a + be^c$
    - $a$: Steady State/Forced Response
    - $be^c$: Transient/Natural Response
- $T_r$: Rise Time - time for the waveform to go from 0.1 to 0.9 of its final value  
$$T_r = \frac{2.2}{a}$$
- $T_s$: Settling Time - time for the response to reach and stay within 2% of its final value  
$$T_s = \frac4a$$

Second-Order Systems
- $$G(s) = \frac{b}{s^2 + as + b}$$  
$$G(s) = \frac{w_n^2}{s^2 + 2 \zeta w_n s + w_n^2}$$
- $$s_1, s_2 = \frac{-a \pm \sqrt{a^2 - 4b}}2$$  
$$s_1, s_2 = -\zeta w_n \pm w_n \sqrt{\zeta^2 - 1}$$
- If a=0:  
$$G(s) = \frac{b}{s^2 + b}$$
    - Poles will only have imaginary part $\pm jw$ and the natural frequency $w_n = \sqrt{b}$ is the frequency of oscillation of the system
- $\sigma$: Real part of complex poles  
$$\sigma = \frac{-a}2$$
- $\zeta$: Damping Coefficient/Ratio  
$$\zeta = \frac{\sigma}{w_n} = \frac{a/2}{w_n}$$
- Pole Analysis
    - **Undamped**: Two imaginary poles at $\pm j \omega_n : \zeta = 0$ 
    - **Underdamped** Two complex poles at $\sigma_d \pm j \omega_n : 0 \lt \zeta \lt 1$
    - **Critically Damped** Two real poles at $\sigma_1 : \zeta = 1$
    - **Overdamped** Two real poles at $\sigma_1, \sigma_2 : \zeta \gt 1$ 
- **Settling Time ($T_s$)**: Time for the response to reach and stay within 2% of its final value:  
$$T_s = \frac4{\zeta \omega_n}$$
- **Peak Time ($T_p$)**: Time required to reach the first or max peak:  
$$T_p = \frac{\pi}{\omega_n \sqrt{1 - \zeta^2}}$$
- **Percent Overshoot (%OS)**: The amount that the waveform overshoots the steady state of final value at peak time:  
$$\%OS = \frac{c_{max} - c_{final}}{c_{final}} = e^{- \frac{\pi \zeta}{\sqrt{1 - \zeta^2}}} * 100$$

**Final Value Theorem**: Find steady state value without solving entire system response
$$\lim_{t \to \inf} x(t) = \lim_{s \to 0} s X(s)$$

**Root Locus**
1. **Number of Branches**: Equal to the number of closed loop poles or finite open loop poles
2. **Symmetry**: Symmetric about the real axis
3. **Starting and Ending Points**: Begins at finite and infinite open loop poles of $G(s)H(s)$ and ends at the finite and infinite open loop zeros
4. **Behaviour at Infinity**: Asymptote values given by real axis intercept $\sigma_a$ and angle $theta_a$:  
$$\sigma_a = \frac{\sum \text{finite poles} - \sum \text{finite zeros}}{\# \text{finite poles} - \# \text{finite zeros}}$$  
$$\theta_a = \frac{(2k+1)\pi}{\# \text{finite poles} - \# \text{finite zeros}}$$  
where *k* is an integer value
5. **Breakaway or Break-In Points**: Located at the roots of the derivative of the equation:  
$$\frac{d[G(s) H(s)]}{ds} = 0$$
- The gain *K* is minimum for break-in points, and maximum for breakaway points
- Stability only when the root locus is only in the negative real axis

**Controllers**
- $u(t)$: Input
- $e(t)$: Error signal ($u(t) - y(t)$)
- $y(t)$: Output
- **Proportional Control**: Multiply error signal by a constant gain  
$$G_c = K_p$$
- **Integral Contrl**: Multiply by gain over s  
$$\frac{K_I}{s}$$
- **Proportional-Integral Control**: Combine the two above  
$$G_c = \frac{K_I}s + K_p$$
- **Derivative Control**: Gain multiplied by s  
$$G_c = K_D s$$
- **PID Control**  
$$G_c = K_p + \frac{K_I}s + K_D s$$

**Z-Transform**
- $T$: Sampling period (sec)
    - A sampling period given as *x* Hz gives $T = 1/x$
- **Quantization Error**: Estimation error from sampling using *n* bits with *M* maximum analog voltage  
$$\frac{M}{2^{n+1}}$$
- **Zero Order Hold**: Holds and repeats a sampled signal  
$$\frac{1 - e^{sT}}{s}$$  
$$Z[G(s)] = (1 - z^{-1}) Z \left[ \frac{G(s)}s \right]$$
- **S/Z-Transform Table**  
![img]({{ site.url }}/assets/4aa4/zps5.PNG)
- NOTE:  
$$a^k = ? = \frac{z}{z-a}, u(k) \text{ if } a=1$$

**Z-Plane Root Locus**
1. The loci starts from and goes away from the poles and ends at and goes towards the zeros of $K \bar{GH}$
2. For $K \gt 0$, the real axis loci lie to the left of an odd number of poles and zeros
3. The loci are symmetrical with respect to the real axis
4. The number of asymptotes is equal to the number of $K \bar{GH}$ poles, $n_p$, minus the number of zeros, $n_z$. The angle can be found with:  
$$\theta_a = \frac{(2k + 1)\pi}{n_p - n_z}$$
5. The origin of the asymptotes on the real axis is given by:  
$$\sigma_a = \frac{\sum \text{poles} - \sum \text{zeros}}{n_p - n_z}$$
6. The breakaway point for the locus between two poles or break-in point for the locus between two zeros is found by:  
$$\frac{d[\bar{GH} (z)]}{dz} = 0$$
- The system is **marginaly stable** when $z = -1$, find the *K* value for this

**CCS to DCS**
- Delay in Time Domain to Z Domain:  
$$u(k - n) \leftrightarrow z^{-n}U(z)$$
- Sample Domain into Time Domain:  
$$s U(s) \to u'(t)$$
- Solving the Z-Transform of a Difference Equation  
$$D(s) = \frac{U(s)}{E(s)} = K_0 \frac{s+a}{s+b}$$
    - Go from s-domain to t-domain to z-domain, ultimately solving for $U(z)/E(z)$

**Approximation Methods**
- Forward Rectangular Rule
- Backwards Rectangular Rule
- Trapezoid Rectangular Rule / Tustin's Method / Bilinear Transformation
![img]({{ site.url }}/assets/4aa4/exm1.PNG)

**Links**

[Inverse Z-Transform](https://www.wolframalpha.com/input/?i=inverse+Z+transform+calculator)

[Partial Fraction Decomposition](https://www.symbolab.com/solver/partial-fractions-calculator)

[Derivative Calculator](https://www.derivative-calculator.net/)

[Integral Calculator](https://www.integral-calculator.com/)

[Root Locus Drawing](https://lpsa.swarthmore.edu/Root_Locus/RLDraw.html)
